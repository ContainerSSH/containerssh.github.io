{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ContainerSSH: Launch containers on demand","text":"Build a lab <p>Building a lab environment can be time-consuming. ContainerSSH solves this by providing dynamic SSH access with APIs, automatic cleanup on logout using ephemeral containers, and persistent volumes for storing data. Perfect for vendor and student labs.</p> <p>Read more \u00bb</p> Debug a production system <p>Provide production access to your developers, give them their usual tools while logging all changes. Authorize their access and create short-lived credentials for the database using simple webhooks. Clean up the environment on disconnect.</p> <p>Read more \u00bb</p> Run a honeypot <p>Study SSH attack patterns up close. Drop attackers safely into network-isolated containers or even virtual machines, and capture their every move using the audit logging ContainerSSH provides. The built-in S3 upload ensures you don't lose your data.</p> <p>Read more \u00bb</p>"},{"location":"about/","title":"About ContainerSSH","text":"About ContainerSSH <p>ContainerSSH is a fully open source community-driven project. If you would like to know more about our contribution and decision making process, check out the ContainerSSH Working Group Charter. This project is made with \u2764\ufe0f by the following group of volunteers.</p> <p> We are a Cloud Native Computing Foundation sandbox project.</p>"},{"location":"about/#working-group","title":"Working Group","text":""},{"location":"about/#chairs","title":"Chairs","text":"<p> Nikos Tsipinakis </p>"},{"location":"about/#members","title":"Members","text":"<p>If you would like to join the WG, please check out the charter, code of conduct, and contribution guide.</p>"},{"location":"about/#contributors","title":"Contributors","text":"<p> Bence Santha Ce Gao Duc Tran Janos Bonic Jean-Pascal Journet Keming Montgomery Edwards\u2074\u2074\u2078 Richard Kovacs Zhizhen He Ziyang Li </p> <p>Note: The contributors list is opt-in for privacy reasons. If you wish to be listed on this page please add your name here.</p>"},{"location":"about/imprint/","title":"Imprint","text":"Imprint <p>ContainerSSH is being developed by a community of volunteers.</p> <p>For a postal address suitable for legal service please contact handshake@containerssh.io. The address will be handed out to registered law firms and government agencies only.</p>"},{"location":"about/license/","title":"License","text":"License <p>The ContainerSSH source code is distributed under the Apache 2.0 license. This website and any documentation for ContainerSSH is distributed under the Creative Commons Attribution 4.0 International license.</p> <p>The ContainerSSH binary contains open source components under the following licenses:</p> <ul> <li>MIT, MIT-0</li> <li>BSD</li> <li>Apache 2.0</li> <li>Mozilla</li> <li>ISC</li> </ul> <p>Details of these licenses can be found in the <code>NOTICE.md</code> file published with each release or by running <code>containerssh --license</code>.</p>"},{"location":"about/license/#trademarks","title":"Trademarks","text":"<p>The Linux Foundation\u00ae (TLF) has registered trademarks and uses trademarks. For a list of TLF trademarks, see Trademark Usage. Company marks such as GitHub, Docker, Twitter, etc. Trademarks are properties of their respective owners and may only be used according to their trademark guidelines.</p>"},{"location":"about/packages/","title":"ContainerSSH packages","text":"<p>ContainerSSH maintains an experimental package repository at <code>packages.containerssh.io</code>. This page describes how to add the repository to your operating system.</p> <p>Warning</p> <p>The package repository is experimental and is likely to change in the future.</p> Debian/Ubuntu <p>First, you need to add the tools needed for adding a custom repository:</p> <pre><code>sudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n</code></pre> <p>Next, you should add our GnuPG key as a trusted key for packages:</p> <pre><code>curl -fsSL https://packages.containerssh.io/debian/gpg | sudo apt-key add -\n</code></pre> <p>Verify that you now have the correct fingerprint:</p> <pre><code>sudo apt-key fingerprint F358FABA\n</code></pre> <p>Add our repository:</p> <pre><code>sudo add-apt-repository \\\n   \"deb [arch=amd64] https://packages.containerssh.io/debian ./\"\n</code></pre> <p>Finally, refresh the package list:</p> <pre><code>sudo apt-get update\n</code></pre> <p>Now you can install the ContainerSSH packages.</p>"},{"location":"about/privacy/","title":"Privacy policy","text":"<p>This is the privacy policy for the website containerssh.io.</p>"},{"location":"about/privacy/#cookies","title":"Cookies","text":"<p>This website uses no cookies.</p>"},{"location":"about/privacy/#personal-data-collected-by-this-website","title":"Personal data collected by this website","text":"<p>This website does not directly collect personal data.</p>"},{"location":"about/privacy/#third-party-providers","title":"Third party providers","text":"<p>This website is hosted on GitHub pages. As such, when loading this website, your IP address will be available and may be stored by GitHub in their access logs, along with the information which page specifically you loaded. </p>"},{"location":"about/security/","title":"Security","text":"Security <p>ContainerSSH is a security-relevant piece of software. If you have found any security issues you wish to report to us please mail handshake@containerssh.io.</p> <p>We are also in the process of signing all our binaries, which is expected to complete during 2021.</p>"},{"location":"about/security/#our-gpg-key","title":"Our GPG key","text":"<p>You may use the following GPG key to encrypt messages to us. This key is always available at https://containerssh.io/gpg.txt.</p> <pre><code>-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n1IxE\n=/NjK\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2020/11/25/the-road-to-0-4/","title":"The road to ContainerSSH 0.4: modularized structure, audit logging, and more","text":"<p>After a rapid rush of releases this summer we have announced that version 0.4.0 would have a long-awaited feature: detailed audit logging. This feature would allow for a forensic reconstruction of an SSH session. The use cases for this are diverse: from building honeypots to securing a corporate environment. We even published a preview release for test driving this feature. We even implemented an automatic upload for the audit logs to an S3-compatible object storage. So, what happened? Why isn't 0.4.0 released yet?</p> <p>The delay has everything to do with maintainability. The PR-1 implementation of the audit logging was built right into ContainerSSH causing a deluge of code changes. While it technically worked, it blew up the code in size and made features extremely hard to test. Look at this code, for example. The actual authentication code dwarfs in comparison to the audit logging parts. You could say, the code violates the Single Responsibility Principle. There is no way we could retrofit component-level tests into this.</p> <p>When we began work on ContainerSSH we knew that the code quality was prototype-level at best and we'd have to overhaul large parts before the <code>1.0</code> release. In essence, version <code>0.4</code> became the release to make that overhaul happen. We started pulling out large parts of the codebase into independent libraries and started retrofitting them with unittests. Needless to say, moving from only having a few integration tests to writing unittests for each component unearthed a slew of bugs, which were promptly fixed.</p> <p>Starting ContainerSSH with a prototyping approach wasn't a bad decision, though: it helped us getting something working fairly quickly and kept motivation high. This is especially important with a purely open source project.</p> <p>Pulling everything apart into separate libraries also gave us a couple of additional advantages. We created developer documentation for each library, and we now also have the ability to extend ContainerSSH in a significant way.</p> <p>ContainerSSH now has clean APIs to handle SSH events. These clean APIs allow developers to plug in additional functionality without breaking any existing features. For example, the audit log functionality is integrated in a separate repository with this approach.</p> <p>Using a layered approach gives us quite a few options. One idea we are toying with is to create an SSH proxy that forwards connections to a backend SSH server. This would allow users to deploy ContainerSSH as a pure audit logging facility.</p> <p>Another idea is to build in PAM authentication and enable a direct shell on the host, which would enable ContainerSSH to function as a replacement for OpenSSH with the added functionality of audit logging.</p> <p>If you like these ideas please comment on the issues linked above and let us know about your use case.</p> <p>So, when is <code>0.4</code> coming out? We don't know yet. Our plan is early next year, but our focus in this release is stability.</p>"},{"location":"blog/2020/12/24/the-agent/","title":"Announcing the ContainerSSH Guest Agent","text":"<p>ContainerSSH is an integration project between the SSH library and the Docker and Kubernetes API. However, neither the Docker nor the Kubernetes API have been designed to host some of the more intricate SSH specific features.</p> <p>For example, the Kubernetes \"attach\" API does not allow for retrieving the output of the command running in the container that happened before attaching reliably,and neither Docker nor Kubernetes allow sending signals to commands running in an \"exec\", etc.</p> <p>We won't go into details on these various issues, suffice it to say, some of them break the expectations you would have for a classic SSH server. There are two paths ahead of us: either try to send pull requests to the Docker and Kubernetes projects to patch these features in, or add a guest agent to the container images that enable these extra features.</p> <p>Sending in patches to enable all the functionality would be a very long process and chances are that our patches wouldn't be accepted as they add additional functionality that is, admittedly, fringe for most users. Therefore, we opted to build a guest agent.</p> <p>The ContainerSSH guest agent is a binary containing only minimal functionality and no external dependencies that can easily be added to any container image as a single binary. We have already added it to the default <code>containerssh/containerssh-guest-image</code> and we encourage users who built their own image to include the agent as well and keep it updated. Please see github.com/containerssh/agent for details.</p> <p>That being said, the guest image is and will be optional. It will be a feature that needs to be explicitly enabled in the configuration. Guest agent support will arrive in ContainerSSH 0.4 after the holidays.</p> <p>Merry Christmas and Happy Holidays!</p>"},{"location":"blog/2021/03/19/we-messed-up/","title":"We broke your images \ud83d\ude22","text":"<p>Two days ago, on March 17, 2021 around 4:30 PM UTC we pushed a change to our build system that broke the container images we published on the Docker Hub. This change resulted in the following error when running the container:</p> <p>Cannot start service containerssh: OCI runtime create failed: container_linux.go:349: starting container process caused \"exec: \\\"/containerssh\\\": permission denied\": unknown</p> <p>To make matters worse, this did not only affect the most recent image, it broke all container images. The issue was reported an hour later and fixed on around noon UTC on the 18th of March, 2021.</p> <p>If you are affected by this issue you can pull the fixed ContainerSSH image by pulling it:</p> DockerPodmanKubernetes <pre><code>docker pull containerssh/containerssh:&lt;version&gt;\n</code></pre> <pre><code>podman pull containerssh/containerssh:&lt;version&gt;\n</code></pre> <p>Please set the <code>imagePullPolicy</code> in your pod spec to <code>Always</code> or switch to the image <code>containerssh/containerssh:&lt;version&gt;-20200318</code></p> <p>The <code>&lt;version&gt;</code> tag in this case should be replaced with your ContainerSSH version (e.g. <code>0.3.1</code>).</p> <p>There is no way around it: we messed up. Pretty badly at that, we potentially broke your production environment without an easy way to roll back to a previous version. This should not happen, not even in a pre-1.0 version, especially not with something as trivial as a permission mistake.</p> <p>These images should have never made it to the Docker Hub, our testing procedures (obviously lacking) should have caught this and we are determined to fix them. We are very sorry for the inconvenience and the potential outage this issue has caused.</p> <p>Please reach out to us</p> <p>If you need to reach out quickly you can tag or DM us in Twitter to notify the core maintainers.</p>"},{"location":"blog/2021/03/19/we-messed-up/#versioning-in-the-future","title":"Versioning in the future","text":"<p>Going forward the ContainerSSH images will be versioned in two parts: the ContainerSSH version and the image build date. For example, the image tag <code>0.3.1-20200318</code> points to the image built from ContainerSSH version 0.3.1 on the 18th of March, 2020. The tag <code>0.3.1</code> points to the latest built from ContainerSSH version 0.3.1. The tag <code>0.3</code> points to the latest ContainerSSH version in the 0.3 series, and so on. You can find the full list of available tags on the Docker hub.</p> <p>Why are we doing this?</p> <p>This is necessary because container base images (Alpine Linux in our case) get security updates much more frequently than we release ContainerSSH versions.</p>"},{"location":"blog/2021/03/19/we-messed-up/#why-did-this-happen-post-mortem","title":"Why did this happen (post mortem)","text":"<p>With the upcoming 0.4 release we are drawing closer to creating a first stable version of ContainerSSH (1.0). As part of the effort to secure the container images we publish we are now using Snyk, graciously made available to open source projects for free. As we scanned our container images built late last year we realized that there were multiple vulnerable libraries in them. These vulnerabilities did not affect ContainerSSH, but it highlights the need to update the images we release much more frequently than we release new ContainerSSH versions. It is also unreasonable to ask system administrators to jump through the hoops of switching to a new ContainerSSH version just because there is a new container image available. Therefore, we decided to version the container images separately from ContainerSSH as described above.</p> <p>However, our existing build system (Goreleaser, an excellent tool) does not have the capability to manage images in this manner. Therefore, we had to come up with a new build system. We were looking at several ones, but none of them could fulfill the need for a cross-platform build system that is easy for developers to run on their potentially non-Linux machines. Therefore, we decided to code the relatively simple process of building a container image in Go. This tool downloads the Linux <code>.tar.gz</code> files from the GitHub releases as specified in the configuration and unpacks them.</p> <p>This is where the critical mistake happened: the unpacking code used <code>os.Create()</code> instead of <code>os.OpenFile()</code>. This resulted in the permissions not being set on the files extracted from the archive. The <code>Dockerfile</code> moved from the Goreleaser build system also did not contain the required <code>chmod</code> instruction.</p> <p>How did this make it into production? It clearly never should have, and that's a hole in our testing procedures. It is not enough for us to test the ContainerSSH code, we should have tested the built images before pushing them, no matter what build tool was used. Mistakes can happen, we need to make sure big ones don't make it to the images you are using.</p> <p>To fix this issue we will institute automated tests in the build tool that test the functionality of ContainerSSH end-to-end by opening a real SSH connection and executing a real command. This will not be a comprehensive test suite since those are covered by lower level tests, but we will test if the SSH connection can be established, the containers are started, and the output of an executed command is as expected.</p> <p>To sum it up, here's what we learned:</p> <ol> <li>Create automated end to end tests for container images before they are pushed.</li> <li>Don't rely on a previously-working <code>Dockerfile</code> in different circumstances.</li> <li>Always explicitly set permissions for binaries in the <code>Dockerfile</code>, don't rely on filesystem permissions.</li> <li>Make sure you keep permissions across the whole toolchain.</li> </ol> <p>Again, we are very sorry we messed up this badly, we will do everything we can to not repeat this mistake.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/","title":"ContainerSSH 0.4: Audit &amp; Proxy","text":"<p>In November last year we were optimistic that we'd be launching 0.4 early 2021 with the new audit logging feature. Now it is finally time: we are very proud to announce the immediate availability of ContainerSSH 0.4: Audit &amp; Proxy.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#tldr","title":"TL;DR","text":"<p>What we added:</p> <ul> <li>Audit logging</li> <li>Improved logging</li> <li>New SSH proxy backend</li> <li>New Kubernetes backend</li> <li>New Docker backend</li> <li>New security layer</li> <li>The guest agent</li> </ul> <p>What we deprecated:</p> <ul> <li>The KubeRun backend</li> <li>The DockerRun backend</li> <li>The Listen config option</li> <li>The sessionId parameter in the auth/config webhook</li> </ul> <p>What we removed:</p> <ul> <li>The pubKeyBase64 field in the auth webhook</li> <li>Moving from GET to POST in webhooks</li> </ul>"},{"location":"blog/2021/04/01/containerssh-0-4/#audit-logging","title":"Audit logging","text":"<p>The biggest feature of this release is no doubt the audit logging feature. This audit log records everything a user does, on a byte-by-byte basis. That includes things like SFTP uploads, which may slip through your net if you just record typed commands. The audit logging feature can also upload the stored audit logs to an S3-compatible object storage for long-term archival.</p> <p>In the future we plan to release a tool to visually inspect the stored audit logs, and the planned SSH proxy feature will allow you to use it with traditional SSH servers.</p> <p>For details check out the audit logging reference manual.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#new-ssh-proxy-backend","title":"New SSH proxy backend","text":"<p>This new backend forwards connections to a second SSH server instead of starting containers. This makes it possible to use ContainerSSH in two roles:</p> <ol> <li>As a way to dynamically authenticate SSH users or dynamically route users to SSH backends.</li> <li>As a way to audit SSH connections after decryption.</li> </ol> <p>For details check out the logging reference manual.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#better-logging","title":"Better logging","text":"<p>One of the reasons why this release took so long to complete was the addition of a comprehensive logging interface. We added hundreds of log messages, most of them on the debug level that allow you to trace exactly what ContainerSSH is doing. Each of the log messages contains a unique code you can use to identify what's wrong with your setup. These codes are documented in the codes list.</p> <p>We also added options for log targets: you can now log to a file, a syslog server via <code>/dev/log</code> or UDP, or to the standard output.</p> <p>For details check out the logging reference manual.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#new-kubernetes-backend","title":"New Kubernetes backend","text":"<p>Since we wanted to support more use cases we added a completely new backend for Kubernetes to replace the now-deprecated <code>KubeRun</code> backend. The <code>KubeRun</code> backend will remain available until the next release.</p> <p>This new backend supports the new agent we announced back in December to support all the SSH features Kubernetes doesn't normally support. For example, the agent will fix a long-standing issue with Kubernetes where the user would not see the shell because it was written to the standard output before ContainerSSH has attached to the pod.</p> <p>In addition, ContainerSSH supports multiple execution models. The original execution from <code>KubeRun</code> would run one pod per session (multiple pods per SSH connection). The new (default) execution mode now creates one pod per SSH connection and uses the <code>kubectl exec</code> functionality to start the programs for the individual sessions.</p> <p>This also paves the way for future development where we will have (semi) persistent pods. For details check the Kubernetes backend reference manual and the KubeRun deprecation notice.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#new-docker-backend","title":"New Docker backend","text":"<p>Similar to the Kubernetes backend above this release also adds a new backend for Docker and Podman, deprecating the <code>DockerRun</code> backend. The old <code>DockerRun</code> backend will remain available until the next release.</p> <p>As with Kubernetes the new backend supports the new agent we announced back in December to support all the SSH features Kubernetes doesn't normally support, mainly signal delivery.</p> <p>In addition, ContainerSSH supports multiple execution models. The original execution from <code>DockerRun</code> would run one container per session (multiple pods per SSH connection). The new (default) execution mode now creates one container per SSH connection and uses the <code>docker exec</code> functionality to start the programs for the individual sessions.</p> <p>This also paves the way for future development where we will have (semi) persistent containers. For details check the Docker backend reference manual and the DockerRun deprecation notice.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#security-filters","title":"Security filters","text":"<p>This release also adds a set of security filters that can be used for fine-grained control over what SSH interactions to allow or block. For example, you could limit the user to a set of environment variables, only allow running certain programs, etc.</p> <p>For details check the security reference manual.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#ssh-key-format","title":"SSH key format","text":"<p>In the previous releases we transmitted the SSH key to the authentication server in the OpenSSH wire format. This was not easy to implement so in this release we switch to the more popular authorized key format, which is transmitted in the <code>publicKey</code> field.</p> <p>For details check the deprecation notice</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#reference-manual","title":"Reference manual","text":"<p>Alongside of this release we are also adding a comprehensive reference manual which describes in great detail how to set up and configure ContainerSSH.</p>"},{"location":"blog/2021/04/01/containerssh-0-4/#future-plans","title":"Future plans","text":"<p>This release is a 90% rewrite of the ContainerSSH codebase which splits it into modules. This presents a basis for exciting new features, such as SSH port forwarding, SSH single sign-on via a web interface (OAuth2/OIDC), web client, launching VMs instead of containers, etc.</p> <p>For details on the planned features please check our development dashboard.</p>"},{"location":"blog/2021/04/13/devlog-oauth2/","title":"DevLog: SSH authentication via OAuth2","text":"<p>Traditionally, SSH supports authentication via a number of methods. Typically, you'd use passwords or SSH keys to log in. However, other methods are also possible: keyboard-interactive can be used to ask the connecting user a series of questions. This can be used for two factor authentication, for example. GSSAPI authentication allows for using Kerberos tokens obtained, for example, by logging into a Windows domain to be used as SSH credentials.</p>"},{"location":"blog/2021/04/13/devlog-oauth2/#oauth2-why","title":"OAuth2: Why?","text":"<p>When SSH was first invented by by Tatu Yl\u00f6nen in 1995 most terminal windows had the size of 80 by 25 characters. Netscape Navigator was barely a year old and systems like single sign-on weren't even on the horizon.</p> <p>The SSH protocol has, of course, evolved over the years, but even the most recent RFC's that are implemented are over 10 years old.</p> <p>Traditionally, large enterprises (telco providers, etc) relied on strong firewall rules to isolate their administrative (SSH) access from the Internet. In some cases central authentication was implemented, for example by means of authenticating from a central LDAP server, or via configuration management. This was rather the exception and implemented only when required by security standards such as PCI-DSS.</p> <p>As companies moved ouside their traditional network environment into the cloud in recent years access control became more and more of a problem. However, this is not a new problem. SAML was introduced in 2005, but proved to be an unwieldy XML beast, difficult to implement.</p> <p>Recently OpenID Connect (OIDC) became a very popular add-on to OAuth2 to manage single sign-on needs. (This is not to be confused with the traditional OpenID, the two are not related.) Microsoft Active Directory Federation Services offers OIDC support along with SAML 2.0, and Kubernetes also supports OIDC as an authentication method for administrator.</p> <p>What's missing? SSH. The method of accessing the servers running Kubernetes, or traditional workloads.</p> <p>Of course, you can use GSSAPI to provide automatic login capabilities using your Windows domain, but in the age of Bring Your Own Device that's no longer appropriate. </p>"},{"location":"blog/2021/04/13/devlog-oauth2/#oauth2-how","title":"OAuth2: How?","text":"<p>Ok, so it's not 1995 any more, how can we get SSH to authenticate via a browser-based authentication flow?</p> <p>The key lies in the <code>keyboard-interactive</code> authentication method described in RFC 4256. This method is supported by almost all SSH clients and gives the SSH server the ability to send the client a list of questions the client needs to answer. It also allows the server to send the client an instruction text. This instruction text can be used to show the client a link.</p> <p>From here it's fairly simple. Option one is the authorization code flow: client logs in to the web browser and must then copy the code back into their console. The SSH server checks their identity and that's it. See this 17 second video.</p> <p>Option two is the device code flow, where the user is sent to a link and must enter a code from the SSH console. Here we don't send a question, we simply poll the auth token endpoint for the code to be entered. See this 25 second video.</p> <p>The latter would also lend itself to displaying a QR code, but OpenSSH, unfortunately, limits the length of the instruction field to 255 characters and doesn't support UTF-8 either.</p>"},{"location":"blog/2021/04/13/devlog-oauth2/#client-support","title":"Client support","text":"<p>As of writing, we have tested the following clients:</p> <ul> <li>OpenSSH limits the instruction field to 255 characters and does not display non-ASCII characters. </li> <li>PuTTY displays the link, but breaks the link after 78 characters.</li> <li>WinSCP displays the instruction text, but the link is not clickable or copyable. The author of WinSCP has sent us a preview build which contains this feature.</li> <li>FileZilla exhibits a similar link breakage as PuTTY, but it also duplicates all <code>&amp;</code> characters as well. This has been fixed in a nightly build. It also does not display the instructions when no questions are sent. This is required for the device flow.</li> <li>Termius does not display the instruction field on mobile at all, and does not make it possible to copy or click the link on desktop. This issue has been forwarded to their dev team.</li> <li>Bitvise does not make it possible to copy or click the link. They are addressing this as a bug.</li> <li>JuiceSSH (Android) does not display the instruction field on mobile at all.</li> </ul>"},{"location":"blog/2021/04/13/devlog-oauth2/#when","title":"When?","text":"<p>Soon. We don't have an exact release date, it's uncharted territory and we want to wait at least for the more popular SSH clients to release full support for this feature.</p> <p>We would like to thank everyone who helped this project with ideas, input and testing.</p>"},{"location":"blog/2021/05/26/containerssh-0-4-1/","title":"ContainerSSH 0.4.1: Bugfixing Audit &amp; Proxy","text":"<p>ContainerSSH 0.4.1 is now available and contains several bugfixes for the previous version. We encourage all users to upgrade. </p>"},{"location":"blog/2021/05/26/containerssh-0-4-1/#changes-in-detail","title":"Changes in detail","text":"<p>This release fixes 3 bugs that were introduced with the refactor to version 0.4.0. These are:</p> <ul> <li>#201: Incorrect JSON serialization/deserialization from the configuration server when using the Docker backend</li> <li>#209: Incorrect YAML deserialization when using the Kubernetes backend</li> <li>#167: Authentication server ignores password and pubkey options</li> </ul> <p>Thanks to GitHub users ne-bknn and tomcsi for reporting these issues.</p>"},{"location":"blog/2021/05/26/containerssh-0-4-1/#incorrect-json-serializationdeserialization-from-the-configuration-server-when-using-the-docker-backend","title":"Incorrect JSON serialization/deserialization from the configuration server when using the Docker backend","text":"<p>When refactoring ContainerSSH for version 0.4.0 we implemented the JSON serialization and deserialization for the Docker backend incorrectly as reported by GitHub user ne-bknn. The returned JSON from the configuration server had this structure:</p> <pre><code>{\n  \"docker\": {\n    \"execution\": {\n      \"Launch\": {\n      }\n    }\n  }\n}\n</code></pre> <p>The <code>Launch</code> component is not supposed to be in this structure and should be inlined. The serialization is now fixed and the <code>Launch</code> component is removed.</p>"},{"location":"blog/2021/05/26/containerssh-0-4-1/#incorrect-yaml-deserialization-when-using-the-kubernetes-backend","title":"Incorrect YAML deserialization when using the Kubernetes backend","text":"<p>Another serialization issue has been reported by GitHub user tomcsi. This issue has been present since version 0.3 where we added Kubernetes support. Kubernetes uses its own YAML serialization and deserialization library based on ghodss/yaml. This library doesn't add separate YAML tags to the configuration structures, but instead uses the JSON tags. This prevented using several Kubernetes configuration options, such as the <code>hostPath</code> volume type:</p> <pre><code>backend: kubernetes\nkubernetes:\n  pod:\n    spec:\n      volumes:\n        - name: home\n          hostPath:\n            path: /home/ubuntu\n            type: Directory\n</code></pre> <p>We have now introduced using the Kubernetes YAML decoding library for the Kubernetes and KubeRun backends only to facilitate proper serialization. </p>"},{"location":"blog/2021/05/26/containerssh-0-4-1/#authentication-server-ignores-password-and-pubkey-options","title":"Authentication server ignores password and pubkey options","text":"<p>Another bug we discovered after the release was that the new version did't take into account the <code>password</code> or <code>pubkey</code> options in the authentication section.</p> <p>The authentication server could just reject those authentication methods, but in order to cut down on 404 entries in the logs we added these options. This release restores the aforementioned functionality.</p>"},{"location":"blog/2021/05/26/containerssh-0-4-1/#upgrading-to-the-new-release","title":"Upgrading to the new release","text":"<p>If you haven't upgraded to version 0.4.0 yet please see the 0.4.0 announcement for details on what changed from version 0.3. If you have already upgraded to 0.4.0 we recommend testing the new release for you scenario before upgrading and scheduling a brief downtime as you upgrade both the auth-config servers and ContainerSSH itself.</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/","title":"ContainerSSH 0.5: Everything but the Kitchen Sink","text":"<p>After a long slumber ContainerSSH is back with a brand new release! A tremendous amount of changes have been incorporated since the last release including multiple codebase refactors, a move to a monorepo setup for our internal modules and many long and highly requested features such as Oauth2, Kerberos authentication, port and connection forwarding and an advanced metadata handling system.</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#configuration-restructure","title":"Configuration Restructure","text":"<p>The structure of the configuration file has been slightly altered especially when pertaining to the authentication settings and the configuration server. Please consult the reference docs or the quickstart guide and update your configuration file to the new format.</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#change-summary","title":"Change summary","text":"<ol> <li>OAuth2 and Kerberos authentication</li> <li>Authorization webhook</li> <li>Passing metadata from authentication to configuration servers and backends</li> <li>Deploying files into the containers from the authentication and configuration hooks</li> <li>X11 forwarding</li> <li>SSH keepalives</li> <li>Health check endpoint</li> <li>Changes to the Prometheus integration</li> <li>Removed the deprecated DockerRun and KubeRun backends</li> </ol>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#oauth2-and-kerberos-authentication","title":"OAuth2 and Kerberos authentication","text":"<p>The biggest change of this release is support for multiple authentication backends. Thanks to our contributors we now have support for OAuth2 and Kerberos authentication.</p> <p>OAuth2 authentication works with GitHub and any OIDC-compliant authentication server, such as Keycloak and Microsoft Active Directory Federation Services. We have actively worked with several SSH client vendors to make this authentication method work and we are happy to report that it works in OpenSSH, PuTTY, Filezilla, WinSCP, and more. The authentication prompts the user to click on a link in their SSH client and then log in via their normal browser-based flow. What's more, you can automatically expose the GitHub or OIDC token to the container. Your users can directly use their credentials in your ContainerSSH environment.</p> <p>Similarly, Kerberos authentication is also useful in an enterprise setting. When users are logged in to their personal devices using company credentials, they will now be able to automatically log in to ContainerSSH with Kerberos. Optionally, users can also log in to ContainerSSH from a non-authenticated device using username and password, and ContainerSSH will automatically create a Kerberos ticket for them. This ticket is available in the container directly, so your users can work with their Kerberos credentials without any additional steps.</p> <p>Read more \u00bb</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#authorization-webhook","title":"Authorization webhook","text":"<p>As part of our authentication and authorization overhaul we added a separate webhook. This webhook lets you match up the username entered in SSH and the authenticated credentials in a separate step. You can, for example, authenticate a user from Kerberos and then use a webhook to match up their Kerberos identity with the SSH username. </p> <p>Read more \u00bb</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#metadata-handling","title":"Metadata handling","text":"<p>The authentication and configuration servers now support passing metadata between each-other and to ContainerSSH.</p> <p>Read more \u00bb</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#deploying-files","title":"Deploying files","text":"<p>As part of the new metadata system both the authentication and configuration servers can now set environment variables and deploy files in the container user containers. This functionality depends on the ContainerSSH agent to be installed and available in the container image.</p> <p>Read more \u00bb</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#port-socket-and-x11-forwarding","title":"Port, socket and X11 forwarding","text":"<p>From this release support for forward and reverse port forwarding is supported natively, as a result remote development with VSCode and similar functionalities in other IDEs can now be used with ContainerSSH as long as forwarding is enabled. For these features the ContainerSSH agent has to be enabled as the agent acts as the entry &amp; exit points of the connections inside the container.</p> <p>The features implemented correspond to the openssh commands:</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#forward-reverse-port-forwarding","title":"Forward &amp; Reverse port forwarding","text":"<p>Example: Forward port 8080 on the local host the service running on port 8080 on the remote container <pre><code>ssh -L 8080:127.0.0.1:8080 user@example.org\n</code></pre></p> <p>Example: Forward connections from a socket on the local machine to a socket in the container <pre><code>ssh -L /path/to/local/socket:/path/to/remote/socket\n</code></pre></p> <p>Example: Forward connections from port 8080 on the container to the service running on port 8080 on the local machine <pre><code>ssh -R 8080:127.0.0.1:8080\n</code></pre></p> <p>Example: Forward connections from a socket on the container to a socket on the local machine <pre><code>ssh -L /path/to/local/socket:/path/to/remote/socket\n</code></pre></p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#connection-proxying-support-eg-socks","title":"Connection proxying support (e.g. SOCKS)","text":"<pre><code>ssh -D 8080 user@example.com\n</code></pre> <p>You can then use ContainerSSH as a proxy with anything that supports the SOCKS protocol (e.g. Firefox)</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#x11-forwarding","title":"X11 forwarding","text":"<p><pre><code>ssh -X user@example.com\n</code></pre> Any X11 applications launched within the container will be visible on the local machine</p> <p>Read more \u00bb</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#ssh-keepalives","title":"SSH keepalives","text":"<p>Explicit support has been added for SSH KeepAlives. Previously, keepalives received from the client would wield an unknown global command warning and flood the logs, keepalives are now handled transparently and do not generate a warning.</p> <p>Additionally, support has been added to send keepalives to all clients from the server at a pre-defined interval. This can be configured with the following parameters:</p> <pre><code>ssh:\n    # The interval that keepalive messages are sent to each client, defaults to 0 which disables the feature (no keepalives are sent).\n    clientAliveInterval: 10s\n    # The number of unanswered keepalives before ContainerSSH considers a client unresponsive and kills the connection, defaults to 3.\n    clientAliveCountMax: 3\n</code></pre> <p>This can be useful if ContainerSSH is sitting behind a load balancer which automatically kills idle connections after a pre-defined interval. A keepalive will keep the connection active as long as the client is responsive.</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#health-check-endpoint","title":"Health check endpoint","text":"<p>A new health check service has been created that can be used with Kubernetes or loadbalancers to automatically remove unhealthy ContainerSSH instances from the pool.</p> <p>Read more \u00bb</p>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#changes-to-the-prometheus-integration","title":"Changes to the Prometheus integration","text":"<p>The name of some prometheus metrics and units has been altered to adhere to the convention of the metric name ending with the unit.</p> <p>In detail the following metrics have been modified:</p> <ul> <li><code>containerssh_auth_server_requests</code>:<ul> <li>Name changed to <code>containerssh_auth_server_requests_total</code></li> <li>Unit name change from <code>requests</code> to <code>requests_total</code></li> </ul> </li> <li><code>containerssh_auth_server_failures</code>: <ul> <li>Name changed to <code>containerssh_auth_server_failures_total</code></li> <li>Unit name change from <code>requests</code> to <code>failures_total</code></li> </ul> </li> <li><code>containerssh_auth_success</code>: <ul> <li>Name changed to <code>containerssh_auth_success_total</code></li> <li>Unit name change from <code>requests</code> to <code>success_total</code></li> </ul> </li> <li> <p><code>containerssh_auth_failures</code>:</p> <ul> <li>Name changed to <code>containerssh_auth_failures_total</code></li> <li>Unit name change from <code>requests</code> to <code>failures_total</code></li> </ul> </li> <li> <p><code>containerssh_backend_requests</code>:</p> <ul> <li>Name changed to <code>containerssh_backend_requests_total</code></li> <li>Unit name change from <code>requests</code> to <code>requests_total</code></li> </ul> </li> <li> <p><code>containerssh_backend_errors</code>:</p> <ul> <li>Name changed to <code>containerssh_backend_errors_total</code></li> <li>Unit name change from <code>requests</code> to <code>errors_total</code></li> </ul> </li> <li> <p><code>containerssh_config_server_requests</code>:</p> <ul> <li>Name changed to <code>containerssh_config_server_requests_total</code></li> <li>Unit name change from <code>requests</code> to <code>requests_total</code></li> </ul> </li> <li> <p><code>containerssh_config_server_failures</code>:</p> <ul> <li>Name changed to <code>containerssh_config_server_failures_total</code></li> <li>Unit name change from <code>requests</code> to <code>failures_total</code></li> </ul> </li> <li> <p><code>containerssh_ssh_connections</code>:</p> <ul> <li>Name changed to <code>containerssh_ssh_connections_total</code></li> <li>Unit name change from <code>connections</code> to <code>connections_total</code></li> </ul> </li> <li><code>containerssh_ssh_handshake_successful</code>:<ul> <li>Name changed to <code>containerssh_ssh_successful_handshakes_total</code></li> <li>Unit name change from <code>handshakes</code> to <code>handshakes_total</code></li> </ul> </li> <li><code>containerssh_ssh_handshake_failed</code>:<ul> <li>Name changed to <code>containerssh_ssh_failed_handshakes_total</code></li> <li>Unit name change from <code>handshakes</code> to <code>handshakes_total</code></li> </ul> </li> </ul>"},{"location":"blog/2024/01/07/containerssh-05-everything-but-the-kitchen-sink/#removal-of-the-deprecated-dockerrun-and-kuberun-backends","title":"Removal of the deprecated DockerRun and KubeRun backends","text":"<p>Following the deprecation notice in the previous versions, the dockerrun and kuberun backends have been removed. The updated docker and kubernetes backends should be used instead.</p>"},{"location":"deprecations/","title":"Deprecated and removed ContainerSSH features","text":"<p>This page lists all features that have been removed or deprecated. You can click each one to learn more about how to transition from the deprecated or removed feature.</p> Feature Deprecated Removed publicKeyBase64 in auth protocol \u2014 0.4 sessionId field in auth/config protocol 0.4 \u2014 listen option 0.4 \u2014 dockerrun backend 0.4 \u2014 kuberun backend 0.4 \u2014 Moving from GET to POST in webhooks \u2014 0.4"},{"location":"deprecations/authconfigget/","title":"Moving from GET to POST in webhooks","text":"<p>In ContainerSSH version 0.3.1 and before the authentication and configuration webhooks used the <code>GET</code> HTTP method for sending webhooks due to a mistake. This has been changed to a <code>POST</code> request in ContainerSSH 0.4. If your authentication or configuration server only supports <code>GET</code> please add support for the <code>POST</code> method.</p>"},{"location":"deprecations/dockerrun/","title":"Deprecating the DockerRun backend (since 0.4)","text":"<p>In version <code>0.4</code> ContainerSSH received a generalized Docker backend and we are deprecating the <code>dockerrun</code> backend from version <code>0.3.1</code> and earlier. We are adding this new backend because we are changing several default values to options which could cause security problems if the old configuration was used. Version <code>0.4</code> still includes support for the <code>dockerrun</code> backend, but log a warning when used:</p> <p>You are using the dockerrun backend deprecated since ContainerSSH 0.4. This backend will be removed in the future. Please switch to the new docker backend as soon as possible. See https://containerssh.io/deprecations/dockerrun for details.</p> <p>This page explains how to switch to the new backend.</p>"},{"location":"deprecations/dockerrun/#changing-the-configuration-structure","title":"Changing the configuration structure","text":"<p>The new configuration is structured into 3 components:</p> <pre><code>docker:\n  connection:\n    # These options were on the root level of the dockerrun configuration.\n    host:\n    cacert:\n    cert:\n    key:\n  execution:\n    # These options are moved here from the old dockerrun -&gt; config option.\n    container:\n      # ...\n    host:\n      # ...\n    network:\n      # ...\n    platform:\n      # ...\n    containername: \"\"\n\n    # Subsystems that can be requested.\n    subsystems:\n      sftp: /usr/lib/openssh/sftp-server\n\n    # the \"disableCommand\" option has been removed and is configured in the\n    # \"security\" option.\n\n    # Pick an image pull policy from \"Always\", \"IfNotPresent\" or \"Never\". See below.\n    imagePullPolicy: \"IfNotPresent\"\n\n    # Execution mode, see below.\n    mode: connection\n    # Idle command for the new \"connection\" mode, see below.\n    idleCommand:\n      - \"/bin/sh\"\n      - \"-c\"\n      - \"sleep infinity &amp; PID=$!; trap \\\"kill $PID\\\" INT TERM; wait\"\n    # Shell command  for the new \"connection\" mode, see below.\n    shellCommand:\n      - \"/bin/bash\"\n    # Path to the new ContainerSSH Guest Agent.\n    agentPath: \"/usr/bin/containerssh-agent\"\n    # Disable the ContainerSSH guest agent.\n    disableAgent: true\n\n  timeouts:\n    # This section replaces the dockerrun -&gt; config -&gt; timeout option.\n\n    # Timeout for a container to start.\n    containerStart: 60s\n    # Timeout for a container to stop.\n    containerStop: 60s\n    # Timeout for a shell or command to start.\n    commandStart: 60s\n    # Timeout for HTTP calls\n    http: 15s\n    # Timeout for signal requests\n    signal: 60s\n    # Timeout for window change requests\n    window: 60s\n</code></pre>"},{"location":"deprecations/dockerrun/#the-new-execution-modes","title":"The new execution modes","text":"<p>The new <code>docker</code> backend supports two execution modes: <code>connection</code> or <code>session</code>. The old <code>dockerrun</code> backend worked identical to the <code>session</code> mode, where each command execution within an SSH connection would cause a new container to be started.</p> <p>The new <code>connection</code> mode, on the other hand, starts a container with an idle command from the configuration and then uses the <code>docker exec</code> facility to launch commands.</p> <p>In <code>connection</code> mode the pods are launched with the command specified in <code>docker</code> \u2192 <code>execution</code> \u2192 <code>idleCommand</code> as a command. The purpose of this command is to keep the pod alive and wait for a <code>TERM</code> signal. Any commands (shell, etc.) will be launched similar to how you would use <code>docker exec</code> to run an additional command in the pod. When a shell is requested the <code>docker</code> \u2192 <code>execution</code> \u2192 <code>shellCommand</code> parameter is used.</p> <p>Warning</p> <p>The <code>connection</code> execution mode means that the <code>CMD</code> and <code>ENTRYPOINT</code> settings from the container image or the configuration are ignored. If you are switching from the <code>dockerrun</code> backend and used the <code>CMD</code> as a security measure it is strongly recommended that you configure the <code>idleCommand</code> and <code>shellCommand</code> options properly.</p>"},{"location":"deprecations/dockerrun/#the-guest-agent","title":"The guest agent","text":"<p>ContainerSSH 0.4 also includes support for the new ContainerSSH Guest Agent that enables support for various features the Docker API does not provide, such as sending signals to processes.</p> <p>The agent must be included into the guest image in order to work. When the agent is included it can be configured as follows:</p> <pre><code>docker:\n  execution:\n    # Path to the new ContainerSSH Guest Agent.\n    agentPath: \"/usr/bin/containerssh-agent\"\n    # Disable the ContainerSSH guest agent.\n    disableAgent: true\n</code></pre> <p>Warning</p> <p>The agent is enabled by default, you should explicitly disable it if you want to run an image that doesn't have an integrated agent.</p>"},{"location":"deprecations/dockerrun/#image-pull-policy","title":"Image pull policy","text":"<p>The new <code>docker</code> backend also includes an option when to pull images. This option helps with the Docker Hub rate limits and is built to be similar to the Kubernetes option with the same name.</p> <p>Tip</p> <p>Docker has added ContainerSSH as an Open Source Community Application. Pulls to <code>containerssh/containerssh</code> and the default guest image <code>containerssh/containerssh-guest-image</code> are excluded from the rate limits.</p> <pre><code>docker:\n  execution:\n    # Pick an image pull policy from \"Always\", \"IfNotPresent\" or \"Never\". See below.\n    imagePullPolicy: \"IfNotPresent\"\n</code></pre> <p>The following options are supported:</p> <code>Always</code> Always pulls images. This is the same behavior as the <code>dockerrun</code> backend. <code>IfNotPresent</code> Pull image if it is not locally present, has no image tag, or has the <code>:latest</code> tag. <code>Never</code> Never pulls the image. If the image is not locally present the execution will fail."},{"location":"deprecations/dockerrun/#removing-the-disablecommand-option","title":"Removing the <code>disableCommand</code> option","text":"<p>The <code>disableCommand</code> option was added to ContainerSSH to prevent connecting users to run a custom application. This filled a similar role to the <code>ForceCommand</code> option in OpenSSH: it prevented connecting users to launch custom commands.</p> <p>However, this command was separately implemented in the <code>kuberun</code> and in the <code>dockerrun</code> backend. This was not maintainable, so it was moved into the <code>security</code> module and can be configured as follows:</p> <pre><code>security:\n  command:\n    mode: disable\n</code></pre>"},{"location":"deprecations/kuberun/","title":"Deprecating the KubeRun backend (since 0.4)","text":"<p>In version <code>0.4</code> ContainerSSH received a generalized Kubernetes backend and we are deprecating the <code>kuberun</code> backend from version <code>0.3.1</code> and earlier. We are adding this new backend because we are changing several default values to options which could cause security problems if the old configuration was used. Version <code>0.4</code> still includes support for the <code>kuberun</code> backend, but logs a warning when used:</p> <p>You are using the kuberun backend deprecated since ContainerSSH 0.4. This backend will be removed in the future. Please switch to the new docker backend as soon as possible. See https://containerssh.io/deprecations/kuberun for details.</p> <p>This page explains how to switch to the new backend.</p>"},{"location":"deprecations/kuberun/#changing-the-configuration-structure","title":"Changing the configuration structure","text":"<p>The new configuration structure is very similar to the old <code>kuberun</code> structure. The most important change is the relocated and more detailed timeouts section:  </p> <pre><code>kubernetes:\n  timeouts:\n    # Timeout for a container to start.\n    podStart: 60s\n    # Timeout for a container to stop.\n    podStop: 60s\n    # Timeout for a shell or command to start.\n    commandStart: 60s\n    # Timeout for HTTP calls\n    http: 15s\n    # Timeout for signal requests\n    signal: 60s\n    # Timeout for window change requests\n    window: 60s\n</code></pre> <p>This replaces the old <code>kubernetes</code> \u2192 <code>connection</code> \u2192 <code>timeout</code> option.</p> <p>The configuration now also moves the <code>kubernetes</code> \u2192 <code>pod</code> \u2192 <code>namespace</code> option to the new <code>metadata</code> section, which is can now be fully customized with Kubernetes pod metadata. The <code>podSpec</code> option was renamed <code>spec</code> to align with Kubernetes:</p> <pre><code>kubernetes:\n  pod:\n    metadata:\n      namespace: default\n      generateName: myPodNamePrefix-\n      labels:\n        foo: bar\n  # Rename podSpec\n  spec:\n</code></pre> <p>Please run <code>kubectl explain pod.metadata</code> for the full list of options.</p>"},{"location":"deprecations/kuberun/#the-new-execution-modes","title":"The new execution modes","text":"<p>The new <code>kubernetes</code> backend supports two execution modes: <code>connection</code> or <code>session</code>. The old <code>kuberun</code> backend worked identical to the <code>session</code> mode, where each command execution within an SSH connection would cause a new container to be started.</p> <p>The new <code>connection</code> mode, on the other hand, starts a container with an idle command from the configuration and then uses the <code>exec</code> facility to launch commands.</p> <p>In <code>connection</code> mode the pods are launched with the command specified in <code>kubernetes</code> \u2192 <code>pod</code> \u2192 <code>idleCommand</code> as a command. The purpose of this command is to keep the pod alive and wait for a <code>TERM</code> signal. Any commands (shell, etc.) will be launched similar to how you would use <code>kubectl exec</code> to run an additional command in the pod. When a shell is requested the <code>kubernetes</code> \u2192 <code>pod</code> \u2192 <code>shellCommand</code> parameter is used.</p> <p>Warning</p> <p>The <code>connection</code> execution mode means that the <code>CMD</code> and <code>ENTRYPOINT</code> settings from the container image or the configuration are ignored. If you are switching from the <code>kuberun</code> backend and used the <code>CMD</code> as a security measure it is strongly recommended that you configure the <code>idleCommand</code> and <code>shellCommand</code> options properly.</p>"},{"location":"deprecations/kuberun/#the-guest-agent","title":"The guest agent","text":"<p>ContainerSSH 0.4 also includes support for the new ContainerSSH Guest Agent that enables several features the Kubernetes API does not support support. For example, the guest agent enables waiting for ContainerSSH to attach to the process in <code>session</code> mode before starting the desired program. It is strongly recommended to enable the guest agent for Kubernetes as the API misses several features required for proper operations.</p> <p>The agent must be included into the guest image in order to work. When the agent is included it can be configured as follows:</p> <pre><code>kubernetes:\n  pod:\n    # Path to the new ContainerSSH Guest Agent.\n    agentPath: \"/usr/bin/containerssh-agent\"\n    # Disable the ContainerSSH guest agent.\n    disableAgent: true\n</code></pre> <p>Warning</p> <p>The agent is enabled by default, you should explicitly disable it if you want to run an image that doesn't have an integrated agent.</p>"},{"location":"deprecations/kuberun/#removing-the-disablecommand-option","title":"Removing the <code>disableCommand</code> option","text":"<p>The <code>disableCommand</code> option was added to ContainerSSH to prevent connecting users to run a custom application. This filled a similar role to the <code>ForceCommand</code> option in OpenSSH: it prevented connecting users to launch custom commands.</p> <p>However, this command was separately implemented in the <code>kuberun</code> and in the <code>dockerrun</code> backend. This was not maintainable, so it was moved into the <code>security</code> module and can be configured as follows:</p> <pre><code>security:\n  command:\n    mode: disable\n</code></pre>"},{"location":"deprecations/kuberun/#removing-the-insecure-option","title":"Removing the <code>insecure</code> option","text":"<p>We are also removing the <code>insecure</code> option from the connection configuration and no longer support connecting a Kubernetes cluster without certificate verification. Using the <code>insecure</code> option represents the worst practices in terms of security. If you are using it, please set up a proper CA infrastructure.</p>"},{"location":"deprecations/listen/","title":"Moving the ContainerSSH <code>listen</code> option (since 0.4)","text":"<p>In ContainerSSH 0.4 we are introducing a framework to run multiple services within one daemon. In the future we want to add more services like a web-based interface.</p> <p>To make this change happen we will stop treating the SSH service as special, so we are moving the <code>listen</code> option from the configuration root to <code>ssh</code> \u2192 <code>listen</code>:</p> <pre><code># Deprecated version\nlisten: 0.0.0.0:2222\n# New version\nssh:\n  listen: 0.0.0.0:2222\n</code></pre> <p>If you use the old option you will receive the following log warning:</p> <p>You are using the 'listen' option deprecated in ContainerSSH 0.4. Please use the new 'ssh -&gt; listen' option. See https://containerssh.io/deprecations/listen for details.</p> <p>If you provide both options the new option will take precedence and you will receive the following log message:</p> <p>You are using the 'listen' option deprecated in ContainerSSH 0.4 as well as the new 'ssh -&gt; listen' option. The new option takes precedence. Please see https://containerssh.io/deprecations/listen for details.</p>"},{"location":"deprecations/publicKeyBase64/","title":"publicKeyBase64","text":"Deprecating the publicKeyBase64 field in the authentication protocol (since 0.4) <p>Before ContainerSSH version 0.4 sent a field called <code>publicKeyBase64</code> to the authentication server which contained the SSH key in the binary OpenSSH wire format. However, this was not easy to integrate, so ContainerSSH 0.4 adds a field called <code>publicKey</code> containing the public key in the OpenSSH authorized keys format.</p> <p>The <code>publicKeyBase64</code> field is now removed because it was never useful. Authentication server implementations should switch to using the <code>publicKey</code> field.</p>"},{"location":"deprecations/sessionId/","title":"sessionId","text":"Deprecating the sessionId field in the authentication and configuration server protocol (since 0.4) <p>Before ContainerSSH 0.4 both the authentication server protocol and the configuration server protocol contained a field called <code>sessionId</code> which would include a self-generated ID for the session.</p> <p>In ContainerSSH 0.4 we are introducing a new ID called <code>connectionId</code> which uniquely identifies the SSH connection across all ContainerSSH-related platforms (auth/config/audit log/etc).</p> <p>The <code>sessionId</code> field is now deprecated and mirrors the contents of <code>connectionId</code>. The field will be removed in the future and auth / config server implementers should no longer rely on it.</p>"},{"location":"development/","title":"Developing ContainerSSH","text":"Developing ContainerSSH <p>Welcome! And a big thank you for wanting to contribute! This page will explain how to get started with contributing to ContainerSSH. This guide will help you through the basics of getting development up and running.</p> <p>If you have any questions regarding ContainerSSH, you may want to join the ContainerSSH Slack.</p> Getting started <p>This quick guide will help you hop into ContainerSSH development on the quick.</p> <p>Read more \u00bb</p> Setting up your development environment <p>This guide walks you though the steps needed to set up a development environment, from Git, through Goland and GPG, to the IDE.</p> <p>Read more \u00bb</p> Dashboard <p>Our development dashboard shows you the libraries, issues, and pull requests relevant to ContainerSSH development.</p> <p>Open Dashboard \u00bb</p> Understanding ContainerSSH <p>This document describes the concepts around SSH and how ContainerSSH is built internally.</p> <p>Read more \u00bb</p>"},{"location":"development/code-style/","title":"Coding style","text":"Coding style <p>We don't have a strict coding convention that will force you to write code in a very specific way. Instead, we will try to explain how we think about ensuring quality in this document.</p> <p>Please, feel free to bring your own ideas and discuss on the discussions board.</p>"},{"location":"development/code-style/#object-oriented-programming","title":"Object-Oriented Programming","text":"<p>Wait, what? OOP in Go?</p> <p>Go has a concept called receivers that allow you to pass a context structure to a function. This is very similar to how private and public variables are handled in OOP languages.</p> <p>The main benefit of receivers is that they can be used to implement interfaces. Interfaces, in turn, give us the ability to create a standardized API between components without involving a network.</p> <p>The log library, for example, provides the <code>Logger</code> interface and then also includes an implementation for the logger. However, at no point do we have a hard dependency on the actual implementation of the logger. If in the future the implementation turns out to be insufficient replacing it is easy.</p> <p>We use this pattern extensively to separate the ContainerSSH libraries from each other. We are then using these interfaces to write tests for each library without having to run an end-to-end test for every test.</p>"},{"location":"development/code-style/#testing","title":"Testing","text":"<p>This brings us to the topic of testing. ContainerSSH is a security-relevant software so we want to ensure a reasonable level of quality. In the beginning we had a manual testing protocol, but as features became more extensive it became very hard to test each feature for each release.</p> <p>We also rely on GitHub's Dependabot to update our external dependencies. Without tests we would have a very hard time verifying that the updated third party library did not break something.</p> <p>When it comes to test sizes we prefer having unit- or component-level tests and only have a few end-to-end tests. This is because e2e tests require several Kubernetes clusters and a Docker server so they are quite slow and hard to run in a development environment. We want to make sure that contributors can avoid the frustrating cycle of Commit, Push, Wait for CI, Realize it breaks, Repeat, so running tests quickly is very desirable. End-to-end tests also have the drawback that if they break the bug can be hard to track down.</p> <p>In summary, we prefer having granular tests for each library. This is why we have split the codebase into several libraries on GitHub. Each library has their own tests and own CI setup. When a library needs to interact with a different library we usually implement an interface with a well-described contract. This contract can then be used to write tests against.</p> <p>When it comes to actually writing the tests we follow the Detroit/classicist school of testing. Our tests are put in the separate <code>_test</code> package and test our code from the outside.</p>"},{"location":"development/code-style/#structuring-your-code","title":"Structuring your code","text":"<p>In the early versions of ContainerSSH we had a rather monolithic application. The core SSH server would perform logging, write metrics, deal with SSH specifics, etc. Writing and maintaining the code became very tedious. It took a a large amount of concentration to find the right parts to implement a change on, and finding bugs often took a slog through layers and layers of code.</p> <p>This is frustrating and hinders productivity. We don't want contributors to spend more time finding the right code piece than implementing the actual change. This requires a short-term sacrifice: better code structure and abstractions. Yes, we know, they are not fun to implement. When we refactored ContainerSSH in version 0.4 the size of the codebase grew by over 50%. However, this change was worth it as it paved the way for adding new features without pain in the future.</p> <p>Our aim is that each library or component should deal with one concern. The auth library should deal with authentication, the sshserver library with SSH, and so on. This goes so far that the integration work between two libraries is often relegated to a separate library. Sticking with the example before, the authintegration library creates a layer for the SSH server and calls the authentication library when user authentication is desired.</p> <p>There is no hard and fast rule what (not) to separate. Creating a prototype as a single library is fine. If it turns out that it is too unwieldy to test or use it can be refactored. Thankfully, we have no quarterly deadlines we need to hit, so a feature is released when it is ready.  </p>"},{"location":"development/code-style/#third-party-libraries","title":"Third party libraries","text":"<p>We group third party dependencies in two categories: primary and utility. Primary dependencies are the ones that are required to fulfil the primary function of a library. For example, the Docker libraries would be a primary dependency for the dockerrun library. These libraries are integrated directly. Needless to say, the libraries include component-level tests to verify the integration still works.</p> <p>This stands in contrast to utility libraries. For example, we use Yuki Iwanaga's defaults library to provide default values for structs in multiple ContainerSSH libraries. However, since the library may need to be replaced in the future we opt to create a wrapping layer called structutils. This wrapping layer describes our expectation towards the library and also includes tests to verify  that this functionality still holds true.</p>"},{"location":"development/code-style/#dealing-with-networks","title":"Dealing with networks","text":"<p>ContainerSSH integrates several components that can be reached over the network, for example the config server, the auth server, or even Docker and Kubernetes. While in the development environment everything typically works fine, they can be notoriously unreliable in production. </p> <p>What's worse, these issues are extremely hard to debug, so we aim to prevent them. Our two choices of prevention are contexts and retries.</p> <p>Contexts in Go provide a graceful way to observe timeouts. The simplest way to create a timeout context is the following:</p> <pre><code>ctx, cancelFunc := context.WithTimeout(\n    context.Background,\n    60 * time.Second,\n)\ndefer cancelFunc()\n</code></pre> <p>Warning</p> <p>It is very important that you include the call to <code>cancelFunc()</code> otherwise you may leak memory.</p> <p>Now that you have a context you can check it inside a loop:</p> <pre><code>loop:\nfor {\n    select {\n    case &lt;-ctx.Done():\n        break loop\n    default:\n        //Continue whatever you need to do\n    }\n}\n</code></pre> <p>Retries also come into play: when performing a call over the network you may encounter random errors you may wish to retry. We frequently couple the context with retries:</p> <pre><code>var lastError error\nloop:\nfor {\n    lastError = someNetworkCall()\n    if err == nil {\n        break loop\n    } else {\n        logger.Warningf(\n            \"failed to perform network call, retrying in 10 seconds (%v)\",\n            lastError,\n        )\n    }\n    select {\n    case &lt;-ctx.Done():\n        break loop\n    case &lt;-time.After(10 * time.Second):\n       // Next loop\n    }\n}\nif lastError != nil {\n    logger.Errorf(\"failed to perform network call, giving up (%v)\", err)\n    return lastError\n}\n</code></pre>"},{"location":"development/code-style/#microserviecs","title":"Microserviecs","text":"<p>The above-mentioned networks also factor in the concept of microservices. ContainerSSH uses two external services for authentication and configuration. These are provided for user convenience making it easier to integrate ContainerSSH. However, we do not plan to add more microservices for development convenience. We want to avoid having more deployment YAML files than actual code. ContainerSSH should be simple to run, even if that means making it harder to structure the code.</p>"},{"location":"development/code-style/#conclusion","title":"Conclusion","text":"<p>We hope you now have a better idea of the design goals of ContainerSSH. However, it is worth reiterating: there is room for disagreement. If in doubt, feel free to submit a simple pull request and we'll work from there. If your solution is missing bits we'll work with you or even add missing code pieces to come to an agreeable solution.</p>"},{"location":"development/dashboard/","title":"Development Dashboard","text":"Development Dashboard RoadmapRepositoriesIssuesPull RequestsDependency updates Repository Description Version ContainerSSH Main repo Repository Title Milestone Created ContainerSSH Test issue {{ no such element: None['title'] }} Repository Title Created Mergeable Checks No pull requests open. Repository Title Created Mergeable Checks No dependency updates open."},{"location":"development/getting-started/","title":"Getting started with ContainerSSH development","text":"Getting started with ContainerSSH development <p>Welcome to developing ContainerSSH! For the purposes of this guide we will assume you have your development environment set up and ready to go. If not, please follow our handy guide to do just that.</p> <p>Ready? Good.</p>"},{"location":"development/getting-started/#cloning-the-repository","title":"Cloning the repository","text":"<p>Before we begin you will have to decide what you want to do. If you just want to get ContainerSSH running to get the big picture you will need to clone the ContainerSSH/ContainerSSH repository. This contains the main ContainerSSH executable, as well as the Auth-Config server used for testing:</p> <pre><code>git clone https://github.com/containerssh/containerssh\n</code></pre> <p>However, ContainerSSH is built in a highly modular fashion so you may need to change a specific library. You can find the list of libraries on our development dashboard. This dashboard contains an overview of all repositories, issues, pull requests, and everything else you will need to find your way around the codebase.</p> <p>Each repository contains a readme explaining how to use that specific component. If you find the readme not helpful please open an issue on that repository asking for more information.</p> <p>If you find yourself needing a new repository because you want to develop something completely new please file a pull request against the github-terraform repository.</p> <p>Tip</p> <p>For the best results we recommend cloning the ContainerSSH repos into <code>/path/to/your/home/go/src/github.com/containerssh/REPONAME</code>.</p>"},{"location":"development/getting-started/#running-containerssh","title":"Running ContainerSSH","text":"<p>Running ContainerSSH is simple. You will need a clone of the main ContainerSSH repository. Then you have to run two commands.</p> <p>First, the auth-config server needs to be run from the <code>cmd/containerssh-testauthconfigserver</code> directory:</p> <pre><code>go run .\n</code></pre> <p>When that's running create the <code>cmd/containerssh/config.yaml</code> file with the following content:</p> <pre><code>---\nlog:\n  level: debug\nssh:\n  hostkeys:\n    - ssh_host_rsa_key\nbackend: docker\nauth:\n  url: \"http://127.0.0.1:8080\"\n  pubkey: false\nconfigserver:\n  url: \"http://127.0.0.1:8080/config\"\n</code></pre> <p>Now copy the <code>ssh_host_rsa_key</code> file from the <code>example</code> folder and then run ContainerSSH from the <code>cmd/containerssh</code> folder:</p> <pre><code>go run . --config config.yaml\n</code></pre> <p>That's it! Now you have a running ContainerSSH you can connect to on port 2222:</p> <pre><code>ssh foo@localhost -p 2222\n</code></pre>"},{"location":"development/getting-started/#running-the-tests","title":"Running the tests","text":"<p>There are two types of tests for ContainerSSH: end to end tests and component-level tests. Both can be run using the following command from each library's main folder:</p> <pre><code>go test ./...\n</code></pre> <p>Tip</p> <p>Some tests require a working Docker or Kubernetes backend. Make sure that your Docker socket is running on your platform default and your Kubernetes configuration is available in the <code>.kube/config</code> file in your home directory as the tests will use these to connect to.</p>"},{"location":"development/getting-started/#submitting-a-pull-requests","title":"Submitting a pull requests","text":"<p>Once you are done with your development you should fork the repository on GitHub and create a pull request. This pull request will automatically be tested by the CI system. Feel free to keep working on your PR until you are happy with it.</p>"},{"location":"development/getting-started/#understanding-containerssh","title":"Understanding ContainerSSH","text":"<p>ContainerSSH is a reasonably complex piece of software. It uses the built-in Go SSH library to create a server and the client libraries for Docker and Kubernetes to forward the data from the SSH channel to the standard input and output of the container.</p> <p>We have dedicated a whole section to understanding how SSH and ContainerSSH in particular work.</p>"},{"location":"development/containerssh/","title":"Understanding ContainerSSH","text":"Understanding ContainerSSH <p>ContainerSSH is an SSH server that talks to external APIs such as Docker or Kubernetes. This section will explain how ContainerSSH is built.</p>"},{"location":"development/containerssh/#understanding-ssh","title":"Understanding SSH","text":"<p>We don't really think about SSH all that much. Open PuTTY, or your terminal, SSH into a server, and merrily type commands issued to a server running a distance away. Except if you need to write an SSH server. This section will discuss the concepts you need to work on ContainerSSH.</p> <p>Read more \u00bb</p>"},{"location":"development/containerssh/#your-first-ssh-server","title":"Your first SSH server","text":"<p>ContainerSSH may be complex, so let's start simple: let's implement a very simple SSH server in Go that talks to the Docker backend.</p> <p>Read more \u00bb</p>"},{"location":"development/containerssh/#internal-architecture","title":"Internal Architecture","text":"<p>ContainerSSH is a project of several thousand lines of code so overview is critical. Our internal architecture document describes what the moving parts of ContainerSSH are.</p> <p>Read more \u00bb</p>"},{"location":"development/containerssh/first-ssh-server/","title":"Implementing your first SSH server","text":"Implementing your first SSH server <p>This section will guide you through implementing your first SSH server in go and combine it with Docker.</p> <p>Tip</p> <p>If you are new to SSH development please read our Understanding SSH guide first.</p> <p>Tip</p> <p>The source code for this mini project is available on GitHub.</p>"},{"location":"development/containerssh/first-ssh-server/#step-1-the-basic-loop","title":"Step 1: The basic loop","text":"<p>Let's start off easy: implementing a TCP server. On *NIX systems listen sockets can be started using the <code>listen()</code> system call and Go follows that pattern nicely:</p> <pre><code>listener, err := net.Listen(\"tcp\", \"0.0.0.0:2222\")\n</code></pre> <p>However, <code>net.Listen</code> does not accept connections, it merely opens a listen socket telling the system kernel that it should not reject connections coming to the specified port.</p> <p>Now we need to accept any incoming connections. Let's do that:</p> <pre><code>tcpConn, err := listener.Accept()\n</code></pre> <p>This call will block until a client connects or the listen socket is closed. Let's ignore the second case and focus on the first. With <code>tcpConn</code> we now have an open plain text TCP connection. We can read from it, we can write to it, but until we call <code>listener.Accept()</code> again we won't get any new connections. So let's put it in a loop:</p> <pre><code>for {\n    tcpConn, err := listener.Accept()\n}\n</code></pre> <p>Cool, so now we can accept multiple connections! However, these are still just plain text connections, so let's make them into an SSH connection:</p> <pre><code>sshConn, chans, reqs, err := ssh.NewServerConn(tcpConn, sshConfig)\n</code></pre> <p>We won't go into the details of <code>sshConfig</code> here, let's focus on the returned variables instead. The first returned variable, <code>sshConn</code> is the raw SSH connection. If you use an IDE you can use code completion to figure out some useful methods it contains, for example for closing the connection.</p> <p>More interesting to us are the <code>chans</code> and <code>reqs</code> variables, however. The <code>chans</code> variable contains a Go channel containing SSH channel request. When a client wants to open a new channel we can read from this Go channel and process the request. (Confusing, we know, two things with the same name.)</p> <p>The <code>reqs</code> variable is also a Go channel, but it contains global requests. We won't deal with these now, so let's disregard these completely:</p> <pre><code>go ssh.DiscardRequests(reqs)\n</code></pre> <p>As you can see we used the <code>go</code> keyword. This is running the method called in a goroutine. If you are coming from another programming language you can imagine these as multi-threaded coroutines. Suffice it to say, they won't block our main loop.</p> <p>Back to the <code>chans</code>, let's deal with them too. Let's handle them in a method called <code>handleChannels</code>:</p> <pre><code>go handleChannels(sshConn, chans)\n</code></pre> <p>This method will be rather simple:</p> <pre><code>func handleChannels(conn *ssh.ServerConn, chans &lt;-chan ssh.NewChannel) {\n    for newChannel := range chans {\n        go handleChannel(conn, newChannel)\n    }\n}\n</code></pre> <p>For each new channel we open yet another goroutine. Fear not, goroutines are very cheap in Go.</p> <p>Let's deal with that channel:</p> <pre><code>func handleChannel(conn *ssh.ServerConn, newChannel ssh.NewChannel) {\n    if t := newChannel.ChannelType(); t != \"session\" {\n        _ = newChannel.Reject(ssh.UnknownChannelType, fmt.Sprintf(\"unknown channel type: %s\", t))\n        return\n    }\n    channel, requests, err := newChannel.Accept()\n    //...\n}\n</code></pre> <p>So far so good, we reject all non-session channels and otherwise accept. The <code>channel</code> contains the reference to the channel, which is also an <code>io.Reader</code> and an <code>io.Writer</code> for <code>stdin</code> and <code>stdout</code>. The <code>requests</code> variable is a go channel containing SSH channel-specific requests.</p> <p>Now, let's use Docker as our backend. It's simple and it's really well documented. On a *NIX system we can create a Docker client like this:</p> <pre><code>docker, err := client.NewClient(\n    \"unix:///var/run/docker.sock\",\n    nil,\n    make(map[string]string),\n)\n</code></pre> <p>Now we can loop over the requests and handle them, one by one:</p> <pre><code>for req := range requests {\n    reply := func(success bool, message []byte) {\n        if req.WantReply {\n            err := req.Reply(success, message)\n            if err != nil {\n                closeConnections()\n            }\n        }\n    }\n    handleRequest(\n        //...\n    )\n}\n</code></pre> <p>As you can see, the requests may need a reply, so we are constructing a simplified function to send a reply back to the SSH client.</p> <p>For the final piece of our puzzle, let's implement the <code>handleRequest</code> method. For simplicity let's implement a switch-case:</p> <pre><code>switch req.Type {\n    case \"env\":\n        // Save environment variables for later use\n    case \"pty-req\":\n        // Set the TTY flag on the Docker client to true later\n    case \"window-change\":\n        // Use the ContainerResize method on the Docker client later\n    case \"shell\":\n        // Create a container and run it\n    case \"exec\":\n        // Create a container and run it\n}\n</code></pre> <p>That's it! You can find the details on how to run a container in our highly simplified minicontainerssh example. We have skipped many parts like error handling, but it should give you a good overview of how an SSH server in Go works and how it interacts with the container backend.</p> <p>Now you are ready to dive into the internal architecture of ContainerSSH. </p>"},{"location":"development/containerssh/internal-architecture/","title":"Internal Architecture","text":"Internal Architecture <p>ContainerSSH is build as a collection of libraries, each of which is developed independently to ensure quality, but to the purpose of being integrated into what is ContainerSSH.</p> <p>The core architecture consists of several services, such as SSH or the metrics server. These services are started from the core code as part of a service pool. If any one service fails the service pool shuts down.</p> <p>One of the core services is the SSH service, which creates a standardized, object oriented layer to deal with connecting SSH clients. It also abstracts away the complexities of SSH and the Go SSH library. This library defines a set of interfaces that backends need to implement.</p> <p>The SSH backends are then added in layers. One of the most fundamental layers is auditlogintegration, which captures decoded SSH traffic and forwards it to the audit log library.</p> <p>The other critical layer is authintegration, which forwards authentication requests to the authentication library.</p> <p>The final piece of the puzzle is the backend library which acts as a hub. As a first step it calls the configuration library to obtain dynamic, per-user configuration. It then proceeds to load the security layer and the appropriate backend, e.g. Docker or Kubernetes.</p> <p>These backends form the lowermost layer of the SSH handler stack and forward the connections to the container backend.</p> <p></p>"},{"location":"development/containerssh/internal-architecture/#module-dependency-map","title":"Module dependency map","text":"<p>The following graph shows the internal dependencies of ContainerSSH. This is important to know the order in which modules must be updated:</p> <p></p>"},{"location":"development/containerssh/ssh/","title":"Understanding SSH","text":"Understanding SSH <p>Let's face it: we don't think about SSH all that much. We SSH into a server and merrily type away our commands. Until we need to write an SSH server.</p> <p>This document describes the high level concepts of SSH: how do you open a connection, what are channels, and how do requests work.</p> <p>This is a very high level overview, but should contain everything you need to get started with ContainerSSH development.</p>"},{"location":"development/containerssh/ssh/#handshake","title":"Handshake","text":"<p>When the user connects an SSH server the SSH keys are verified. We won't discuss this here as for ContainerSSH the Go SSH library takes care of that.</p> <p>The first thing we are concerned with is authentication. Authentication is described by RFC 4252 and it states the following:</p> <p>The server drives the authentication by telling the client which authentication methods can be used to continue the exchange at any given time.  The client has the freedom to try the methods listed by the server in any order.</p> <p>In other words, when the user connects the SSH server tells the client which authentication method it supports. The client picks one of them and performs the authentication. The server can then decide to reject, allow, or show the client another list of methods (e.g. to perform two factor authentication). The Go library vastly simplifies this process and only allows a single means of authentication for each connection.</p> <p>Each authentication request contains a username. The username may change between authentication attempts to authenticate against different systems, but this is not customary.</p>"},{"location":"development/containerssh/ssh/#connection","title":"Connection","text":"<p>Once the authentication is complete the connection is open and both the client and the server may now send two types of messages: global requests and channels.</p> <p>Global requests describe requests in either direction that one party wants from the other. For example, the OpenSSH extensions describe the <code>no-more-sessions@openssh.com</code> to indicate that no more session channels should be opened on this connection.</p> <p>The channels, on the other hand are means of transporting data. For example, the <code>session</code> channel is responsible for executing a program and then transporting the standard input, output, and error data streams to and from the program. They also give both ends the ability to send channel-specific requests (e.g. setting environment variables, resizing the window, etc.).</p>"},{"location":"development/containerssh/ssh/#session-channels","title":"Session channels","text":"<p>While there are theoretically other types of channels possible, we currently only support <code>session</code> channels. The client can request channels to be opened at any time.</p> <p>We currently support the following requests on the <code>session</code> channel. These are described in RFC 4254.</p> <code>env</code> Sets an environment variable for the soon to be executed program. <code>pty</code> Requests an interactive terminal for user input. <code>shell</code> Requests the default shell to be executed. <code>exec</code> Requests a specific program to be executed. <code>subsystem</code> Requests a well-known subsystem (e.g. <code>sftp</code>) to be executed. <code>window-change</code> Informs the server that an interactive terminal window has changed size. This is only sent once the program has been started with the requests above. <code>signal</code> Requests the server to send a signal to the currently running process. <p>In addition, we also send an <code>exit-status</code> request to the client from the server when the program exits to inform the client of the exit code.</p>"},{"location":"development/containerssh/ssh/#interactive-terminals","title":"Interactive terminals","text":"<p>As you can see above, the user can request an interactive terminal using the <code>pty</code> request. This is done automatically by SSH clients if they detect that their input is an interactive terminal.</p> <p>Using interactive terminals changes the operation mode of stdin, stdout, and stderr. While programs normally write their standard output to <code>stdout</code> and their error output to <code>stderr</code>, programs running in interactive mode send their combined output to <code>stdout</code> using a special framing. (TTY multiplexing)</p> <p>Thankfully, we don't need to know too much about TTY multiplexing for writing an SSH server since it is transparently passed through from the container engine to the SSH channel and we don't interact with it.</p>"},{"location":"development/containerssh/ssh/#rfcs","title":"RFCs","text":"<p>The SSH protocol is governed by the following RFCs:</p> RFC 913: Simple File Transfer Protocol This document describes the SFTP protocol used over SSH. RFC 4250: The Secure Shell (SSH) Protocol Assigned Numbers This document describes the protocol numbers and standard constants used in SSH. RFC 4251: The Secure Shell (SSH) Protocol Architecture This document describes the design decisions taken to work with SSH. RFC 4252: The Secure Shell (SSH) Authentication Protocol This document describes how user authentication works in SSH. RFC 4253: The Secure Shell (SSH) Transport Layer Protocol This document describes the details of how data is transported over SSH. RFC 4254: The Secure Shell (SSH) Connection Protocol This document contains the parts most interesting to us: how channels, sessions, etc. work. RFC 4255: Using DNS to Securely Publish Secure Shell (SSH) Key Fingerprints This document describes how to publish SSH fingerprints using DNS. It has not seen wide adoption. RFC 4256: Generic Message Exchange Authentication for the Secure Shell Protocol (SSH) This document describes the keyboard-interactive authentication for SSH, which is often used for two factor authentication. RFC 4335: The Secure Shell (SSH) Session Channel Break Extension This document describes the telnet-compatible break request for use in SSH. RFC 4344, RFC 4345, RFC 4419, RFC 4432 These documents describe various encryption-related topics. RFC 4462: Generic Security Service Application Program Interface (GSS-API) Authentication and Key Exchange for the Secure Shell (SSH) Protocol This document describes the GSS-API authentication method that can be used to authenticate with a Kerberos ticket. RFC 4716: The Secure Shell (SSH) Public Key File Format This document describes the PEM-like format to store SSH keys in. RFC 4819: Secure Shell Public Key Subsystem This document describes the SSH public key subsystem usable for adding, removing, and listing public keys. RFC 5647, RFC 5656, RFC 6187, RFC 6239, RFC 6594, RFC 6668 These documents describe various cryptography and authentication related topics. RFC 7479: Using Ed25519 in SSHFP Resource Records This document describes publishing ED25519 host keys using DNS. RFC 5592: Secure Shell Transport Model for the Simple Network Management Protocol (SNMP) This protocol describes using SNMP over SSH. RFC 6242: Using the NETCONF Protocol over Secure Shell (SSH) This document describes transporting the RFC 6241 Network Configuration Protocol over SSH. This can be used to manage networking equipment. <p>In addition, OpenSSH defines the following extensions:</p> The OpenSSH Protocol This document describes new cryptographic methods, tunnel forwarding, domain socket forwarding, and many more changes. The CertKeys Document This document describes the OpenSSH CA method. SSH Agent Protocol Describes the protocol used by the SSH agent holding the SSH keys in escrow."},{"location":"development/devenv/","title":"Setting up your development environment","text":"Setting up your development environment <p>Welcome! This guide will help you set up your development environment for writing ContainerSSH code. We recommend to following this guide step by step, even when you have already set up some of them yourself.</p>"},{"location":"development/devenv/#step-1-create-a-github-account","title":"Step 1: Create a GitHub account","text":"<p>ContainerSSH development is exclusively handled on GitHub. In order to send code or website contributions you will need to create a GitHub account. Once you have an account we also recommend setting up two-factor authentication.</p>"},{"location":"development/devenv/#step-2-installing-git","title":"Step 2: Installing Git","text":"<p>Unless you plan to develop exclusively on the GitHub web interface you will also need to install Git on your computer. We support development on Windows, Linux, and MacOS, feel free to use any of those operating systems. Please follow the GitHub guide to install Git on your operating system.</p>"},{"location":"development/devenv/#step-3-creating-a-gpg-key","title":"Step 3: Creating a GPG key","text":"<p>Git is a distributed versioning system and you can make commits in the name of others. In order to verify committer identity (for both security and licencing purposes) we require all commits to be signed using GPG.</p> <p>Please follow our GPG for Git guide to enable code signing on your machine..</p>"},{"location":"development/devenv/#step-4-installing-golang","title":"Step 4: Installing Golang","text":"<p>To compile the code you will need Golang. We have a guide to install Golang on various platforms.</p>"},{"location":"development/devenv/#step-5-installing-the-qa-tools","title":"Step 5: Installing the QA tools","text":"<p>To make sure there are no latent errors are creeping in we are using some QA tools you will need.</p>"},{"location":"development/devenv/#step-6-installing-docker","title":"Step 6: Installing Docker","text":"<p>The <code>dockerrun</code> backend requires Docker to be installed. Please install Docker to develop against.</p>"},{"location":"development/devenv/#step-7-installing-kubernetes","title":"Step 7: Installing Kubernetes","text":"<p>The <code>kuberun</code> backend requires Kubernetes to be installed. Please install a lightweight Kubernetes to develop against.</p>"},{"location":"development/devenv/#step-8-setting-up-your-ide","title":"Step 8: Setting up your IDE","text":"<p>We have a guide to set up VSCode and Goland as your IDE.</p>"},{"location":"development/devenv/#step-9-website","title":"Step 9: Website","text":"<p>This website requires a Python to run locally. This guide explains the details of setting it up.</p>"},{"location":"development/devenv/docker/","title":"Installing Docker","text":"Installing Docker <p>The <code>dockerrun</code> backend requires Docker to create containers. You will need this if you test against this backend.</p> Windows / MacOSLinux <p>As a simple way to get Docker running we recommend installing Docker Desktop.</p> <p>Docker provides convenience scripts to install Docker on Linux:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> <p>Alternatively, please follow the manual installation steps to get Docker running.</p>"},{"location":"development/devenv/golang/","title":"Installing Golang","text":"Installing Golang <p>While there is an official doc on installing Golang it is less than helpful for Linux users. We are attempting to collect the best practices in installing Golang for beginners here.</p> <p>Tip</p> <p>If you are using Goland as an IDE you can skip this step. Goland downloads the Go compiler for you.</p> Linux / WSLWindowsMacOS <p>Ubuntu 20.04</p> <p>On Ubuntu 20.04 you can install Go directly from the package manager: <pre><code>apt update\napt install golang-1.14\n</code></pre></p> <p>RHEL/CentOS <pre><code>yum install golang-bin\n</code></pre></p> <p>Fedora <pre><code>dnf install golang-bin\n</code></pre></p> <p>Gentoo Linux</p> <pre><code>emerge --ask dev-lang/go\n</code></pre> <p>Other / non-administrator</p> <ol> <li>Download the Linux .tar.gz.</li> <li>Extract the archive into a directory.</li> <li>Add the following section to your <code>~/.profile</code>, <code>~/.zshrc</code>, <code>~/.bashrc</code>, or <code>~/.bash_profile</code>, depending on your shell, then restart your terminal:</li> </ol> <pre><code>export PATH=$PATH:/usr/local/bin/go\n</code></pre> <p>We also recommend adding <code>~/go/bin</code> directory to your <code>PATH</code>.</p> <p>** Install as administrator **</p> <p>Golang offers an MSI-based installer for Windows that makes it easy to install Golang on your Windows machine. Follow the installation wizzard and the <code>go</code> command should start working in the terminal.</p> <p>We also recommend adding <code>%USERPROFILE%\\go\\bin</code> to your <code>PATH</code> environment variable to enable running tools from your home directory.</p> <p>** Install as user **</p> <ol> <li>Download the <code>ZIP</code> archive from the archive page</li> <li>Extract the ZIP file to a folder you have access to.</li> <li>Go to Control Panel \u2192 System and Security \u2192 System \u2192 Advanced system settings.</li> <li>Click on Environment variables...</li> <li>Change the <code>PATH</code> environment variable to point to the <code>bin</code> directory inside your Goland directory.</li> </ol> <p>We also recommend adding <code>%USERPROFILE%\\go\\bin</code> to your <code>PATH</code> environment variable to enable running tools from your home directory.</p> <p>Golang offers a PKG installer for MacOS. The <code>go</code> command will be located in <code>/usr/local/go/bin</code>. If the <code>go</code> command doesn't work try restarting the terminal.</p> <p>If it still doesn't work try running the <code>/usr/local/go/bin/go</code> command. If that command works edit the <code>~/.profile</code>, <code>~/.zshrc</code>, or <code>~/.bash_profile</code> files and add the following lines then restart your terminal:</p> <pre><code>export PATH=$PATH:/usr/local/bin/go\n</code></pre> <p>We also recommend adding <code>~/go/bin</code> directory to your <code>PATH</code>.</p>"},{"location":"development/devenv/gpg/","title":"Setting up GPG for code signing (optional)","text":"Setting up GPG for code signing (optional) <p>You can sign your commits with your GPG key. This is currently optional for ContainerSSH contributors.</p>"},{"location":"development/devenv/gpg/#setting-up-gpg","title":"Setting up GPG","text":"Linux / WSLWindowsMacOS <p>On Linux or Windows Subsystem for Linux GPG is already included in the package manager. You can install it using the following commands:</p> <p>Ubuntu <pre><code>sudo apt-get update\nsudo apt-get install gnupg2\n</code></pre> RHEL/CentOS <pre><code>yum install gnupg2\n</code></pre> Fedora <pre><code>dnf install gnupg2\n</code></pre> Gentoo <pre><code>emerge --ask app-crypt/gnupg\n</code></pre></p> <p>Tip</p> <p>You may want to install the Kleopatra GUI for easier access.</p> <p>GPG4Win is a full suite for managing GPG keys on Windows. We recommend installing it with the Kleopatra GUI.</p> <p>Homebrew <pre><code>brew install gnupg2\n</code></pre> MacPorts <pre><code>sudo port install gnupg2\n</code></pre> GUI GPGTools offers a graphical version of GPG.</p>"},{"location":"development/devenv/gpg/#creating-your-gpg-key","title":"Creating your GPG key","text":"CLI (GPG 2.1.17+)KleopatraGPGTools (MacOS)CLI (GPG 2.1.16-) <p>Run the following command: <pre><code>gpg --full-generate-key\n</code></pre></p> <ul> <li>Select <code>RSA and RSA</code> as the key format.</li> <li>Select <code>4096 bits</code> for the bit size.</li> <li>When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config, otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option.</li> </ul> <ul> <li>Select File \u2192 New Key Pair...</li> <li>Select \"Create a personal OpenPGP key pair\"</li> <li>Set your name and the same e-mail address you have on your GitHub account. If you do not wish to publish your e-mail address GitHub gives you a privacy option.</li> <li>Follow the wizard to create your GPG key.</li> </ul> <ul> <li>Please follow the GPGTools guide to create your key.</li> <li>When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config, otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option.</li> </ul> <p>Run the following command: <pre><code>gpg --default-new-key-algo rsa4096 --gen-key\n</code></pre></p> <ul> <li>Select <code>RSA and RSA</code> as the key format.</li> <li>Select <code>4096 bits</code> for the bit size.</li> <li>When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config, otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option.</li> </ul>"},{"location":"development/devenv/gpg/#adding-your-key-to-github","title":"Adding your key to GitHub","text":"CLIKleopatraGPGTools (MacOS) <p>First, list your GPG keys with the key IDs:</p> <pre><code>$ gpg --list-secret-keys --keyid-format LONG\n------------------------------------------------\nsec   rsa4096/YOUR-KEY-ID 2020-06-18 [SC]\n...\n</code></pre> <p>Copy the key ID as you will need it for the next steps, then export your public key:</p> <pre><code>gpg --armor --export YOUR-KEY-ID\n</code></pre> <p>Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface.</p> <ul> <li>Right click the key generated in the previous step.</li> <li>Select \"Export...\".</li> <li>Save the file on your machine.</li> <li>Open the file in a text editor.</li> <li>Copy the key.</li> <li>Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface.</li> </ul> <ul> <li>Select the previously generated key.</li> <li>Click the \"Export\" icon in the toolbar.</li> <li>Click Save.</li> <li>Open the file in a text editor.</li> <li>Copy the key.</li> <li>Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface.</li> </ul>"},{"location":"development/devenv/gpg/#setting-up-gpg-signing-in-git","title":"Setting up GPG signing in Git","text":"GlobalPer repository <p>This method sets up automatic code signing for all git repositories on your computer. Run the following commands under your user account:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your-gpg-email@example.com\"\ngit config --global commit.gpgsign true\ngit config --global tag.gpgsign true\ngit config --global user.signingkey YOUR-KEY-ID\n</code></pre> <p>Run the following commands in the directory where you cloned the repository:</p> <pre><code>git config user.name \"Your Name\"\ngit config user.email \"your-gpg-email@example.com\"\ngit config commit.gpgsign true\ngit config tag.gpgsign true\ngit config user.signingkey YOUR-KEY-ID\n</code></pre> <p>Warning</p> <p>This method sets up GPG signing in a single repository. You must configure this every time you clone a new ContainerSSH repository.  </p>"},{"location":"development/devenv/gpg/#invoking-gpg-agent","title":"Invoking GPG-AGENT","text":"<p><code>gpg-agent</code> is a daemon to manage secret (private) keys independently from any protocol. You should always add the following lines to your .bashrc or whatever initialization file is used for all shell invocations:</p> <pre><code>GPG_TTY=$(tty)\nexport GPG_TTY\n</code></pre> <p>For more information, please read this.</p> <p>That's it! You can now continue with setting up the toolchain!  </p>"},{"location":"development/devenv/ide/","title":"Setting up an IDE","text":"Setting up an IDE <p>We strongly recommend setting up an IDE to warn you about potential issues and make the development process easier.</p>"},{"location":"development/devenv/ide/#visual-studio-code","title":"Visual Studio Code","text":"<p>Visual Studio Code is a free IDE for various languages from Microsoft for Windows, MacOS and Linux. It can be installed without admin permissions.</p> <p>Once you have installed it, please click the \"Extensions\" icon on the left and install the \"Go\" extension.</p> <p>You can then click the \"Explorer\" icon and click the \"Clone Repository\" button to clone a ContainerSSH repository.</p> <p>When you clone the repository you will be asked to install other tools, such as the delve debugger. Please install them.</p> <p>Once the repository is set up you can go to the file you want to run (e.g. <code>cmd/containerssh/containerssh.go</code>), then go to the Run \u2192 Add configuration. You can then customize the parameters of running the program (e.g. where to get the config file from).</p> <p>To run tests open the test file (e.g. <code>godogs_test.go</code>), click on <code>TestMain</code> and click the little \"run test\" text that comes up.</p> <p>The details of running ContainerSSH are discussed in the Getting started with ContainerSSH Development guide. </p>"},{"location":"development/devenv/ide/#goland","title":"Goland","text":"<p>Goland is a commercial IDE from Jetbrains often used for Go development. It contains a number of analysis tools and quality of life features making it a popular choice.</p> <p>We recommend installing Goland using the Jetbrains Toolbox which will also keep it up to date.</p> <p>Once you launch Goland you will have the option to directly clone a Git repository. However, we recommend first going into the settings, then to Go \u2192 <code>GOROOT</code> and setting up Go.</p> <p>Once you have cloned the repository you can navigate to the file you want to run (e.g. <code>cmd/containerssh/containerssh.go</code>), then click the little \"run\" icon next to the <code>main</code> function and click the \"Create\" button. This will create a configuration you can edit from the \"Run\" menu.</p> <p>To run the tests you can open the specific test you want to run or create a <code>go test</code> configuration from the Run menu.</p> <p>The details of running ContainerSSH are discussed in the Getting started with ContainerSSH Development guide.</p>"},{"location":"development/devenv/kubernetes/","title":"Installing Kubernetes","text":"Installing Kubernetes <p>If you develop against the <code>kuberun</code> backend you will need a working Kubernetes.</p> Windows / MacOSWindows / WSLLinux <p>Docker Desktop contains a working Kubernetes. Please enable it to have a working Kubernetes setup. You can test it by running:</p> <pre><code>kubectl get nodes\n</code></pre> <p>For WSL we recommend setting up KinD (Kubernetes in Docker). Please read the KinD guide for getting it running.</p> <p>Please create a cluster with the oldest officially supported Kubernetes to test against:</p> <pre><code>kind create cluster --image=image-url-here\n</code></pre> <p>You can obtain the image URL from the KinD releases section.</p> <p>We recommend using KinD (Kubernetes in Docker) as a reliable way to get a Kubernetes cluster running.</p> <p>Please create a cluster with the oldest officially supported Kubernetes to test against:</p> <pre><code>kind create cluster --image=image-url-here\n</code></pre> <p>You can obtain the image URL from the KinD releases section.</p> <p>Tip</p> <p>Some Linux distributions may support tiny Kubernetes distributions like k3s or microk8s, but we have managed to get consistently good results only with KinD.</p>"},{"location":"development/devenv/qa/","title":"Installing the QA tools","text":"Installing the QA tools"},{"location":"development/devenv/qa/#installing-golangci-lint","title":"Installing golangci-lint","text":"<p>We are using golangci-lint as a way to lint the code for problematic practices. We use golangci-li using the following command line:</p> <pre><code>golangci-lint run -E asciicheck -E bodyclose -E dupl -E errorlint -E exportloopref -E funlen\n</code></pre> <p>Please follow the instructions below:</p> Linux / WSLWindowsBrew (MacOS)MacPorts (MacOS)Manual (All platforms) <ol> <li>Go to the GitHub releases of golangci-lint and download the latest Linux <code>.tar.gz</code>.</li> <li>Extract the file to a directory in your path (e.g. <code>~/bin/go</code>).</li> <li>Add executable rights to the file (e.g. <code>chmod +x ~/bin/go/golangci-lint</code>).</li> </ol> <ol> <li>Go to the GitHub releases of golangci-lint and download the latest Windows ZIP.</li> <li>Extract the <code>golangci-lint.exe</code> to your <code>%USERPROFILE%/go/bin</code> directory. </li> </ol> <pre><code>brew install golangci-lint\n</code></pre> <pre><code>sudo port install golangci-lint\n</code></pre> <ol> <li>Go to the GitHub releases of golangci-lint and download the latest archive for your platform.</li> <li>Extract the <code>golangci-lint</code> to the <code>go/bin</code> directory in your home directory.</li> </ol>"},{"location":"development/devenv/website/","title":"Setting up the website development environment","text":"Setting up the website development environment <p>This website is developed using mkdocs using the Material theme. This guide will run you through the steps of setting it up.</p>"},{"location":"development/devenv/website/#installing-python","title":"Installing Python","text":"<p>You can download Python from the official website. You will need at least Python 3.8.</p>"},{"location":"development/devenv/website/#cloning-the-repository","title":"Cloning the repository","text":"<p>In order to develop this website you will need to clone the repository:</p> <pre><code>git clone https://github.com/containerssh/containerssh.github.io\n</code></pre>"},{"location":"development/devenv/website/#creating-a-venv","title":"Creating a venv","text":"<p>Once you have all that done we recommend you create a venv to avoid polluting your computer with packages:</p> <pre><code>python3 -m venv /path/to/containerssh.github.io\n</code></pre> <p>You can then activate the venv using the following script:</p> <pre><code>venv/Scripts/activate\n</code></pre>"},{"location":"development/devenv/website/#installing-the-dependencies","title":"Installing the dependencies","text":"<p>Now you need to install the dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"development/devenv/website/#optional-setting-the-github_token","title":"Optional: Setting the <code>GITHUB_TOKEN</code>","text":"<p>Some functions of the website require a working GitHub Token without any special permissions. You can create a token here.</p> <p>You can then set the token using the command line:</p> Linux / MacOSWindows (PowerShell)Windows (Command prompt) <pre><code>export GITHUB_TOKEN=\"your-token-here\"\n</code></pre> <pre><code>$env:GITHUB_TOKEN=\"your-token-here\"\n</code></pre> <pre><code>set GITHUB_TOKEN=your-token-here\n</code></pre> <p>Warning</p> <p>Setting <code>GITHUB_TOKEN</code> dramatically slows down the development server because the GitHub API is queried for every refresh. Only set it when you need it.</p>"},{"location":"development/devenv/website/#running-the-dev-server","title":"Running the dev server","text":"<p>Run the following command to get a dev server up and running:</p> <pre><code>python -m mkdocs serve\n</code></pre> <p>This will start the development server on localhost:8000.</p> <p>Tip</p> <p>We recommend using the free Visual Studio Code or the PyCharm Community Edition as a development environment for the website.</p>"},{"location":"development/releases/","title":"ContainerSSH release process","text":"<p>In ContainerSSH we have two components that are in need of regular releases: libraries and binaries.</p> Libraries <p>Most of our libraries are intended for internal consumption, but some are for external consumption. The release process is the same.</p> <p>Read more \u00bb</p> Binaries <p>Binaries are consumed by our users. Therefore, this release process is more involved to ensure quality.</p> <p>Read more \u00bb</p>"},{"location":"development/releases/binaries/","title":"Creating ContainerSSH binary releases","text":"<p>ContainerSSH binaries are what our users consume. This guide will attempt to outline the steps we take to make sure these releases are stable and no steps have been left out.</p>"},{"location":"development/releases/binaries/#documentation","title":"Documentation","text":"<p>The first step when preparing a new release the first step is to start preparing the documentation. The documentation for a new release always goes on the <code>containerssh.io/reference/upcoming/</code> section. This section is copied from the current documentation and is modified for the new release. The new documentation pages must also be added to the <code>mkdocs.yaml</code> to make sure they are in the menu.</p> <p>If there are deprecated features they must be added to the <code>deprecations</code> section of the documentation. Deprecation notices should be written from the perspective of a user, explaining not only what's been deprecated, but also how to upgrade and replace the deprecated feature.</p> <p>You should also pay attention to the quick start section of the website, which may need to be updated for the new version. However, these changes should be added to a branch, only to be merged when the new release goes online.</p>"},{"location":"development/releases/binaries/#versioning","title":"Versioning","text":"<p>When creating a new release you must consider what version number to pick. We follow SemVer for ContainerSSH. Backwards-incompatible changes should increase the minor version number before 1.0, and increase the major version number after 1.0. Minor, backwards incompatible features should increase the minor version number, while bugfixes and security updates should increase the patch version number.</p>"},{"location":"development/releases/binaries/#tests","title":"Tests","text":"<p>The main ContainerSSH repository contains a battery of tests that should pass before any release is made. If new features are added tests should be added to match.</p>"},{"location":"development/releases/binaries/#releasing-binaries","title":"Releasing binaries","text":"<p>Binaries are automatically generated using Goreleaser when a tag is created in the ContainerSSH repository. This will create and upload the built binaries to the GitHub releases section.</p> <p>However, with the binaries being available the job is not done. The build configuration in the images repository must be updated and a tag must be made. This will ensure that the new container images are pushed to the registries.</p>"},{"location":"development/releases/libraries/","title":"Releasing a library","text":"<p>ContainerSSH is made up of over 30 libraries. These libraries are all written in Go and must, therefore, follow the Go modules specification. It's not the most exciting read, but you should familiarize yourself with the process. We will attempt to outline the most important steps here.</p>"},{"location":"development/releases/libraries/#versioning","title":"Versioning","text":"<p>When heading up to a release versioning must be kept in mind. We follow SemVer, so only major versions may contain breaking changes. The only exception is versions before <code>1.0.0</code>, these may break in minor versions too. This includes methods like factories (e.g. <code>New</code> methods).</p> <p>With Go modules versions beyond version 1 should have the module suffix <code>v2</code>, <code>v3</code>, etc. For example, you may have a module called <code>github.com/containerssh/auth/v2</code>.</p>"},{"location":"development/releases/libraries/#release-notes","title":"Release notes","text":"<p>Once the main branch is ready for release the last step is writing release notes. We do not follow the concept of simply listing the commits, we aim to write human-readable release notes that explain the changes. The explanation has to be detailed enough so anyone from the target audience can understand what is changing without needing to understand the implementation.</p> <p>The release notes should be added to <code>CHANGELOG.md</code>.</p>"},{"location":"development/releases/libraries/#updating-the-readme","title":"Updating the README","text":"<p>Before creating a release the <code>README.md</code> file should be updated such that consumers of the library know how to use the new version of the library. Except for a few libraries like auth and configuration our libraries are intended only for consumption in ContainerSSH. This means that you can write the documentation from that perspective.</p>"},{"location":"development/releases/libraries/#creating-a-release","title":"Creating a release","text":"<p>The day has finally come: the release notes and codes are in, tests are passing. Now we need to create a release. We do this exclusively from the GitHub interface. We name versions for libraries in the format of <code>v1.2.3</code> so Go can pull them in, while applications are named <code>1.2.3</code> to avoid pulling them in from a Go program.</p> <p>We copy the name and description from the <code>CHANGELOG.md</code> into the release notes on GitHub. For applications the release mechanism will create and upload the binaries.</p>"},{"location":"development/tools/","title":"External tools we use","text":"<p>This guide runs you through the external tools we use and how we use them. This will give you a better idea on how ContainerSSH is managed.</p> GitHub <p>GitHub provides a large chunk of our infrastructure, ranging from Git hosting, CI system to hosting this very website.</p> <p>Read more \u00bb</p> Terraform <p>Since we have a large number of repositories we use the Terraform Cloud to create and configure most of the settings in our GitHub organization. This also enables non-privileged users to request new repositories or changes to existing ones.</p> <p>Read more \u00bb</p> Snyk <p>Snyk is our tool of choice to keep an eye on security updates we need to apply to Go dependencies, as well as container images.</p> <p>Read more \u00bb</p> Docker Hub <p>Docker Hub is the primary source for our container images. Docker has graciously weaved rate limits for our organization.</p> <p>Read more \u00bb</p>"},{"location":"development/tools/docker/","title":"How we use the Docker Hub","text":"<p>Docker Hub is our main distribution point for ContainerSSH images. We also maintain mirrors on Quay.io.</p> <p>Docker has graciously included us in their open source program so our images are not subject to rate limits. This is especially important because ContainerSSH pulls the default guest image (an Ubuntu with the installed guest agent and SFTP) from the Docker Hub for every connection. At the time of writing this image has already been pulled more than 100.000 times.</p> <p>In order to make sure that our pushes are independent from any one person we have created a machine user called <code>containersshbuilder</code>, which only has permissions to the ContainerSSH organization. The credentials of this user are added to GitHub actions.</p> <p>The container images are build using the images repository. This repository contains a Go build program that uses <code>docker-compose</code> to build and test ContainerSSH images before pushing them. (We learned this the hard way.)</p> <p>The default guest image on the other hand is rebuilt daily to incorporate the latest updates.</p>"},{"location":"development/tools/github/","title":"How we use GitHub","text":"<p>Our GitHub account contains over 30 repositories. These repositories are managed by Terraform and most of them are created from the library-template template repository. This is done in an effort to make sure each component has its own tests and documentation.</p> <p>Tests in each repository are executed by GitHub actions. Most repositories use CodeQL for security analysis, golangci-lint for code quality, and run <code>go test</code> for executing tests.</p> <p>Some repositories have integrations with Docker and Kubernetes. Docker is included in GitHub Actions and Kubernetes support is added by using the Kubernetes in Docker action. When running against Kubernetes we run against all currently supported versions. The tests use the kubeconfig of the current user to fetch the configuration on how to connect Kubernetes. An example of this can be found in the Kubernetes repository.</p> <p>When we release ContainerSSH we use Goreleaser to create binaries for multiple platforms. Goreleaser is also responsible for uploading the generated binaries to GitHub as a release. Currently, Goreleaser also generates <code>.deb</code> and <code>.rpm</code> packages. Later on, this will be replaced by providing a package repository that people can add to their operating systems. This is also hosted in GitHub Pages.</p>"},{"location":"development/tools/snyk/","title":"How we use Snyk","text":"<p>Snyk is a wonderful service and program to monitor dependencies that have security issues. While not all dependencies are high profile enough to make this a replacement for vetting dependencies, it helps tremendously with keeping up with container image updates.</p> <p>Snyk monitors both our container images on Docker Hub as well as our Go dependencies from the <code>go.mod</code> files.</p> <p>Tip</p> <p>The Go monitoring in Snyk is a bit finicky, so when a vulnerable library is discovered we add a <code>replace</code> section to the <code>go.mod</code> file as follows:</p> <pre><code>// Fixes CVE-2020-9283\nreplace (\n    golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2 =&gt; golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83\n    golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550 =&gt; golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83\n    golang.org/x/crypto v0.0.0-20200220183623-bac4c82f6975 =&gt; golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83\n    golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9 =&gt; golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83\n)\n</code></pre> <p>This is done even if the vulnerable library is not ultimately used. The reason for this is part added safety, and in other part to satisfy Snyk.</p> <p>When vulnerable container images are discovered we update our appropriate repositories, while in case of Go library vulnerabilities we release a new version.</p>"},{"location":"development/tools/terraform/","title":"How we use Terraform","text":"<p>Our GitHub organization contains more than 30 repositories. In order to manage the permissions and settings properly we maintain a repository with Terraform code that is consumed by Terraform Cloud. To pull this off we make use of the GitHub provider for Terraform.</p> <p>It is worth noting that the documentation may not always be up to date. It also doesn't support managing all properties of GitHub repositories, most prominently the OpenGraph preview images. These are put in place by hand.</p> <p>Once the code is pushed into the <code>main</code> repository in our GitHub repository, the Terraform Cloud picks up the change and applies it. This can be used to generate new repositories or change the settings on a repository.</p>"},{"location":"downloads/","title":"Downloads","text":"Downloads"},{"location":"downloads/#latest-release-050-released-jan-07-2024","title":"Latest release (0.5.0, released Jan 07, 2024)","text":"<p>Read the reference manual \u00bb</p> ContainerLinuxMacOSWindowsFreeBSD <p>ContainerSSH can be installed in a containerized system (Kubernetes, Docker, Podman) by referencing the following image names:</p> <pre><code>containerssh/containerssh:v0.5\ncontainerssh/containerssh:v0.5.1\n\nquay.io/containerssh/containerssh:v0.5\nquay.io/containerssh/containerssh:v0.5.1\n</code></pre> <p>Our container images are built on Alpine Linux (x86, 64 bit).</p> <p>Note about container image versioning</p> <p>We provide the images with multiple version tags. <code>latest</code> will always reference the latest image build of the latest stable version. <code>0.4</code> will always reference the latest image build of the latest 0.4 version, and <code>0.4.1</code> will always reference the latest image build of 0.4.1.</p> <p>Each of these tags will see updates as we update the base Alpine Linux image to apply security fixes. If you need to roll back to an exact previous image you can reference the image by build date, e.g. <code>0.4.1-20210526</code>. The list of images can be found on the Docker Hub.</p> <p>x86 (.tar.gz) x86 (.deb) x86 (.rpm)</p> <p>Intel (.tar.gz)</p> <p>.zip</p> <p>.tar.gz</p>"},{"location":"downloads/#older-releases","title":"Older releases","text":"Version Container Linux Windows MacOS FreeBSD Manual 0.4.1May 26, 2021 <code>containerssh/containerssh:0.4.1</code> .tar.gz.deb.rpm.apk .zip .tar.gz .tar.gz Read \u00bb 0.4.0Apr 1, 2021 <code>containerssh/containerssh:0.4.0</code> .tar.gz.deb.rpm.apk .zip .tar.gz .tar.gz Read \u00bb 0.3.1Oct 23, 2020 <code>containerssh/containerssh:0.3.1</code> .tar.gz.deb.rpm .zip .tar.gz Read \u00bb 0.3.0Oct 21, 2020 <code>containerssh/containerssh:0.3.0</code> .tar.gz.deb.rpm .zip .tar.gz Read \u00bb 0.2.2Aug 2, 2020 .tar.gz.deb.rpm .zip .tar.gz 0.2.1Jul 30, 2020 .tar.gz.deb.rpm .zip .tar.gz 0.2.0Jul 9, 2020 .tar.gz.deb.rpm .zip .tar.gz 0.1.1Jul 9, 2020 .tar.gz.deb.rpm .zip .tar.gz 0.1.0Jun 18, 2020 .tar.gz.deb.rpm .zip .tar.gz <p>Note</p> <p>Container images that no longer have pulls have been removed to conserve resources.</p>"},{"location":"getting-started/","title":"ContainerSSH Quick start","text":"Quick start <p>This is a quick start guide to get a test server up and running in less than 5 minutes with docker-compose or Kubernetes.</p> <p>\u25b6\ufe0f Watch as Video (Docker) \u25b6\ufe0f Watch as Video (Kubernetes)</p> <p>Warning</p> <p>This setup will let any password authenticate. Only use it for testing.</p>"},{"location":"getting-started/#step-1-set-up-a-dockerized-environment","title":"Step 1: Set up a Dockerized environment","text":"DockerKubernetes <p>To run this quick start please make sure you have a working Docker environment and a working docker-compose.</p> <p>To run this quick start please make sure you have a working Kubernetes environment. We recommend setting up Docker Desktop, k3s, or Kubernetes in Docker.</p>"},{"location":"getting-started/#step-2-download-the-sample-files","title":"Step 2: Download the sample files","text":"<p>Please download the contents of the example directory from the source code repository.</p>"},{"location":"getting-started/#step-3-launch-containerssh","title":"Step 3: Launch ContainerSSH","text":"DockerKubernetes <p>In the downloaded directory run <code>docker-compose up -d</code>.</p> <p>In the downloaded directory run <code>kubectl apply -f kubernetes.yaml</code>.</p>"},{"location":"getting-started/#step-4-logging-in","title":"Step 4: Logging in","text":"<p>Run <code>ssh foo@localhost -p 2222</code> on the same machine via a new terminal window. This is your test client. You should be able to log in with any password.</p> <p>Alternatively you can also try the user <code>busybox</code> to land in a Busybox container.</p>"},{"location":"getting-started/#step-5-cleaning-up","title":"Step 5: Cleaning up","text":"DockerKubernetes <p>Once you're done, you can shut down the server using the <code>docker-compose down</code>, then remove the images using <code>docker-compose rm</code>.</p> <p>Finally, you can also remove the guest image:</p> <pre><code>docker image rm containerssh/containerssh-guest-image\n</code></pre> <p>Once you're done, you can shut down the server by running <code>kubectl delete -f kubernetes.yaml</code>.</p>"},{"location":"getting-started/#step-6-making-it-productive","title":"Step 6: Making it productive","text":"<p>The authentication and configuration server included in the example is a dummy server and lets any password in. To actually use ContainerSSH, you will have to write your own authentication server. We recommend reading the architecture overview before proceeding.</p> <p>Tip</p> <p>You can pass the <code>CONTAINERSSH_ALLOW_ALL</code> environment variable to the demo auth-config server to build a honeypot.</p>"},{"location":"getting-started/architecture/","title":"Architecture","text":"Architecture <p>\u25b6\ufe0f Watch as Video</p> <p>ContainerSSH is a modular software that consists of the following main components:</p> <p></p> <ol> <li>The user connects ContainerSSH using an SSH client (e.g. PuTTY)</li> <li>ContainerSSH performs the handshake and offers the user the authentication methods supported. ContainerSSH will submit the users SSH key or password to the authentication server using HTTP (TLS encryption and authentication possible.) For more details see the page about the Auth Server.</li> <li>If the authentication is successful ContainerSSH will optionally contact the Config Server to fetch the container backend configuration. The config server can pass anything from container backend credentials to image configuration to ContainerSSH. For more details see the page about the Config Server.</li> <li>When the users SSH client requests a shell or program ContainerSSH contacts the backend configured (Docker or Kubernetes) and launches the desired Pod / Container. Currently, each new shell or program request launches a new container. For more details see the backends page.</li> </ol> <p>The authentication and configuration servers are not part of ContainerSSH and you will need to provide them.</p>"},{"location":"getting-started/authserver/","title":"Implementing an authentication server","text":"Implementing an authentication server <p>ContainerSSH does not know your users and their passwords. Therefore, it calls out to a microservice that you have to provide. Your service can verify the users, passwords, and SSH keys. You will have to provide the microservice URL in the configuration.</p> <pre><code>auth:\n  url: \"http://your-server-name/\"\n</code></pre> <p>Tip</p> <p>We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code.</p> <p>For password authentication ContainerSSH will call out to the <code>/password</code> path on your authentication server. The request body will be the following:</p> <pre><code>{\n    \"username\": \"username\",\n    \"remoteAddress\": \"127.0.0.1:1234\",\n    \"connectionId\": \"An opaque ID for the SSH connection\",\n    \"passwordBase64\": \"Base 64-encoded password\"\n}\n</code></pre> <p>The public key auth ContainerSSH will call out to <code>/pubkey</code> in the following format:</p> <pre><code>{\n    \"username\": \"username\",\n    \"remoteAddress\": \"127.0.0.1:1234\",\n    \"connectionId\": \"An opaque ID for the SSH connection\",\n    \"publicKey\": \"ssh-rsa ...\"\n}\n</code></pre> <p>The public key is provided in the SSH authorized key format.</p> <p>Your server will need to respond with the following JSON:</p> <pre><code>{\n  \"success\": true,\n  \"authenticatedUsername\": \"foo\",\n}\n</code></pre> <p>Tip</p> <p>We provide a Go library to implement a authentication server.</p> <p>You can read more in the reference manual</p>"},{"location":"getting-started/backends/","title":"Backend selection","text":"Backend selection <p>ContainerSSH is built to support multiple backends. The backend can be changed in the configuration file:</p> <pre><code># change to `kubernetes` to talk to Kubernetes\nbackend: docker\n</code></pre> <p>ContainerSSH currently supports the following backends:</p> Backend Description <code>docker</code> Runs Docker containers. <code>kubernetes</code> Runs Kubernetes containers. <code>sshproxy</code> Forwards SSH connections to a backend server. <p>Read more in the SSH proxy reference manual \u00bb</p>"},{"location":"getting-started/configuration/","title":"Configuring ContainerSSH","text":"Configuring ContainerSSH <p>Before you can run ContainerSSH, you will need to create a configuration file. In this page you will find some quick-start configuration snippets that will work for the most common use-cases but does not give a full overview of all the features and possible configuration combinations. For more a more in-depth configuration guide you can consult reference manual.</p> <p>In order to have a working ContainerSSH installation you need to define at a minimum 3 sections in your configuration file. <pre><code>ssh:\n  &lt;SSH Configuration options&gt;\nauth:\n  &lt;Authentication options&gt;\nbackend: docker|kubernetes|sshproxy\ndocker:\n  &lt;docker options&gt;\n...\n</code></pre></p> <ol> <li> <p><code>ssh</code>: Details about your ssh server</p> </li> <li> <p><code>auth</code>: How the users will authenticate to your server</p> </li> <li> <p><code>backend</code> + the associated backend configuration: How the backing container that will be used by the user will be created and with what configuration (mounts etc)</p> </li> </ol> <p>The config file must end in <code>.yml</code>, <code>.yaml</code>, or <code>.json</code>. You can dump the entire configuration file using <code>./containerssh --dump-config</code></p>"},{"location":"getting-started/configuration/#ssh-server-configuration","title":"SSH Server configuration","text":"<p>In the <code>ssh</code> section the only mandatory field is <code>hostkeys</code> which defines the private keys to be used for the server to authenticate itself to the clients.</p> <pre><code>ssh:\n  hostkeys:\n    - /path/to/your/host/key\n</code></pre>"},{"location":"getting-started/configuration/#authentication","title":"Authentication","text":"WebhookOAuth2Kerberos <p>The webhook authentication backend authenticates users by consulting an external authentication server implementing the ContainerSSH authentication API.</p> <pre><code>auth:\n  password:\n    method: webhook\n    webhook:\n      url: https://myauthserver.example.com:8080\n  publicKey:\n    method: webhook\n    webhook:\n      url: https://myauthserver.example.com:8080\n</code></pre> <p>Read more \u00bb</p> <p>Feature Preview</p> <p>OAuth2 support is considered as a feature preview as it doesn't have adequate test coverage</p> <p>The OAuth2 authentication backend authenticates users using any OIDC-compliant OAuth2 servers for authentication (such as KeyCloak, Microsoft Active Directory Federation Services, etc) and features explicit support for GitHub and GitHub Enterprise.</p> Generic OIDC providerGitHub provider <pre><code>auth:\n  keyboardInteractive:\n    method: oauth2\n    oauth2:\n      clientId: \"your-client-id\"\n      clientSecret: \"your-client-secret\"\n      provider: oidc\n      oidc:\n        url: https://your-oidc-server.example.com/\n        deviceFlow: true\n</code></pre> <p>Read more \u00bb</p> <pre><code>auth:\n  keyboardInteractive:\n    method: oauth2\n    oauth2:\n      clientId: \"your-client-id\"\n      clientSecret: \"your-client-secret\"\n      provider: github\n</code></pre> <p>Read more \u00bb</p> <p>The Kerberos authentication backend authenticates users using any authentication server that implements the Kerberos protocol (such as Microsoft Active-Directory, FreeIPA etc). It supports the GSSAPI authentication method which allows  users to log in without providing a password provided that a valid kerberos ticket is available on the users device. It additionally supports password authentication in case the user does not have or cannot provide a valid ticket.</p> <pre><code>auth:\n  method: kerberos\n  kerberos:\n    keytab: '/etc/krb5.keytab'\n</code></pre> <p>Read more \u00bb</p>"},{"location":"getting-started/configuration/#backend","title":"Backend","text":"<p>Further customization</p> <p>All users will be logged in separate, but identical, containers (equivalent of <code>docker exec</code> and <code>kubectl exec</code> for the respective backend), in order to customize the resulting container on a per-user basis you'll need to use a configuration server</p> DockerKubernetesSSH Proxy <p>The Docker backend creates containers on the specified docker daemon. You can consult the Docker guide for more examples.</p> <pre><code>backend: docker\ndocker:\n  connection:\n    host: unix:///var/run/docker.sock\n</code></pre> <p>Read more \u00bb</p> <p>The Kubernetes backend creates pods inside a Kubernetes cluster. Please note the following configuration snippet assumes that ContainerSSH is running in the same Kubernetes cluster under a service account with all the required privileges. You can consult the Kubernetes guide for more examples.</p> <pre><code>backend: kubernetes\nkubernetes:\n  connection:\n    host: kubernetes.default.svc\n    cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n  pod:\n    spec:\n      containers:\n        - name: containerssh-user\n          image: busybox\n</code></pre> <p>Read more \u00bb</p> <p>The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend. This allows for using the audit log to inspect SSH traffic, or to dynamically forwarding connections using the configuration server.</p> <pre><code>backend: sshproxy\nsshproxy:\n  # Add the backend server here\n  server: 127.0.0.1\n  # Set the following option to true to reuse the connecting user's username.\n  usernamePassThrough: true\n  # Or specify a username manually\n  username: root\n  # Specify the password\n  password: changeme\n  # Or the private key. This can reference a file or be added directly.\n  privateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n  # Provide all fingerprints of the backing SSH server's host keys:\n  allowedHostKeyFingerprints:\n- SHA256:...\n</code></pre> <p>Read more \u00bb</p>"},{"location":"getting-started/docker/","title":"The Docker backend","text":"The <code>Docker</code> backend <p>The <code>docker</code> backend launches a container using the Docker API. The default configuration connects the Docker socket on its default path.</p>"},{"location":"getting-started/docker/#changing-the-container-image","title":"Changing the container image","text":"<p>The container image depends on the backend you are using. For <code>docker</code> you can change the image in the config file:</p> <pre><code>docker:\n  execution:\n    container:\n      image: your/image\n</code></pre> <p>You can read more in the reference manual</p>"},{"location":"getting-started/faq/","title":"Frequently Asked Questions","text":"FAQ"},{"location":"getting-started/faq/#is-containerssh-secure","title":"Is ContainerSSH secure?","text":"<p>ContainerSSH depends on a number of libraries to achieve what it does. A security hole in any of the critical ones could mean a compromise of your container environment, especially if you are using the Docker backend. (Docker has no access control so a compromise means your whole host is compromised.)</p> <p>Please read the hardening guide if you intend to use ContainerSSH in production.</p>"},{"location":"getting-started/faq/#is-containerssh-production-ready","title":"Is ContainerSSH production-ready?","text":"<p>ContainerSSH is in use by several companies in production and has caused no issues or crashes. That being said, it is very early in its development and the API and configuration file format may still change.</p> <p>If you intend to use ContainerSSH in production please read the hardening guide.</p>"},{"location":"getting-started/faq/#does-containerssh-delete-containers-after-it-is-done","title":"Does ContainerSSH delete containers after it is done?","text":"<p>ContainerSSH does its best to delete containers it creates. However, at this time there is no cleanup mechanism in case it crashes.</p>"},{"location":"getting-started/faq/#do-i-need-to-run-containerssh-as-root","title":"Do I need to run ContainerSSH as root?","text":"<p>No! In fact, you shouldn't! ContainerSSH is perfectly fine running as non-root as long as it has access to Kubernetes or Docker. (Granted, access to the Docker socket means it could easily launch a root process on the host.)</p>"},{"location":"getting-started/faq/#does-containerssh-support-sftp","title":"Does ContainerSSH support SFTP?","text":"<p>Yes, but your container image must contain an SFTP server binary and your config.yaml or config server must contain the correct path for the server.</p>"},{"location":"getting-started/faq/#does-containerssh-support-scp","title":"Does ContainerSSH support SCP?","text":"<p>No, ContainerSSH does not support SCP and since OpenSSH is dropping SCP support too it probably never will.</p>"},{"location":"getting-started/faq/#does-containerssh-support-tcp-port-forwarding","title":"Does ContainerSSH support TCP port forwarding?","text":"<p>Yes, starting from version 0.5.</p>"},{"location":"getting-started/faq/#does-containerssh-support-ssh-agent-forwarding","title":"Does ContainerSSH support SSH agent forwarding?","text":"<p>No, but it is a planned feature.</p>"},{"location":"getting-started/faq/#does-containerssh-support-x11-forwarding","title":"Does ContainerSSH support X11 forwarding?","text":"<p>Yes, starting from version 0.5.</p>"},{"location":"getting-started/faq/#does-containerssh-support-forwarding-signals","title":"Does ContainerSSH support forwarding signals?","text":"<p>Yes, as of version 0.4 all backends support signal forwarding using the ContainerSSH agent.</p>"},{"location":"getting-started/faq/#does-containerssh-support-window-resizing","title":"Does ContainerSSH support window resizing?","text":"<p>Yes.</p>"},{"location":"getting-started/faq/#does-containerssh-support-environment-variable-passing","title":"Does ContainerSSH support environment variable passing?","text":"<p>Yes.</p>"},{"location":"getting-started/faq/#does-containerssh-support-returning-the-exit-status","title":"Does ContainerSSH support returning the exit status?","text":"<p>Yes.</p>"},{"location":"getting-started/faq/#can-containerssh-run-exec-into-existing-containers","title":"Can ContainerSSH run exec into existing containers?","text":"<p>No, all containers are started for a connection or session and are removed at the end. This will be a future feature.</p>"},{"location":"getting-started/faq/#can-containerssh-deploy-additional-services-such-as-sidecar-containers-etc","title":"Can ContainerSSH deploy additional services, such as sidecar containers, etc?","text":"<p>ContainerSSH supports the entire Kubernetes pod specification so you can launch as many containers as you want in a single pod. The Docker backend, however, does not support sidecar containers.</p>"},{"location":"getting-started/faq/#can-i-use-my-normal-kubeconfig-files","title":"Can I use my normal kubeconfig files?","text":"<p>Unfortunately, no. Kubeconfig files are parsed by kubectl and the code is quite elaborate. At this time, adding it to ContainerSSH is not planned.</p>"},{"location":"getting-started/getting-help/","title":"Getting help with ContainerSSH","text":"Getting help <p>This is merely the quick start guide. We also provide a reference manual as well as guides for specific use cases.</p> <p>However, sometimes the best documentation can leave you stuck with an issue. If you need help, want to discuss a topic around ContainerSSH, or have feedback you can use our GitHub Discussions Board to post a question, or join the CNCF Slack for instant messaging.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"Installation StandaloneDockerKubernetes <p>ContainerSSH can be deployed outside of a container. On our downloads page we provide binaries for Linux, Windows, and MacOS. We also provide DEB, RPM and APK (Alpine Linux) packages.</p> <p>Before running ContainerSSH you will need to create a <code>config.yaml</code> file that tells ContainerSSH where to find the SSH host key and the authentication server. The minimum configuration file looks like this:</p> <pre><code>ssh:\n  hostkeys:\n    - /path/to/your/host.key\nauth:\n  password:\n    method: webhook\n    webhook:\n      url: http://your-auth-server/\n</code></pre> <p>Tip</p> <p>You can generate a new host key using <code>openssl genrsa</code>. Please don't use <code>ssh-keygen</code> as it regenerates OpenSSH-specific keys.</p> <p>Tip</p> <p>Details about the authentication server are described in the Authentication section.</p> <p>ContainerSSH can then be started by running <code>./containerssh --config /path/to/your/config.yaml</code></p> <p>When deploying in Docker you must first prepare a configuration file that tells ContainerSSH where to find the SSH host key and the authentication server. The minimum configuration file looks like this:</p> <pre><code>ssh:\n  hostkeys:\n    - /var/run/secrets/host.key\nauth:\n  password:\n    method: webhook\n    webhook:\n      url: http://your-auth-server/\n</code></pre> <p>Tip</p> <p>You can generate a new host key using <code>openssl genrsa</code></p> <p>Tip</p> <p>Details about the authentication server are described in the Authentication section.</p> <p>You can then run ContainerSSH with the following command line:</p> <pre><code>docker run -d \\\n  -v /srv/containerssh/config.yaml:/etc/containerssh/config.yaml \\\n  -v /srv/containerssh/host.key:/var/run/secrets/host.key \\\n  -p 2222:2222 \\\n  containerssh/containerssh:0.4\n</code></pre> <p>When running ContainerSSH inside a Kubernetes cluster you must first create a <code>Secret</code> that contains the host key.</p> <pre><code>openssl genrsa | kubectl create secret generic containerssh-hostkey --from-file=host.key=/dev/stdin\n</code></pre> <p>Next, you can create a ConfigMap to hold the ContainerSSH configuration:</p> <pre><code>( cat &lt;&lt; EOF \nssh:\n  hostkeys:\n    - /etc/containerssh/host.key\nauth:\n  password:\n    method: webhook\n    webhook:\n      url: http://your-auth-server/\nEOF\n) | kubectl create configmap containerssh-config --from-file=config.yaml=/dev/stdin\n</code></pre> <p>Tip</p> <p>Details about the authentication server are described in the Authentication section.</p> <p>Then you can create a deployment to run ContainerSSH:</p> <pre><code>( cat &lt;&lt; EOF \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: containerssh\n  labels:\n    app: containerssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: containerssh\n  template:\n    metadata:\n      labels:\n        app: containerssh\n    spec:\n      containers:\n      - name: containerssh\n        image: containerssh/containerssh:0.4\n        ports:\n        - containerPort: 2222\n        volumeMounts:\n        - name: hostkey\n          mountPath: /etc/containerssh/host.key\n          subPath: host.key\n          readOnly: true\n        - name: config\n          mountPath: /etc/containerssh/config.yaml\n          subPath: config.yaml\n          readOnly: true\n      volumes:\n      - name: hostkey\n        secret:\n          secretName: containerssh-hostkey\n      - name: config\n        configMap:\n          name: containerssh-config\nEOF\n) | kubectl apply -f -\n</code></pre> <p>Finally, you can create a service to expose the SSH port. You can customize this to create a loadbalancer or nodeport to make SSH publicly available. See <code>kubectl expose --help</code> for details.  </p> <pre><code>kubectl expose deployment containerssh \\\n    --port=2222 --target-port=2222 \\\n    --name=containerssh\n</code></pre> <p>Note</p> <p>This still does not configure ContainerSSH to use Kubernetes as a container backend. This is described in detail in the Kubernetes backend section.</p>"},{"location":"getting-started/kubernetes/","title":"The Kubernetes backend","text":"<p>The <code>Kubernetes</code> backend runs a pod in a Kubernetes cluster and attaches to a container there.</p>"},{"location":"getting-started/kubernetes/#running-outside-of-kubernetes","title":"Running outside of Kubernetes","text":"<p>If you are running ContainerSSH outside of Kubernetes you will need the following configuration:</p> <pre><code>kubernetes:\n  connection:\n    host: your-kubernetes-api-server:6443\n    cert: |\n      -----BEGIN CERTIFICATE-----\n      ...\n      -----END CERTIFICATE-----\n    key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      ...\n      -----END RSA PRIVATE KEY-----\n    cacert: |\n      -----BEGIN CERTIFICATE-----\n      ...\n      -----END CERTIFICATE-----\n</code></pre> <p>Alternatively you can use <code>cacertFile</code>, <code>keyFile</code> and <code>certFile</code> to point to files on the filesystem.</p>"},{"location":"getting-started/kubernetes/#running-inside-a-kubernetes-cluster","title":"Running inside a Kubernetes cluster","text":"<p>When you run inside of a Kubernetes cluster you can use the service account token:</p> <pre><code>kubernetes:\n  connection:\n    certFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre>"},{"location":"getting-started/kubernetes/#changing-the-container-image","title":"Changing the container image","text":"<p>For the <code>kubernetes</code> backend the container image can be changed by modifying the pod spec:</p> <pre><code>kubernetes:\n  pod:\n    consoleContainerNumber: 0\n    metadata:\n      namespace: default\n    spec:\n      containers:\n        - name: shell\n          image: containerssh/containerssh-guest-image\n</code></pre> <p>Note: if you are running multiple containers you should specify the <code>consoleContainerNumber</code> parameter to indicate which container you wish to attach to when an SSH session is opened.</p> <p>You can read more in the reference manual</p>"},{"location":"getting-started/sshproxy/","title":"The SSH proxy backend","text":"<p>The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend.</p>"},{"location":"getting-started/sshproxy/#the-base-configuration-structure","title":"The base configuration structure","text":"<p>The minimum configuration is the following:</p> <pre><code>backend: sshproxy\nsshproxy:\n  # Add the backend server here\n  server: 127.0.0.1\n  # Set the following option to true to reuse the connecting user's username.\n  usernamePassThrough: true\n  # Or specify a username manually\n  username: root\n  # Specify the password\n  password: changeme\n  # Or the private key. This can reference a file or be added directly.\n  privateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n  # Provide all fingerprints of the backing SSH server's host keys:\n  allowedHostKeyFingerprints:\n    - SHA256:...\n</code></pre> <p>Read more in the SSH proxy reference manual \u00bb</p>"},{"location":"guides/","title":"ContainerSSH guides","text":"Guides for ContainerSSH Building a honeypot with ContainerSSH <p>This guide will lead you through the steps required to create an SSH honeypot with ContainerSSH.</p> <p>Read more \u00bb</p> Logging to the ELK stack with Docker and Fluentd <p>This guide will lead you through the steps required to transport ContainerSSH logs from Docker to the ELK stack using Fluentd.</p> <p>Read more \u00bb</p>"},{"location":"guides/honeypot/","title":"Creating a honeypot with ContainerSSH","text":"<p>This guide will lead you through the steps of creating an SSH honeypot with ContainerSSH.</p> <p>Danger</p> <p>Creating SSH honeypots with a real Linux backend is inherently dangerous. Any local privilege escalation could lead to the attacker taking over your host system. While this tutorial represents the best practices in building a honeypot, the responsibility of securing your installation ultimately rests upon you. Please do not attempt this unless you are intimately familiar with securing container environments. Docker has really good documentation on this topic.</p>"},{"location":"guides/honeypot/#step-1-infrastructure","title":"Step 1: Infrastructure","text":"<p>In order to set up a honeypot securely you will need at least two hosts: one to run ContainerSSH and the second to run the container infrastructure the attacker is dropped into. We'll call the first host the <code>gateway</code> VM and the second one <code>sacrificial</code> VM. Ideally, the <code>sacrificial</code> VM should run on its own dedicated physical hardware to prevent leakage of secrets due to CPU bugs. Both VMs need sufficient disk space to hold audit logs and containers.</p> <p>Furthermore, you will need an S3-compatible object storage to upload audit logs and we will need a Prometheus installation for monitoring.</p> <p>We strongly recommend automating the setup with a tool like Terraform to rapidly apply security updates.</p>"},{"location":"guides/honeypot/#step-2-firewalling-the-gateway","title":"Step 2: Firewalling the gateway","text":"<p>You should set up the <code>gateway</code> host in such a way that it is visible from the Internet. You will need the following firewall rules:</p> <ul> <li>Port <code>22</code> should be open to the Internet.</li> <li>Ports <code>9100</code> and <code>9101</code> should be open from your Prometheus instance. These will be used by the Prometheus node exporter and the ContainerSSH metrics server respectively.</li> <li>Outbound rules to your S3-compatible object storage.</li> </ul>"},{"location":"guides/honeypot/#step-3-firewalling-the-sacrificial-host","title":"Step 3: Firewalling the sacrificial host","text":"<p>The <code>sacrificial</code> host should not have any public Internet connectivity, instead it should only be connected to the <code>gateway</code> host. In order to keep this host up to date a prebuilt VM image with Docker installed should be used. The update process of this VM image can be automated using tools like Packer.</p> <p>On the firewall side, the sacrificial host should not allow any outbound connections and only allow inbound connections on TCP port 2376 from the <code>gateway</code> host.</p>"},{"location":"guides/honeypot/#step-4-creating-certificates-for-authentication-on-the-sacrificial-host","title":"Step 4: Creating certificates for authentication on the sacrificial host","text":"<p>The next step involves creating a CA infrastructure so ContainerSSH can authenticate against the Docker daemon on the sacrificial host. This is described in the Docker manual.</p> <p>Depending on how you start the Docker daemon after CA setup, you may need to set <code>nofile</code> to a high enough value (e.g., <code>65535</code>), so the daemon can run enough number of containers at the same time.</p> <ul> <li> <p>If you start Docker daemon in command line, refer to Docker daemon config guide and set both both <code>Hard</code> and <code>Soft</code> of <code>default-ulimits.nofile</code> to your value.</p> </li> <li> <p>If you start Docker daemon with <code>systemd</code>, it should already have <code>LimitNOFILE=infinity</code> in its default config file (verify via <code>systemctl cat docker</code>).</p> <p>You only need to override <code>ExecStart</code> to run <code>dockerd</code> with TLS:</p> <p><pre><code>TARGET_DIR=\"$(systemctl cat docker | grep docker.service | awk '{print $NF}').d\"\nmkdir \"$TARGET_DIR\"\nvi \"$TARGET_DIR/override.conf\"\n</code></pre> Add the following content: <pre><code>[Service]\nExecStart=\nExecStart=/usr/bin/dockerd \\\n  -H fd:// --containerd=/run/containerd/containerd.sock \\\n  --tlsverify --tlscacert=&lt;path_to_cacert&gt;\\\n  --tlskey=&lt;path_to_server_key&gt; \\\n  --tlscert=&lt;path_to_server_cert&gt; \\\n  -H=0.0.0.0:2376\n</code></pre> Then reload the config and restart <code>dockerd</code> via <pre><code>systemctl daemon-reload\nsystemctl restart docker\n</code></pre></p> </li> </ul> <p>To verify your <code>nofile</code> configuration:</p> <p><pre><code>cat /proc/$(pidof dockerd)/limits\n</code></pre> You should see <code>Max open files</code> with your configured values.</p> <p>Once your Docker socket is exposed you should test if it can be accessed without certificates. Running the following two commands from the gateway host without configuring the certificates should fail:</p> <pre><code>docker -H tcp://your-sacrificial-host:2375 run -ti ubuntu\ndocker -H tcp://your-sacrificial-host:2376 run -ti ubuntu\n</code></pre> <p>If this command does not fail the certificates have not been set up correctly.</p>"},{"location":"guides/honeypot/#step-5-installing-the-node-exporter","title":"Step 5: Installing the node exporter","text":"<p>On the gateway host you will need to install the Prometheus node exporter to make metrics such as disk space usage available to your monitoring system. Please read their readme on how to do this.</p>"},{"location":"guides/honeypot/#step-6-building-the-guest-image","title":"Step 6: Building the guest image","text":"<p>Since our sacrificial host will have no internet access you will need to upload the guest image files. You can do this by exporting the image using <code>docker export</code>, then uploading the tar file to the host and using <code>docker import</code> to import it into the Docker daemon.</p> <p>Optionally, you can build your custom image and create an <code>ubuntu</code> user in the image to give the attacker a more realistic system.</p>"},{"location":"guides/honeypot/#step-7-creating-the-containerssh-configuration-file","title":"Step 7: Creating the ContainerSSH configuration file","text":"<p>Finally, we can create the ContainerSSH configuration file on the gateway host. Let's create a few directories:</p> <pre><code>mkdir -p /srv/containerssh/config/\nmkdir -p /srv/containerssh/audit/\n</code></pre> <p>Then we generate the host key. This should be written in <code>/srv/containerssh/ssh_host_rsa_key</code>.</p> <pre><code>openssl genrsa\n</code></pre> <p>Then we can create the config file in <code>/srv/containerssh/config.yaml</code></p> <pre><code>log:\n  level: warning\nssh:\n  banner: |\n\n    ********************************************************************\n                               Warning!\n    ********************************************************************\n\n    This is a honeypot. All information, including IP address, username,\n    password, any commands you type, or files you upload will be visible\n    to the honeypot.\n\n    If you do not agree disconnect now.\n\n    ********************************************************************\n\n  hostkeys:\n    - /etc/containerssh/ssh_host_rsa_key\nbackend: docker\ndocker:\n  connection:\n    host: tcp://SACRIFICIAL-HOST-IP:2376\n    cert: |\n      -----BEGIN CERTIFICATE-----\n      &lt;client certificate here&gt;\n      -----END CERTIFICATE-----\n    key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      &lt;client key here&gt;\n      -----END RSA PRIVATE KEY-----\n    cacert: |\n      -----BEGIN CERTIFICATE-----\n      &lt;CA certificate here&gt;\n      -----END CERTIFICATE-----\n  execution:\n    imagePullPolicy: Never\n    container:\n      image: containerssh/test-guest\n      hostname: bitcoin\n      # Disable network in the container\n      networkdisabled: true\n      # Force running as user 1000\n      user: 1000\n      # Optionally set working directory\n      workingdir: /home/ubuntu\n    host:\n      # Don't let the attacker write to the root FS.\n      readonlyrootfs: true\n      resources:\n        # 10% of CPU\n        cpuperiod: 10000\n        cpuquota: 1000\n        # 50 MB of memory with swap\n        memoryswap: 52428800\n        memoryswappiness: 50\n        # 25 MB of memory\n        memory: 26214400\n        # Reserve 20 MB of memory\n        memoryreservation: 20000000\n        # Max 1000 processes to prevent fork bombs\n        pidslimit: 1000\n      tmpfs:\n        # Create writable directories in memory\n        /tmp: rw,noexec,nosuid,size=65536k,uid=1000,gid=1000\n        /run: rw,noexec,nosuid,size=65536k,uid=1000,gid=1000\n        /home/ubuntu: rw,noexec,nosuid,size=65536k,uid=1000,gid=1000\nmetrics:\n  enable: true\n  listen: \"0.0.0.0:9101\"\n  path: \"/metrics\"\naudit:\n  enable: true\n  format: binary\n  storage: s3\n  intercept:\n    stdin: true\n    stdout: true\n    stderr: true\n    passwords: true\n  s3:\n    # Local directory to store the audit log temporarily.\n    local: /var/log/containerssh/audit/\n    accessKey: YOUR-S3-ACCESS-KEY-HERE\n    secretKey: YOUR-S3-SECRET-KEY-HERE\n    region: YOUR-S3-REGION\n    bucket: YOUR-S3-BUCKET-NAME\n    # Optional: set your S3 endpoint\n    endpoint: https://YOUR-S3-ENDPOINT\n    # Optional: use path-style access for buckets\n    pathStyleAccess: true\n    metadata:\n      # Which metadata fields to set in the object storage.\n      username: true\n      ip: false\nauth:\n  url: \"http://127.0.0.1:8080\"\nconfigserver:\n  url: \"http://127.0.0.1:8080/config\"\n</code></pre>"},{"location":"guides/honeypot/#step-7-starting-containerssh","title":"Step 7: Starting ContainerSSH","text":"<p>Before starting ContainerSSH, <code>chown</code> the audit directory so ContainerSSH can write to it:</p> <pre><code>chown 1022:1022 /srv/containerssh/audit\n</code></pre> <p>Now you are ready to start ContainerSSH:</p> <pre><code>docker run -d \\\n  --restart=always \\\n  -v /srv/containerssh/:/etc/containerssh/ \\\n  -v /srv/containerssh/audit/:/var/log/containerssh/audit/ \\\n  --net=host \\\n  containerssh/containerssh:0.4.1\n</code></pre>"},{"location":"guides/honeypot/#step-8-starting-the-auth-config-server","title":"Step 8: Starting the auth-config server","text":"<p>Next, we'll need the auth-config server to let the users in:</p> <pre><code>docker run -d \\\n  --restart=always \\\n  -p 127.0.0.1:8080:8080 \\\n  -e CONTAINERSSH_ALLOW_ALL=1 \\\n  containerssh/containerssh-test-authconfig:0.4.1\n</code></pre>"},{"location":"guides/honeypot/#step-9-redirecting-port-22","title":"Step 9: Redirecting port 22","text":"<p>As a final step we will need to redirect port 22 to port 2222:</p> <pre><code>iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222\n</code></pre> <p>You will need to use the firewall facilities of your OS to make this rule persistent.</p>"},{"location":"guides/honeypot/#step-10-setting-up-monitoring","title":"Step 10: Setting up monitoring","text":"<p>Please set up monitoring for both the host metrics (such as disk space usage) and ContainerSSH itself in your Prometheus instance.</p>"},{"location":"guides/honeypot/#further-hardening","title":"Further hardening","text":"<p>This creates a honeypot that lets attackers access a container. However, in a real world scenario you may want to integrate micro virtual machines instead of containers for better security, such as Firecracker. Alternatively, you may want to investigate tools like gVisor which implement a separate security layer for your container. This is beyond the scope of this guide.</p>"},{"location":"guides/docker-elk/","title":"Logging to the ELK stack with Docker and Fluentd","text":"<p>This guide will show you how you can set up logging from ContainerSSH to your ELK stack when running in Docker. To facilitate the log transport we will be using Fluentd.</p> <p>The source code of this guide is provided in our examples repository. We will be using docker-compose to deploy the elements of the setup inside our Docker engine.</p> <p>In order to follow this guide you will need a local Docker setup similar to that in the quick start guide.</p>"},{"location":"guides/docker-elk/#step-1-starting-containerssh","title":"Step 1: Starting ContainerSSH","text":"<p>As with the quick start guide, we will be starting ContainerSSH using docker-compose. To do this we will first create our ContainerSSH configuration called <code>config.yaml</code>:</p> <pre><code>---\nssh:\n  hostkeys:\n    - /var/secrets/ssh_host_rsa_key\nauth:\n  url: \"http://authconfig:8080\"\nconfigserver:\n  url: \"http://authconfig:8080/config\"\ndockerrun:\n  host: unix:///var/run/docker.sock\nlog:\n  level: debug\n</code></pre> <p>Next we will create a Dockerfile for ContainerSSH. We need to do this because ContainerSSH runs as non-root by default, but we won't have access to the Docker socket like this. Hence, our <code>Dockerfile</code> is rather simple:</p> <pre><code>FROM containerssh/containerssh:0.4.1\nUSER 0\n</code></pre> <p>Warning</p> <p>Do not use this for production. See the Docker reference manual for details how to harden your setup.</p> <p>Now we can create the SSH host keys. You can do this using OpenSSL by running <code>openssl genrsa</code>. For testing purposes you can use the dummy key from the example repo. The key should be saved as <code>ssh_host_rsa_key</code>.</p> <p>The final piece of the puzzle is creating the <code>docker-compose.yaml</code> file:</p> <pre><code>---\nversion: '3.2'\nservices:\n  containerssh:\n    build: .\n    ports:\n      - 127.0.0.1:2222:2222\n    volumes:\n    - type: bind\n      source: ./config.yaml\n      target: /etc/containerssh/config.yaml\n    - type: bind\n      source: ./ssh_host_rsa_key\n      target: /var/secrets/ssh_host_rsa_key\n    - type: bind\n      source: /var/run/docker.sock\n      target: /var/run/docker.sock\n  authconfig:\n    image: containerssh/containerssh-test-authconfig:0.4.1\n</code></pre> <p>That's it, now we can start ContainerSSH using <code>docker-compose up</code> and log in using <code>ssh foo@localhost -p 2222</code> with the password <code>bar</code>.</p>"},{"location":"guides/docker-elk/#step-2-adding-fluentd","title":"Step 2: Adding Fluentd","text":"<p>As a next step we will add Fluentd to our <code>docker-compose.yaml</code> and configure the ContainerSSH container to log to Fluentd.</p> <p>First, let's create a file called <code>fluentd/conf/fluent.conf</code>. We add the listening config:</p> <pre><code>&lt;source&gt;\n  @type forward\n  port 24224\n  bind 0.0.0.0\n&lt;/source&gt;\n</code></pre> <p>This will cause Fluentd to listen on port <code>24224</code>. We will configure Docker to send the logs here later. Next we'll add a filter to unpack the JSON log messages ContainerSSH sends:</p> <pre><code>&lt;filter containerssh.**&gt;\n  @type parser\n  format json\n  key_name log\n  reserve_data true\n&lt;/filter&gt;\n</code></pre> <p>As a final piece we'll add forwarding to ElasticSearch:</p> <pre><code>&lt;match containerssh.**&gt;\n  @type elasticsearch\n  host elasticsearch\n  port 9200\n  logstash_format true\n&lt;/match&gt;\n</code></pre> <p>Note</p> <p>The container named <code>elasticsearch</code> will be started later.</p> <p>Now that we have the config ready we can create the Fluentd <code>Dockerfile</code> in the <code>fluentd</code> folder:</p> <pre><code>FROM fluent/fluentd:v1.12.0-debian-1.0\nUSER root\nRUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \"--no-document\", \"--version\", \"5.0.1\"]\nUSER fluent\n</code></pre> <p>Everything is in place, let's add Fluentd to our <code>docker-compose.yaml</code>:</p> <pre><code>services:\n  # ...\n  fluentd:\n    build: ./fluentd\n    volumes:\n      - ./fluentd/conf:/fluentd/etc\n    ports:\n      # We need to expose these ports to the host so the Docker engine can log to it.\n      - \"127.0.0.1:24224:24224\"\n      - \"127.0.0.1:24224:24224/udp\"\n</code></pre> <p>And finally, let's change the ContainerSSH part of the same file to send logs to Fluentd:</p> <pre><code>services:\n  containerssh:\n    #...\n    logging:\n      driver: fluentd\n      options:\n        # This address is from the perspective of the Docker daemon\n        \"fluentd-address\": \"127.0.0.1:24224\"\n        # This is the tag we match in the Fluentd config.\n        \"tag\": \"containerssh.{{.ID}}\"\n    depends_on:\n      - fluentd\n</code></pre> <p>Now everything is done and we can start the modified setup by running <code>docker-compose build</code> and then <code>docker-compose up</code>.</p>"},{"location":"guides/docker-elk/#step-3-starting-the-elk-stack","title":"Step 3: Starting the ELK stack","text":"<p>For our test setup we'll start a single-node Elasticsearch and Kibana by adding them to our <code>docker-compose.yaml</code>:</p> <pre><code>services:\n  # ...\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n    container_name: elasticsearch\n    environment:\n      # We are running ElasticSearch in single-node mode.\n      # Do we need to say this is not production ready?\n      - \"discovery.type=single-node\"\n  kibana:\n    image: kibana:7.10.1\n    ports:\n      - \"127.0.0.1:5601:5601\"\n    depends_on:\n      - elasticsearch\n</code></pre> <p>Now we're all done so we can start the stack with <code>docker-compose up</code>.</p>"},{"location":"guides/docker-elk/#step-4-configuring-kibana","title":"Step 4: Configuring Kibana","text":"<p>When you first start Kibana you will need to configure Kibana. To do that you need to head to http://localhost:5601/app/management/kibana/indexPatterns/create to create a new index pattern. Please enter the following:</p> <pre><code>logstash-*\n</code></pre> <p>At this point you may get an error that this index isn't present. This is usually because there are no logs in Elasticsearch yet. This can easily be fixed by manually stopping and restarting the ContainerSSH container to generate some logs:</p> <pre><code>docker stop &lt;id of ContainerSSH container&gt; &amp;&amp; docker start &lt;id of ContainerSSH container&gt;\n</code></pre> <p>On the next step Kibana will ask you for the timestamp field. You can use both <code>@timestamp</code> and <code>timestamp</code>, they will contain the same values.</p> <p>When the index is created you can head to http://localhost:5601/app/discover#/ and you should now see ContainerSSH logs in your Kibana. Congratulations!</p> <p></p>"},{"location":"reference/","title":"ContainerSSH 0.5.0 Reference Manual","text":"ContainerSSH 0.5.0 Reference Manual <p>The Reference Manual provides reference material for ContainerSSH 0.4 and is oriented towards system operators wishing to use ContainerSSH on their system.</p>"},{"location":"reference/#introduction","title":"Introduction","text":"<p>This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation. If you need a one-minute primer on how ContainerSSH works please watch this video.</p>"},{"location":"reference/#changes-since-containerssh-041","title":"Changes since ContainerSSH 0.4.1","text":"<p>ContainerSSH 0.5.0 is a feature and bugfix release. The reference manual for the older ContainerSSH 0.4.1 is available here. This release adds a significant number of features and bug fixes:</p> <ol> <li>OAuth2 and Kerberos authentication</li> <li>Authorization webhook</li> <li>Passing metadata from authentication to configuration servers and backends</li> <li>Deploying files into the containers from the authentication and configuration hooks</li> <li>Passing SSH certificate information to the authentication webhook</li> <li>X11 forwarding</li> <li>SSH keepalives</li> <li>Health check endpoint</li> <li>Changes to the Prometheus integration</li> <li>Removed the deprecated DockerRun and KubeRun backends</li> </ol>"},{"location":"reference/#oauth2-and-kerberos-authentication","title":"OAuth2 and Kerberos authentication","text":"<p>The biggest change of this release is support for multiple authentication backends. Thanks to our contributors we now have support for OAuth2 and Kerberos authentication.</p> <p>OAuth2 authentication works with GitHub and any OIDC-compliant authentication server, such as Keycloak and Microsoft Active Directory Federation Services. We have actively worked with several SSH client vendors to make this authentication method work and we are happy to report that it works in OpenSSH, PuTTY, Filezilla, WinSCP, and more. The authenticaiton prompts the user to click on a link in their SSH client and then log in via their normal browser-based flow. What's more, you can automatically expose the GitHub or OIDC token to the container. Your users can directly use their credentials in your ContainerSSH environment.</p> <p>Similarly, Kerberos authentication is also useful in an enterprise setting. When users are logged in to their personal devices using company credentials, they will now be able to automatically log in to ContainerSSH with Kerberos. Optionally, users can also log in to ContainerSSH from a non-authenticated device using username and password, and ContainerSSH will automatically create a Kerberos ticket for them. This ticket is available in the container directly, so your users can work with their Kerberos credentials without any additional steps.</p> <p>Read more \u00bb</p>"},{"location":"reference/#authorization-webhook","title":"Authorization webhook","text":"<p>As part of our authentication and authorization overhaul we added a separate webhook. This webhook lets you match up the username entered in SSH and the authenticated credentials in a separate step. You can, for example, authenticate a user from Kerberos and then use a webhook to match up their Kerberos identity with the SSH username. </p> <p>Read more \u00bb</p>"},{"location":"reference/#metadata-handling","title":"Metadata handling","text":"<p>The authentication and configuration servers now support passing metadata between each-other and to ContainerSSH.</p> <p>Read more \u00bb</p>"},{"location":"reference/#deploying-files","title":"Deploying files","text":"<p>As part of the new metadata system both the authentication and configuration servers can now set environment variables and deploy files in the container user containers. This functionality depends on the ContainerSSH agent to be installed and available in the container image.</p> <p>Read more \u00bb</p>"},{"location":"reference/#ssh-certificate-information","title":"SSH certificate information","text":""},{"location":"reference/#port-socket-and-x11-forwarding","title":"Port, socket and X11 forwarding","text":"<p>From this release support for forward and reverse port forwarding is supported natively. For these features the ContainerSSH agent has to be enabled as the agent acts as the entry &amp; exit points of the connections inside the container.</p> <p>The features implemented correspond to the openssh commands:</p>"},{"location":"reference/#forward-reverse-port-forwarding","title":"Forward &amp; Reverse port forwarding","text":"<p>Example: Forward port 8080 on the local host the service running on port 8080 on the remote container <pre><code>ssh -L 8080:127.0.0.1:8080 user@example.org\n</code></pre></p> <p>Example: Forward connections from a socket on the local machine to a socket in the container <pre><code>ssh -L /path/to/local/socket:/path/to/remote/socket\n</code></pre></p> <p>Example: Forward connections from port 8080 on the container to the service running on port 8080 on the local machine <pre><code>ssh -R 8080:127.0.0.1:8080\n</code></pre></p> <p>Example: Forward connections from a socket on the container to a socket on the local machine <pre><code>ssh -L /path/to/local/socket:/path/to/remote/socket\n</code></pre></p>"},{"location":"reference/#connection-proxying-support-eg-socks","title":"Connection proxying support (e.g. SOCKS)","text":"<pre><code>ssh -D 8080 user@example.com\n</code></pre> <p>You can then use ContainerSSH as a proxy with anything that supports the SOCKS protocol (e.g. Firefox)</p>"},{"location":"reference/#x11-forwarding","title":"X11 forwarding","text":"<p><pre><code>ssh -X user@example.com\n</code></pre> Any X11 applications launched within the container will be visible on the local machine</p> <p>Read more \u00bb</p>"},{"location":"reference/#ssh-keepalives","title":"SSH keepalives","text":"<p>Explicit support has been added for SSH KeepAlives. Previously, keepalives received from the client would wield an unknown global command warning and flood the logs, keepalives are now handled transparently and do not generate a warning.</p> <p>Additionally, support has been added to send keepalives to all clients from the server at a pre-defined interval. This can be configured with the following parameters:</p> <pre><code>ssh:\n    // The interval that keepalive messages are sent to each client, defaults to 0 which disables the feature (no keepalives are sent).\n    clientAliveInterval: 10s\n    // The number of unanswered keepalives before ContainerSSH considers a client unresponsive and kills the connection, defaults to 3.\n    clientAliveCountMax: 3\n</code></pre> <p>This can be useful if ContainerSSH is sitting behind a load balancer which automatically kills idle connections after a pre-defined interval. A keepalive will keep the connection active as long as the client is responsive.</p>"},{"location":"reference/#health-check-endpoint","title":"Health check endpoint","text":"<p>A new health check service has been created that can be used with Kubernetes or loadbalancers to automatically remove unhealthy ContainerSSH instances from the pool.</p> <p>Read more \u00bb</p>"},{"location":"reference/#changes-to-the-prometheus-integration","title":"Changes to the Prometheus integration","text":"<p>The name of some prometheus metrics and units has been altered to adhere to the convention of the metric name ending with the unit.</p> <p>In detail the following metrics have been modified:</p> <ul> <li><code>containerssh_auth_server_requests</code>:<ul> <li>Name changed to <code>containerssh_auth_server_requests_total</code></li> <li>Unit name change from <code>requests</code> to <code>requests_total</code></li> </ul> </li> <li><code>containerssh_auth_server_failures</code>: <ul> <li>Name changed to <code>containerssh_auth_server_failures_total</code></li> <li>Unit name change from <code>requests</code> to <code>failures_total</code></li> </ul> </li> <li><code>containerssh_auth_success</code>: <ul> <li>Name changed to <code>containerssh_auth_success_total</code></li> <li>Unit name change from <code>requests</code> to <code>success_total</code></li> </ul> </li> <li> <p><code>containerssh_auth_failures</code>:</p> <ul> <li>Name changed to <code>containerssh_auth_failures_total</code></li> <li>Unit name change from <code>requests</code> to <code>failures_total</code></li> </ul> </li> <li> <p><code>containerssh_backend_requests</code>:</p> <ul> <li>Name changed to <code>containerssh_backend_requests_total</code></li> <li>Unit name change from <code>requests</code> to <code>requests_total</code></li> </ul> </li> <li> <p><code>containerssh_backend_errors</code>:</p> <ul> <li>Name changed to <code>containerssh_backend_errors_total</code></li> <li>Unit name change from <code>requests</code> to <code>errors_total</code></li> </ul> </li> <li> <p><code>containerssh_config_server_requests</code>:</p> <ul> <li>Name changed to <code>containerssh_config_server_requests_total</code></li> <li>Unit name change from <code>requests</code> to <code>requests_total</code></li> </ul> </li> <li> <p><code>containerssh_config_server_failures</code>:</p> <ul> <li>Name changed to <code>containerssh_config_server_failures_total</code></li> <li>Unit name change from <code>requests</code> to <code>failures_total</code></li> </ul> </li> <li> <p><code>containerssh_ssh_connections</code>:</p> <ul> <li>Name changed to <code>containerssh_ssh_connections_total</code></li> <li>Unit name change from <code>connections</code> to <code>connections_total</code></li> </ul> </li> <li><code>containerssh_ssh_handshake_successful</code>:<ul> <li>Name changed to <code>containerssh_ssh_successful_handshakes_total</code></li> <li>Unit name change from <code>handshakes</code> to <code>handshakes_total</code></li> </ul> </li> <li><code>containerssh_ssh_handshake_failed</code>:<ul> <li>Name changed to <code>containerssh_ssh_failed_handshakes_total</code></li> <li>Unit name change from <code>handshakes</code> to <code>handshakes_total</code></li> </ul> </li> </ul>"},{"location":"reference/#removal-of-the-deprecated-dockerrun-and-kuberun-backends","title":"Removal of the deprecated DockerRun and KubeRun backends","text":"<p>Following the deprecation notice in the previous versions, the dockerrun and kuberun backends have been removed. The updated docker and kubernetes backends should be used instead.</p>"},{"location":"reference/audit/","title":"Audit Logging","text":"Audit logging <p>ContainerSSH contains an audit logging facility that can log every interaction happening over SSH. This functionality is disabled by default as it has serious security and privacy implications, as well as severe resource requirements.</p> <p>Audit logging can be enabled in the configuration using the following structure:</p> <pre><code>audit:\n  enable: true\n  format: none|binary|asciinema # Which format to log in. Defaults to none.\n  storage: none|s3|file        # Where to write audit log. Defaults to none.\n  intercept:\n    stdin: true|false          # Intercept keystrokes from user\n    stdout: true|false         # Intercept standard output\n    stderr: true|false         # Intercept standard error\n    passwords: true|false      # Intercept passwords during authentication\n</code></pre> <p>Audit logging is a powerful tool. It can capture the following events.</p> <ul> <li>Connections</li> <li>Authentication attempts, optionally with credentials</li> <li>Global and channel-specific SSH requests</li> <li>Programs launched from SSH</li> <li>Input from the user (optional)</li> <li>Output and errors to the user (optional)</li> </ul> <p>The events recorded depend on the chosen format. With the <code>audit</code> format all information is recorded with nanosecond timing, so events can be accurately reconstructed after the fact.</p>"},{"location":"reference/audit/#about-interceptions","title":"About interceptions","text":"<p>The <code>intercept</code> options give you a wide range of options when it comes to detailed logging of actions by users. You may want to, for example, enable <code>stdout</code> logging while keeping <code>stdin</code> disabled to avoid accidentally capturing passwords typed into the console.</p> <p>However, this approach may fail if SFTP is enabled as you will fail to capture binaries uploaded to the server. Audit logging should therefore be enjoyed with great care and the logs should always be stored on an encrypted storage device.</p>"},{"location":"reference/audit/#log-formats","title":"Log formats","text":""},{"location":"reference/audit/#the-binary-format-recommended","title":"The <code>binary</code> format (recommended)","text":"<p>The binary format is intended for an accurate reconstruction of everything happening during an SSH session. It allows for accurate reconstruction of what happened during the session.</p> <p>Audit logs are stored in a compressed binary format and can be decoded to a series of JSON messages using the <code>containerssh-auditlog-decoder</code> supplied as part of the ContainerSSH release. Alternatively, you can implement your own decoder. We are providing a Go library for decoding audit log messages.</p> <p>This format can be decoded using the <code>containerssh-auditlog-decoder</code> application supplied with ContainerSSH.</p>"},{"location":"reference/audit/#the-asciinema-format","title":"The <code>asciinema</code> format","text":"<p>The asciinema format stores logs in a format suitable for replay in the Asciinema player.</p> <p>Note</p> <p>Make sure you enable the <code>stdout</code> and <code>stderr</code> interceptions otherwise the <code>asciinema</code> encoder won't capture anything. </p> <p>Warning</p> <p>Asciinema is intended for entertainment purposes only and doesn't store all relevant information required for an accurate audit log.</p>"},{"location":"reference/audit/#storage-backends","title":"Storage backends","text":""},{"location":"reference/audit/#the-s3-storage-recommended","title":"The <code>s3</code> storage (recommended)","text":"<p>The S3 storage sends the logs to an S3-compatible object storage for long term storage. This is the recommended way of storing audit logs because it is a server-independent storage device that supports permissions.</p> <p>The S3 storage stores the logs in a local directory and uploads them once an upload part is full (default: 5MB) or the connection closes. If the upload fails, ContainerSSH will retry the upload as soon as possible. If ContainerSSH is stopped and restarted it will attempt to upload the audit logs still in the local directory, but no guarantee is made that these logs will not be corrupt after a crash.</p> <p>Warning</p> <p>The local directory should be stored on a persistent storage and must not be shared between ContainerSSH instances. It must be large enough to host all sessions in their entirety that are currently running. When IO interception is enabled and your users are downloading or uploading large amounts of data this can run you up to several GB of storage needed locally. We recommend turning off IO interception for cases where large amounts of data are being transferred.  </p> <p>The S3 storage can be configured as follows:</p> <pre><code>audit:\n  ... \n  storage: s3\n  s3:\n    local: /local/storage/directory\n    accessKey: \"your-access-key-here\"\n    secretKey: \"your-secret-key-here\"\n    bucket: \"your-existing-bucket-name-here\"\n    region: \"your-region-name-here\"\n    endpoint: \"https://your-custom-s3-url\" # Optional\n    uploadPartSize: 5242880 # In bytes, min: 5MB, max: 5GB\n    acl: \"public-read\" # Optional, in case you want to set an ACL\n    metadata:\n      username: true # Expose username via S3 metadata. Defaults to false.\n      ip: true # Expose IP address via S3 metadata. Defaults to false.\n    cacert: | # Optional\n      Your trusted CA certificate in PEM format here for your S3 server.\n</code></pre> <p>Tip</p> <p>You can restrict the access key permissions to <code>PutObject</code>, <code>CreateMultipartUpload</code>, <code>UploadPart</code>, <code>CompleteMultipartUpload</code>, <code>ListMultipartUploads</code>, and <code>AbortMultipartUpload</code>. Other permissions are not required.</p> <p>Tip</p> <p>You may also want to investigate if your S3 provider supports WORM / object locking, object lifecycles, or server side encryption for compliance.</p>"},{"location":"reference/audit/#the-file-storage","title":"The <code>file</code> storage","text":"<p>The file storage writes audit logs to files on the disk. The storage location can be configured using the following option:</p> <pre><code>audit:\n  storage: file\n  file:\n    directory: /var/log/audit\n</code></pre>"},{"location":"reference/auth-kerberos/","title":"Kerberos authentication","text":"Kerberos authentication SSH Authentication method Password Public-Key Keyboard-interactive GSSAPI Kerberos backend <p>The Kerberos authentication backend authenticates users using any authentication server that implements the Kerberos protocol (such as Microsoft Active-Directory, FreeIPA etc). It supports the GSSAPI authentication method which allows users to log in without providing a password provided that a valid kerberos ticket is available on the users device. It additionally supports password authentication in case the user does not have or cannot provide a valid ticket. The main advantage of Kerberos authentication over other authentication methods is that by utilizing credential forwarding the user can authenticate to additional services without re-typing their passwords by means of credential forwarding.</p> <p>To use the kerberos backend you'll need two things:</p> <ul> <li>A service keytab</li> <li>A valid kerberos config file (<code>krb5.conf</code>) for your infrastructure</li> </ul>"},{"location":"reference/auth-kerberos/#configuration","title":"Configuration","text":"<p>The configuration for Kerberos authentication looks as follows:</p> <pre><code>auth:\n  password:\n    method: kerberos\n    kerberos:\n      keytab: /path/to/krb5.keytab\n      configPath: /path/to/krb5.conf\n  gssapi:\n    method: kerberos\n    kerberos:\n      keytab: /path/to/krb5.keytab\n      configPath: /path/to/krb5.conf\n</code></pre> <p>This will allow logging in either via passwordless login using kerberos tickets (GSS-Api authentication) or by typing the users password.</p> <p>An example <code>krb5.conf</code> file looks like this:</p> <pre><code>[libdefaults]\n default_realm = TESTING.CONTAINERSSH.IO\n dns_lookup_realm = false\n dns_lookup_kdc = false\n ticket_lifetime = 24h\n renew_lifetime = 7d\n forwardable = true\n\n[realms]\nTESTING.CONTAINERSSH.IO = {\n    kdc = 127.0.0.1\n}\n\n[domain_realm]\n</code></pre> <p>The <code>ticket_lifetime</code>, <code>renew_lifetime</code> and <code>forwardable</code> flags take effect when generating an initial ticket for the user when they logged in via password authentication. They control the lifetime of the users credentials, the amount of time the user can renew those credentials and if these credentials can be forwarded to other services the user logs into.</p>"},{"location":"reference/auth-kerberos/#additional-configuration","title":"Additional configuration","text":"<p>Furthermore, inside the <code>kerberos</code> section the following options are supported (and can be customized per-authentication method).</p> Option Type Default Description <code>keytab</code> string <code>/etc/krb5.keytab</code> The location of the keytab file for the SSH service <code>acceptor</code> string <code>any</code> The keytab entry (acceptor) that will be used to verify the users tickets when using GSS-Api authentication. The special value <code>any</code> will check all acceptors. <code>configPath</code> string <code>/etc/krb5.conf</code> The location of the kerberos configuration file, used only for password-based authentication, a valid realm must be configured <code>enforceUsername</code> boolean <code>true</code> Whether to ensure that the authenticated username matches the requested username. \u26a0 DANGER if set to false: See the Authorization section \u26a0 <code>credentialCachePath</code> string <code>/tmp/krb5cc</code> The path to store the users credentials inside the container. <code>clockSkew</code> time duration <code>5m</code> The maximum allowed clock skew for Kerberos messages. Any messages with an older timestamp will be rejected. This is used to prevent replay attacks."},{"location":"reference/auth-kerberos/#authorization-and-username-matching","title":"Authorization and username matching","text":"<p>The setting <code>enforceUsername</code> controls whether to make sure that users can only log in to their own account. When a user connects to an SSH server via kerberos there are 2 different usernames in force, first is the principal username, this is the username present in the kerberos credentials and the real username of the user. The second username is the username that the user requests to log in as.</p> <p>As an example, if my username is <code>nikos</code> and I run the following ssh command: <pre><code>ssh root@myfancykerberosserver.example.org\n</code></pre> The principal (authenticated) username is <code>nikos</code>, my username, and the username that I request to log in as is <code>root</code>.</p> <p>In other words, <code>enforceUsername</code> makes sure that <code>authenticatedUsername == requestedUsername</code> and as a result, with the default value of this setting, the aforementioned ssh command would fail as <code>nikos</code> is not allowed to log into the <code>root</code> account (only <code>root</code> can).</p> <p>In cases where it is desirable for some users to be able to log in with a different username than their own, this setting can be disabled. In this mode, it is strongly advised to use an authorization webhook to control the authorization. In the authorization webhook both the authenticated username and the requested username are provided so any custom logic can be implemented.</p> <p>Danger</p> <p>By disabling <code>enforceUsername</code> you are disabling a very important security mechanism that ensures that each user can only access his own account. By disabling this setting without an authorization server guarding logins means that any user can log in as any username including root.</p>"},{"location":"reference/auth-kerberos/#credential-forwarding","title":"Credential Forwarding","text":"<p>ContainerSSH can place a kerberos ticket to the file specified in <code>credentialCachePath</code> inside the container. This ticket can be used to authenticate to any other kerberos-enabled service with the users credentials. In order for a ticket to be placed the following conditions have to be satisfied:</p> <ol> <li>The parent directory of <code>credentialCachePath</code> needs to exist (in the user container!) and be writable</li> <li>The ContainerSSH agent needs to be enabled and present inside the container</li> <li>The user has to have used a forwardable kerberos ticket when logging in via GSSAPI or the user must have logged in via password authentication.</li> </ol> <p>If all of the above is satisfied then the users ticket-granting-ticket will be written to <code>credentialCachePath</code>. Please note that in order for the credential cache to be picked up by kerberos the path needs to be configured as such in <code>/etc/krb5.conf</code>, or the equivalent kerberos configuration file.</p> <p>Info</p> <p>The relevant configuration setting in the kerberos configuration is <code>default_ccache_name</code>. An example snippet is: <pre><code>[libdefaults]\ndefault_ccache_name = FILE:/tmp/krb5cc\n</code></pre></p>"},{"location":"reference/auth-oauth2/","title":"OAuth2 authentication","text":"OAuth2 authentication SSH Authentication method Password Public-Key Keyboard-interactive GSSAPI OAuth2 backend <p>Feature Preview</p> <p>OAuth2 support is considered as a feature preview as it doesn't have adequate test coverage</p> <p>The OAuth2 authentication backend authenticates users using any OIDC-compliant OAuth2 servers for authentication (such as KeyCloak, Microsoft Active Directory Federation Services, etc) and features explicit support for GitHub and GitHub Enterprise. Authentication is done using the <code>keyboard-interactive</code> SSH authentication mechanism which is supported by most, but not all, SSH clients.</p>"},{"location":"reference/auth-oauth2/#supported-clients","title":"Supported clients","text":"<p>We have tested the following clients and know them to work:</p> <ul> <li>OpenSSH</li> <li>PuTTY</li> <li>WinSCP</li> <li>Filezilla</li> </ul>"},{"location":"reference/auth-oauth2/#configuration","title":"Configuration","text":"<p>The configuration structure for OAuth2 authentication looks as follows:</p> <pre><code>auth:\n  keyboardInteractive:\n    method: oauth2\n    oauth2:\n      clientId: \"client ID string\"\n      clientSecret: \"client secret string\"\n      provider: oidc|github\n      github:\n        &lt;GitHub configuration&gt;\n      oidc:\n        &lt;OIDC configuration&gt;\n      qrCodeClients:\n        - &lt;Client version string regexps that should be sent an ASCII QR code&gt;\n      deviceFlowClients:\n        - &lt;Client version string regexps to use the device flow with&gt;\n      redirect:\n        &lt;configuration for the redirect server&gt;\n</code></pre>"},{"location":"reference/auth-oauth2/#client-credentials","title":"Client credentials","text":"<p>Both OIDC and GitHub needs a client ID and a client secret in order to verify the identity to the OAuth2 server. These can be provided in the <code>clientId</code> and <code>clientSecret</code> fields.</p>"},{"location":"reference/auth-oauth2/#provider-configuration","title":"Provider configuration","text":"<p>Currently, we support OIDC and GitHub as providers of OAuth2-based authentication.</p>"},{"location":"reference/auth-oauth2/#oidc-configuration","title":"OIDC configuration","text":"<p>OpenID Connect (OIDC) is a popular authentication protocol used for Single-Sign-On. It is supported in popular authentication products such as Keycloak and Microsoft Active Directory Federation Services. The ContainerSSH OIDC provider allows users to authenticate using the same single sign on infrastructure as any web-based service. When a user connects, ContainerSSH will provide the user with the configured OIDC servers authentication url to click on and authenticate. There are two different supported OIDC authentication flows that can be used, the device flow and the authorization flow.</p>"},{"location":"reference/auth-oauth2/#oidc-device-flow","title":"OIDC Device Flow","text":"<pre><code>auth:\n  keyboardInteractive:\n    method: oauth2\n    oauth2:\n      clientId: \"your-client-id\"\n      clientSecret: \"your-client-secret\"\n      provider: oidc\n      oidc:\n        url: https://your-oidc-server.example.com/\n        deviceFlow: true|false\n        authorizationFlow: true|false\n        &lt;other oidc options&gt;\n</code></pre> <p>The OIDC device flow authentication leverages the SSH Keyboard-interactive authentication method to provide the user with a login URL pointing to the OIDC services device flow page and provides the user with a unique one-time-code to enter into the page in order to be authenticated. Depending on the OIDC implementation used the url that is provided can include the authentication code baked-in the url, in which case the user just has to click 'Authorize' in the SSO page, or in other cases they'd have to enter the provided code in order to be authenticated.</p> <p>Example user login with OIDC device flow <pre><code>$ ssh username@myssh.example.com\nPlease click the following link: https://sso.example.com/login/device?code=8B60-2612\n\nEnter the following code: 8B60-2612\nroot@containerssh-fxsjd:/#\n</code></pre></p>"},{"location":"reference/auth-oauth2/#oidc-authorization-flow","title":"OIDC Authorization Flow","text":"<pre><code>auth:\n  keyboardInteractive:\n    method: oauth2\n    oauth2:\n      clientId: \"your-client-id\"\n      clientSecret: \"your-client-secret\"\n      provider: oidc\n      oidc:\n        url: https://your-oidc-server.example.com/\n        deviceFlow: true|false\n        authorizationCodeFlow: true|false\n        extraScopes:\n          - scope1\n          - scope2\n        enforceScopes: true\n        &lt;other oidc options&gt;\n</code></pre> <p>The following configuration options are supported:</p> Option Type Description <code>deviceFlow</code> <code>bool</code> Use device flow when authenticating. Defaults to false. <code>authorizationCodeFlow</code> <code>bool</code> Use authorization code flow when authenticating. Defaults to false. <code>usernameField</code> <code>string</code> The field from the result of the userinfo OIDC endpoint to use as the username. Defaults to <code>sub</code> <code>redirectURI</code> <code>string</code> The URI the client is returned to after successful authorization flow authentication. <code>extraScopes</code> <code>[]string</code> A list of extra scopes to request from the user in addition to \"openid\". <code>enforceScopes</code> <code>bool</code> If set to <code>true</code> the authentication will fail if the user doesn't grant the scopes requested in <code>extraScopes</code>. <p>The device flow takes precedence over the authorization code flow if enabled.</p> <p>All further options are described on the HTTP and TLS page. The <code>url</code> field must be provided with the base URL of the OIDC server.</p>"},{"location":"reference/auth-oauth2/#github-configuration","title":"GitHub configuration","text":"<pre><code>auth:\n  oauth2:\n    provider: github\n    clientId: \"your-client-id\"\n    clientSecret: \"your-client-secret\"\n    github:\n      &lt;GitHub-specific options&gt;\n</code></pre> <p>The following options are available for GitHub:</p> Option Type Description <code>url</code> <code>string</code> URL for GitHub. Defaults to <code>https://github.com</code>. Can be changed for GitHub Enterprise. <code>apiurl</code> <code>string</code> API URL for GitHub. Defaults to <code>https://api.github.com</code>. Can be changed for GitHub Enterprise. <code>enforceUsername</code> <code>bool</code> Enforce that the SSH and GitHub username must match. Defaults to <code>true</code>. If set to <code>false</code> the configuration server must evaluate the <code>github_username</code> metadata field to create the correct configuration as the SSH username may be incorrect. <code>requireOrgMembership</code> <code>string</code> If set ContainerSSH will require the user to be in the specified organization. It will also request the <code>org:read</code> permission from the user to verify this. Failing to provide the permission or not being a member of the organization results in failed authentication. <code>require2FA</code> <code>bool</code> If enabled ContainerSSH will require to have two factor authentication enabled on GitHub. <code>extraScopes</code> <code>[]string</code> A list of GitHub scopes (permissions) to request from the user. <code>enforceScopes</code> <code>bool</code> If set to <code>true</code> the authentication will fail if the user doesn't grant the scopes requested in <code>extraScopes</code>. <p>Further options are described on the HTTP and TLS page.</p>"},{"location":"reference/auth-oauth2/#metadata","title":"Metadata","text":"<p>The OAuth2 authenticator exposes the following metadata fields to the configuration server:</p> <ul> <li><code>oauth2_access_token</code></li> </ul> <p>The OIDC authenticator additionally exposes all fields from the UserInfo endpoint with the <code>oidc_</code> prefix.</p> <p>The GitHub authenticator additionally exposes the following fields:</p> <ul> <li><code>github_username</code> - Username on GitHub.</li> <li><code>github_email</code> - Publicly visible profile e-mail of the user.</li> <li><code>github_name</code> - Name the user provided on GitHub.</li> <li><code>github_company</code> - Company the user entered on GitHub.</li> <li><code>github_avatar_url</code> - URL of the user's GitHub avatar.</li> </ul> <p>Tip</p> <p>If you configure the backend to expose the <code>oauth2_access_token</code> as the <code>GITHUB_TOKEN</code> environment variable you can use the GitHub CLI and many other GitHub-integrated CLI tools without further configuration.</p>"},{"location":"reference/auth-oauth2/#qr-code-authentication","title":"QR code authentication","text":"<p>When authenticating using the device flow, it can be easier to log in via a mobile phone. When ContainerSSH knows that the client can display an ASCII-art QR code ContainerSSH can send a QR code for mobile login.</p> <p>While ContainerSSH has a sane list of defaults, you can configure which clients to send a QR code to. This is a list of Go regular expressions to match against the SSH client version string.</p> <pre><code>auth:\n  oauth2:\n    qrCodeClients:\n      - \"regexp1\"\n      - \"regexp2\"\n      - \"...\"\n</code></pre>"},{"location":"reference/auth-oauth2/#device-flow-workarounds","title":"Device flow workarounds","text":"<p>Some clients may not be able to handle the device flow properly since it doesn't show a prompt. ContainerSSH contains a list of clients that the device flow should be attempted on. This is a list of regular expressions that match the SSH client version string:</p> <pre><code>auth:\n  oauth2:\n    deviceFlowClients:\n      - \"regexp1\"\n      - \"regexp2\"  \n</code></pre> <p>Tip</p> <p>ContainerSSH may still fall back to the authorization code if the device flow is temporarily not available for the matching clients.</p>"},{"location":"reference/auth-oauth2/#redirect-server","title":"Redirect server","text":"<p>The redirect server is used when the OAuth2 provider doesn't support or temporarily can't provide device flow authentication (e.g. GitHub rate limiting). It is also used if the client can't handle device flow authentication.</p> <p>In this case the user returns to a website after successful authentication and is presented with a code they need to copy to their SSH window. For this to work you need to configure the HTTP redirect server as follows.</p> <p>The configuration structure is as follows:</p> <pre><code>auth:\n  oauth2:\n    redirect:\n      &lt;configuration options&gt;\n</code></pre> <p>The following configuration options are supported:</p> Option Type Description <code>webroot</code> <code>string</code> Path to customized web directory. Optional. <p>Additionally, all options in the HTTP server section on the HTTP and TLS page are available. The redirect server defaults to port 8080.</p>"},{"location":"reference/auth-oauth2/#customizing-the-redirect-site","title":"Customizing the redirect site","text":"<p>You also have the option to customize the redirect page. In order to do that you will have to specify a directory to load the webroot from:</p> <pre><code>auth:\n  oauth2:\n    redirect:\n      webroot: /path/to/webroot/\n</code></pre> <p>The webroot must at least contain one <code>index.html</code> file. This file will be used as a Go template and must contain the <code>{{ .Code }}</code> fragment to insert the code the user must copy to their SSH client. Other files in the webroot will be served as normal static files and not be processed as Go templates. </p>"},{"location":"reference/auth-webhook/","title":"Webhook authentication","text":"Webhook authentication SSH Authentication method Password Public-Key Keyboard-interactive GSSAPI Webhook support <p>The webhook authentication backend authenticates users by consulting an external authentication server implementing the ContainerSSH authentication API.</p> <p>The authentication webhook can be configured in the main configuration using the following structure:</p> <pre><code>auth:\n  password:\n    method: webhook\n    webhook:\n      url: https://myauthenticationserver.example.org\n      &lt;options&gt;\n  publicKey:\n    method: webhook\n    webhook:\n      url: https://myauthenticationserver.example.org\n      &lt;webhook options&gt;\n  authz:\n    method: webhook\n    webhook: \n      url: https://myauthenticationserver.example.org\n      &lt;webhook options&gt;\n</code></pre> <p>The following options are supported:</p> Name Type Description <code>authTimeout</code> <code>string</code> Timeout for the authentication process. HTTP calls that result in a non-200 response call will be retried until this timeout is reached. <p>Additional options here are described on the HTTP and TLS page. The <code>url</code> field must be provided.</p>"},{"location":"reference/auth-webhook/#the-authentication-webhook","title":"The authentication webhook","text":"<p>The authentication webhook is a simple JSON <code>POST</code> request to which the server must respond with a JSON response.</p> <p>Note</p> <p>We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code.</p> <p>Tip</p> <p>We provide a Go library to create an authentication server.</p> <p>Warning</p> <p>A warning about rate limiting: if the authentication server desires to do rate limiting for connecting users it should take into account that a user is allowed to try multiple authentication attempts (currently hard-coded to 6 per connection) before they are disconnected. Some of the authentication attempts (e.g. public keys) happen automatically on the client side without the user having any influence on them. Furthermore, ContainerSSH retries failed HTTP calls. To be effective the authentication server should count the unique connection identifiers seen in the <code>connectionId</code> field and implement a lock-out based on these.</p>"},{"location":"reference/auth-webhook/#password-authentication","title":"Password authentication","text":"<p>On password authentication the authentication server will receive the following request to the <code>/password</code> endpoint:</p> <pre><code>{\n  \"username\": \"username\",\n  \"remoteAddress\": \"127.0.0.1:1234\",\n  \"connectionId\": \"An opaque ID for the SSH connection\",\n  \"clientVersion\": \"SSH client version string\",\n  \"passwordBase64\": \"Base 64-encoded password\"\n}\n</code></pre>"},{"location":"reference/auth-webhook/#public-key-authentication","title":"Public key authentication","text":"<p>On public key authentication the authentication server will receive the following request to the <code>/pubkey</code> endpoint:</p> <pre><code>{\n  \"username\": \"username\",\n  \"remoteAddress\": \"127.0.0.1:1234\",\n  \"connectionId\": \"An opaque ID for the SSH connection\",\n  \"publicKey\": \"ssh-rsa ...\"\n}\n</code></pre> <p>The public key will be sent in the authorized key format. It is the responsibility of the authentication server only to verify that the provided public key matches the public key on record for the user. If a positive authentication response is received ContainerSSH will then continue to verify that the client has posession of the private key before completing the authentication process.</p>"},{"location":"reference/auth-webhook/#authorization","title":"Authorization","text":"<p>When the separate authorization webhook is configured, you will receive a separate request on the <code>/authz</code> endpoint. This is most useful when the primary authentication was done via other methods, such as OAuth2 or Kerberos.</p> <pre><code>{\n  \"username\": \"username\",\n  \"authenticatedUsername\": \"username obtained during authentication\",\n  \"remoteAddress\": \"127.0.0.1:1234\",\n  \"connectionId\": \"An opaque ID for the SSH connection\",\n  \"clientVersion\": \"SSH client version string\",\n  \"metadata\": {\n    \"metadata_name\": {\n      \"value\": \"metadata_value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"environment\": {\n    \"env_variable_name\": {\n      \"value\": \"env variable value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"files\": {\n    \"/path/to/file\": {\n      \"value\": \"base64-encoded contents of the file\",\n      \"sensitive\": true|false\n    }\n  }\n}\n</code></pre>"},{"location":"reference/auth-webhook/#response","title":"Response","text":"<p>The authorization server should always respond with a 200 status code regardless of the authentication status, non-200 status codes are reserved for (permanent or not) errors. The response, at a minimum, consists of a <code>success</code> field indicating if the authentication should be considered successful and the <code>authenticatedUsername</code> field which indicates the username of the authenticated user.</p> <pre><code>{\n  \"success\": true,\n  \"authenticatedUsername\": \"foo\",\n}\n</code></pre> <p>When responding the authentication server has the opportunity to define extra metadata, environment variables or files for the user connection. All three are forwarded to all following requests (e.g. Authentication Webhook -&gt; Authorization Webhook -&gt; Configuration Webhook) made and can be used to influence authorization or configuration decisions, the environment variables are added to the users environment when the connection is established and the files are placed in the container before the users command executes.</p> <p>If any metadata, environment variables or files are marked as sensitive they will not be re-transmitted with further webhook calls but they will be taken account of and added to the users environment or, in the case of files, placed in the container. This can be used to limit exposure in case the file contains sensitive information e.g. users credentials.</p> <p>All endpoints need to respond with an <code>application/json</code> response of the following content:</p> <pre><code>{\n  \"success\": true,\n  \"authenticatedUsername\": \"username that was verified\",\n  \"metadata\": {\n    \"metadata_name\": {\n      \"value\": \"metadata_value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"environment\": {\n    \"env_variable_name\": {\n      \"value\": \"env variable value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"files\": {\n    \"/path/to/file\": {\n      \"value\": \"base64-encoded contents of the file\",\n      \"sensitive\": true|false\n    }\n  }\n}\n</code></pre> <p>Tip</p> <p>We provide a Go library to implement an authentication server.</p>"},{"location":"reference/auth/","title":"Authentication","text":"Authentication <p>ContainerSSH does not have a built-in user database. It needs to use external services to verify user credentials, such as a webhook, OAuth2, or Kerberos. This page describes what authentication methods ContainerSSH supports and how you can tie them to your external authentication databases.</p>"},{"location":"reference/auth/#ssh-authentication-methods","title":"SSH authentication methods","text":"<p>This section gives a brief explanation on SSH authentication. In all authentication cases the SSH client may prompt the user for a username.</p>"},{"location":"reference/auth/#password-authentication","title":"Password authentication","text":"<p>When the SSH client requests a password authentication it will prompt the user for a password and submit it to the SSH server. The SSH server will then verify the password. In contrast to keyboard-interactive authentication (see below) there is no way to customize the password prompt.</p>"},{"location":"reference/auth/#public-key-authentication","title":"Public key authentication","text":"<p>In case of public key authentication the client has a cryptographic key. The public component of the cryptographic key is submitted to the server. The client signs this public key with its private key, so the server can verify that the client indeed holds the corresponding key. ContainerSSH can then verify if the public key belongs to the username that has been submitted.</p> <p>Additionally, you can sign these keys with an SSH certificate authority. Using certificate authorities simplify the key management since you do not have to put every single key in your database, having the CA certificate only is enough.</p> <p>Be warned though, the SSH certificate authority is not the same as the certificate authorities you may be used to from configuring a webserver. (x509 certificates) SSH certificate authorities are a lot simpler and do not have the same capabilities as x509 authorities. They do not provide the ability to chain certificates, and they also don't provide a  built-in mechanism for certificate revocation other than the expiration date of the certificate.</p>"},{"location":"reference/auth/#keyboard-interactive-authentication","title":"Keyboard-interactive authentication","text":"<p>At first glance keyboard-interactive authentication is very similar to password authentication. However, with this method the SSH server can provide customized questions to the client, to which the user has to provide answers. There can also be multiple consecutive questions and answers.</p>"},{"location":"reference/auth/#gssapi-kerberos","title":"GSSAPI / Kerberos","text":"<p>GSSAPI is a generic authentication interface for peer-to-peer authentication. It is mainly used as part of Kerberos authentication, which is the only implementation supported by ContainerSSH. It is often used in corporate environment to authenticate using existing Active-Directory or FreeIPA systems and support passwordless authentication.</p>"},{"location":"reference/auth/#authentication-backends-supported-by-containerssh","title":"Authentication backends supported by ContainerSSH","text":"<p>ContainerSSH supports a number of authentication backends. The table below summarizes which integration supports which SSH authentication method. You can configure multiple backends in parallel, but one SSH authentication method is always tied to one backend.</p> ContainerSSH Backend Password Public-Key Keyboard-interactive GSSAPI Webhook support OAuth2 backend Kerberos backend"},{"location":"reference/auth/#authorization","title":"Authorization","text":"<p>ContainerSSH offers a separate webhook to process authorization after the authentication is complete. This is useful in the case where you want users to prove their identity using a standard identity solution but would like to implement more complex access control rules in addition. The details of the authorization protocol are described on the webhook page.  If authorization is enabled the authorization webhook will be sent for every successful authentication attempt regardless of the backend used.</p>"},{"location":"reference/auth/#relationship-between-ssh-authenticated-and-container-username","title":"Relationship between SSH, authenticated, and container username","text":""},{"location":"reference/auth/#configuration","title":"Configuration","text":"<p>The following configuration variables are used to configure the authentication systems each SSH authentication method is routed to:</p> <pre><code>auth:\n  password:\n    method: \"\"|webhook|kerberos\n    webhook: \n      &lt;webhook options&gt;\n    kerberos:\n      &lt;kerberos options&gt;\n  publicKey:\n    method: \"\"|webhook\n    webhook: \n      &lt;webhook options&gt;\n  keyboardInteractive:\n    method: \"\"|oauth2\n    oauth2: \n      &lt;oauth2 options&gt;\n  gssapi:\n    method: \"\"|kerberos\n    kerberos:\n      &lt;kerberos options&gt;\n  authz:\n    method: \"\"|webhook\n    webhook: \n      &lt;webhook options&gt;\n</code></pre>"},{"location":"reference/auth/#detailed-configuration-options","title":"Detailed configuration options","text":"<ul> <li>Webhook configuration options</li> <li>OAuth2 configuration options</li> <li>Kerberos configuration options</li> </ul>"},{"location":"reference/backends/","title":"Backend Selection","text":"Backend selection <p>ContainerSSH is built to support multiple backends. The backend can be changed in the configuration file:</p> <pre><code>backend: &lt;backend type&gt;\n</code></pre> <p>ContainerSSH currently supports the following backends:</p> Backend Description <code>docker</code> Runs Docker containers. <code>kubernetes</code> Runs Kubernetes containers. <code>sshproxy</code> Forwards SSH connections to a backend server."},{"location":"reference/codes/","title":"Message codes","text":"<p>This page contains all message codes logged by ContainerSSH. Some of these are errors, while others only give you status information. Many of these messages are only logged when the log level is set to <code>debug</code>.</p>"},{"location":"reference/codes/#core","title":"Core","text":"Code Explanation <code>CORE_CONFIG_CANNOT_WRITE_FILE</code> ContainerSSH cannot update the configuration file with the new host keys and will only use the host key for the current run. <code>CORE_CONFIG_ERROR</code> ContainerSSH encountered an error in the configuration. <code>CORE_CONFIG_FILE</code> ContainerSSH is reading the configuration file <code>CORE_HOST_KEY_GENERATION_FAILED</code> ContainerSSH could not generate host keys and is aborting the run. <code>CORE_NO_HOST_KEYS</code> The configuration does not contain host keys. ContainerSSH will attempt to generate host keys and update the configuration file."},{"location":"reference/codes/#auditlog","title":"Auditlog","text":"Code Explanation <code>AUDIT_S3_CANNOT_CLOSE_METADATA_FILE_HANDLE</code> ContainerSSH could not close the metadata file in the local folder. This typically happens when the local folder is on an NFS share. (This is NOT supported.) <code>AUDIT_S3_CLOSE_FAILED</code> ContainerSSH failed to close an audit log file in the local directory. This usually happens when the local directory is on an NFS share. (This is NOT supported.) <code>AUDIT_S3_FAILED_CREATING_METADATA_FILE</code> ContainerSSH failed to create the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is writable and has enough disk space. <code>AUDIT_S3_FAILED_METADATA_JSON_ENCODING</code> ContainerSSH failed to encode the metadata file. This is a bug, please report it. <code>AUDIT_S3_FAILED_READING_METADATA_FILE</code> ContainerSSH failed to read the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is readable and the files have not been corrupted. <code>AUDIT_S3_FAILED_STAT_QUEUE_ENTRY</code> ContainerSSH failed to stat the queue file. This usually happens when the local directory is being manually manipulated. <code>AUDIT_S3_FAILED_WRITING_METADATA_FILE</code> ContainerSSH failed to write the local metadata file. Please check if your disk has enough disk space. <code>AUDIT_S3_MULTIPART_ABORTING</code> ContainerSSH is aborting a multipart upload. Check the log message for details. <code>AUDIT_S3_MULTIPART_FAILED_ABORT</code> ContainerSSH failed aborting a multipart upload from a previously crashed ContainerSSH run. <code>AUDIT_S3_MULTIPART_FAILED_LIST</code> ContainerSSH failed to list multipart uploads on the object storage bucket. This is needed to abort uploads from a previously crashed ContainerSSH run. <code>AUDIT_S3_MULTIPART_PART_UPLOADING</code> ContainerSSH is uploading a part of an audit log to the S3-compatible object storage. <code>AUDIT_S3_MULTIPART_PART_UPLOAD_COMPLETE</code> ContainerSSH completed the upload of an audit log part to the S3-compatible object storage. <code>AUDIT_S3_MULTIPART_PART_UPLOAD_FAILED</code> ContainerSSH failed to upload a part to the S3-compatible object storage. Check the message for details. <code>AUDIT_S3_MULTIPART_UPLOAD</code> ContainerSSH is starting a new S3 multipart upload. <code>AUDIT_S3_MULTIPART_UPLOAD_FINALIZATION_FAILED</code> ContainerSSH has uploaded all audit log parts, but could not finalize the multipart upload. <code>AUDIT_S3_MULTIPART_UPLOAD_FINALIZED</code> ContainerSSH has uploaded all audit log parts and has successfully finalized the upload. <code>AUDIT_S3_MULTIPART_UPLOAD_FINALIZING</code> ContainerSSH has uploaded all audit log parts and is now finalizing the multipart upload. <code>AUDIT_S3_MULTIPART_UPLOAD_INITIALIZATION_FAILED</code> ContainerSSH failed to initialize a new multipart upload to the S3-compatible object storage. Check if the S3 configuration is correct and the provided S3 access key and secrets have permissions to start a multipart upload. <code>AUDIT_S3_NO_SUCH_QUEUE_ENTRY</code> ContainerSSH was trying to upload an audit log from the metadata file, but the audit log does not exist. <code>AUDIT_S3_RECOVERING</code> ContainerSSH found a previously aborted multipart upload locally and is now attempting to recover the upload. <code>AUDIT_S3_REMOVE_FAILED</code> ContainerSSH failed to remove an uploaded audit log from the local directory. This usually happens on Windows when a different process has the audit log open. (This is not a supported setup.) <code>AUDIT_S3_SINGLE_UPLOAD</code> ContainerSSH is uploading the full audit log in a single upload to the S3-compatible object storage. This happens when the audit log size is below the minimum size for a multi-part upload. <code>AUDIT_S3_SINGLE_UPLOAD_COMPLETE</code> ContainerSSH successfully uploaded the audit log as a single upload. <code>AUDIT_S3_SINGLE_UPLOAD_FAILED</code> ContainerSSH failed to upload the audit log as a single upload."},{"location":"reference/codes/#authentication","title":"Authentication","text":"Code Explanation <code>AUTH</code> ContainerSSH is trying to contact the authentication backend to verify the user credentials. <code>AUTH_BACKEND_ERROR</code> The ContainerSSH authentication server responded with a non-200 status code. ContainerSSH will retry the authentication for a few times before giving up. This is most likely a bug in your authentication server, please check your logs. <code>AUTH_FAILED</code> The user has provided invalid credentials and the authentication is rejected. <code>AUTH_INVALID_STATUS</code> This message indicates that the authentication server returned an invalid HTTP status code. <code>AUTH_NOT_SUPPORTED</code> The authentication method the client requested is not supported by ContainerSSH. <code>AUTH_SUCCESSFUL</code> The user has provided the correct credentials and the authentication is accepted."},{"location":"reference/codes/#backend","title":"Backend","text":"Code Explanation <code>BACKEND_CONFIG_ERROR</code> The backend retreived from the configuration server is invalid. See the error message for details."},{"location":"reference/codes/#configuration","title":"Configuration","text":"Code Explanation <code>CONFIG_BACKEND_ERROR</code> ContainerSSH has received an invalid response from the configuration server or the network connection broke. ContainerSSH will retry fetching the user-specific configuration until the timeout. If this error persists check the connectivity to the configuration server, or the logs of the configuration server itself to find out of there may be a specific error. <code>CONFIG_INVALID_STATUS_CODE</code> ContainerSSH has received a non-200 response code when calling a per-user backend configuration from the configuration server. <code>CONFIG_REQUEST</code> ContainerSSH is sending a quest to the configuration server to obtain a per-user backend configuration. <code>CONFIG_RESPONSE</code> ContainerSSH has received a per-user backend configuration from the configuration server. <code>CONFIG_SERVER_AVAILABLE</code> The ContainerSSH configuration server is now available at the specified address."},{"location":"reference/codes/#docker","title":"Docker","text":"Code Explanation <code>DOCKER_AGENT_READ_FAILED</code> The ContainerSSH Docker module failed to read from the ContainerSSH agent. This is most likely because the ContainerSSH guest agent is not present in the guest image, but agent support is enabled. <code>DOCKER_CLOSE_INPUT_FAILED</code> The ContainerSSH Docker module attempted to close the input (stdin) for reading but failed to do so. <code>DOCKER_CLOSE_OUTPUT_FAILED</code> The ContainerSSH Docker module attempted to close the output (stdout and stderr) for writing but failed to do so. <code>DOCKER_CONFIG_ERROR</code> The ContainerSSH Docker module detected a configuration error. Please check your configuration. <code>DOCKER_CONTAINER_ATTACH</code> The ContainerSSH Docker module is attaching to a container in session mode. <code>DOCKER_CONTAINER_ATTACH_FAILED</code> The ContainerSSH Docker module has failed to attach to a container in session mode. <code>DOCKER_CONTAINER_CREATE</code> The ContainerSSH Docker module is creating a container. <code>DOCKER_CONTAINER_CREATE_FAILED</code> The ContainerSSH Docker module failed to create a container. This may be a temporary and retried or a permanent error message. Check the log message for details. <code>DOCKER_CONTAINER_REMOVE</code> The ContainerSSH Docker module os removing the container. <code>DOCKER_CONTAINER_REMOVE_FAILED</code> The ContainerSSH Docker module could not remove the container. This message may be temporary and retried or permanent. Check the log message for details. <code>DOCKER_CONTAINER_REMOVE_SUCCESSFUL</code> The ContainerSSH Docker module has successfully removed the container. <code>DOCKER_CONTAINER_SHUTTING_DOWN</code> The ContainerSSH Docker module is shutting down a container. <code>DOCKER_CONTAINER_SIGNAL</code> The ContainerSSH Docker module is sending a signal to the container. <code>DOCKER_CONTAINER_SIGNAL_FAILED</code> The ContainerSSH Docker module has failed to send a signal to the container. <code>DOCKER_CONTAINER_START</code> The ContainerSSH Docker module is starting the previously-created container. <code>DOCKER_CONTAINER_START_FAILED</code> The ContainerSSH docker module failed to start the container. This message can either be temporary and retried or permanent. Check the log message for details. <code>DOCKER_CONTAINER_STOP</code> The ContainerSSH Docker module is stopping the container. <code>DOCKER_CONTAINER_STOP_FAILED</code> The ContainerSSH Docker module failed to stop the container. This message can be either temporary and retried or permanent. Check the log message for details. <code>DOCKER_EXEC</code> The ContainerSSH Docker module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the container. <code>DOCKER_EXEC_ATTACH</code> The ContainerSSH Docker module is attaching to the previously-created execution. <code>DOCKER_EXEC_ATTACH_FAILED</code> The ContainerSSH Docker module could not attach to the previously-created execution. <code>DOCKER_EXEC_CREATE</code> The ContainerSSH Docker module is creating an execution. <code>DOCKER_EXEC_CREATE_FAILED</code> The ContainerSSH Docker module has failed to create an execution. This can be temporary and retried or permanent. See the error message for details. <code>DOCKER_EXEC_PID_READ_FAILED</code> The ContainerSSH Docker module has failed to read the process ID from the ContainerSSH Guest Agent. This is most likely because the guest image does not contain the guest agent, but guest agent support has been enabled. <code>DOCKER_EXEC_RESIZE</code> The ContainerSSH Docker module is resizing the console. <code>DOCKER_EXEC_RESIZE_FAILED</code> The ContainerSSH Docker module failed to resize the console. <code>DOCKER_EXEC_SIGNAL</code> The ContainerSSH Docker module is delivering a signal in container mode. <code>DOCKER_EXEC_SIGNAL_FAILED</code> The ContainerSSH Docker module failed to deliver a signal. <code>DOCKER_EXEC_SIGNAL_FAILED_NO_AGENT</code> The ContainerSSH Docker module failed to deliver a signal because ContainerSSH Guest Agent support is disabled. <code>DOCKER_EXEC_SIGNAL_SUCCESSFUL</code> The ContainerSSH Docker module successfully delivered the requested signal. <code>DOCKER_EXIT_CODE</code> The ContainerSSH Docker module is fetching the exit code from the program. <code>DOCKER_EXIT_CODE_CONTAINER_RESTARTING</code> The ContainerSSH Docker module could not fetch the exit code from the program because the container is restarting. This is typically a misconfiguration as ContainerSSH containers should not automatically restart. <code>DOCKER_EXIT_CODE_FAILED</code> The ContainerSSH Docker module has failed to fetch the exit code of the program. <code>DOCKER_EXIT_CODE_NEGATIVE</code> The ContainerSSH Docker module has received a negative exit code from Docker. This should never happen and is most likely a bug. <code>DOCKER_EXIT_CODE_STILL_RUNNING</code> The ContainerSSH Docker module could not fetch the program exit code because the program is still running. This error may be temporary and retried or permanent. <code>DOCKER_GUEST_AGENT_DISABLED</code> The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the container image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. <code>DOCKER_IMAGE_LISTING</code> The ContainerSSH Docker module is listing the locally present container images to determine if the specified container image needs to be pulled. <code>DOCKER_IMAGE_LISTING_FAILED</code> The ContainerSSH Docker module failed to list the images present in the local Docker daemon. This is used to determine if the image needs to be pulled. This can be because the Docker daemon is not reachable, the certificate is invalid, or there is something else interfering with listing the images. <code>DOCKER_IMAGE_PULL</code> The ContainerSSH Docker module is pulling the container image. <code>DOCKER_IMAGE_PULL_FAILED</code> The ContainerSSH Docker module failed to pull the specified container image. This can be because of connection issues to the Docker daemon, or because the Docker daemon itself can't pull the image. If you don't intend to have the image pulled you should set the <code>ImagePullPolicy</code> to <code>Never</code>. See the Docker documentation for details. <code>DOCKER_IMAGE_PULL_NEEDED_CHECKING</code> The ContainerSSH Docker module is checking if an image pull is needed. <code>DOCKER_PROGRAM_ALREADY_RUNNING</code> The ContainerSSH Docker module can't execute the request because the program is already running. This is a client error. <code>DOCKER_SIGNAL_FAILED_NO_PID</code> The ContainerSSH Docker module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. <code>DOCKER_STREAM_INPUT_FAILED</code> The ContainerSSH Docker module failed to stream stdin to the Docker engine. <code>DOCKER_STREAM_OUTPUT_FAILED</code> The ContainerSSH Docker module failed to stream stdout and stderr from the Docker engine. <code>DOCKER_SUBSYSTEM_NOT_SUPPORTED</code> The ContainerSSH Docker module is not configured to run the requested subsystem."},{"location":"reference/codes/#http","title":"HTTP","text":"Code Explanation <code>HTTP_CLIENT_CONNECTION_FAILED</code> This message indicates a connection failure on the network level. <code>HTTP_CLIENT_DECODE_FAILED</code> This message indicates that decoding the JSON response has failed. The status code is set for this code. <code>HTTP_CLIENT_ENCODE_FAILED</code> This message indicates that JSON encoding the request failed. This is usually a bug. <code>HTTP_CLIENT_REDIRECT</code> This message indicates that the server responded with a HTTP redirect. <code>HTTP_CLIENT_REDIRECTS_DISABLED</code> This message indicates that ContainerSSH is not following a HTTP redirect sent by the server. Use the allowRedirects option to allow following HTTP redirects. <code>HTTP_CLIENT_REQUEST</code> This message indicates that a HTTP request is being sent from ContainerSSH <code>HTTP_CLIENT_RESPONSE</code> This message indicates that ContainerSSH received a HTTP response from a server. <code>HTTP_SERVER_ENCODE_FAILED</code> The HTTP server failed to encode the response object. This is a bug, please report it. <code>HTTP_SERVER_RESPONSE_WRITE_FAILED</code> The HTTP server failed to write the response."},{"location":"reference/codes/#kubernetes","title":"Kubernetes","text":"Code Explanation <code>KUBERNETES_CLOSE_OUTPUT_FAILED</code> The ContainerSSH Kubernetes module attempted to close the output (stdout and stderr) for writing but failed to do so. <code>KUBERNETES_CONFIG_ERROR</code> The ContainerSSH Kubernetes module detected a configuration error. Please check your configuration. <code>KUBERNETES_EXEC</code> The ContainerSSH Kubernetes module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the pod. <code>KUBERNETES_EXEC_RESIZE</code> The ContainerSSH Kubernetes module is resizing the terminal window. <code>KUBERNETES_EXEC_RESIZE_FAILED</code> The ContainerSSH Kubernetes module failed to resize the console. <code>KUBERNETES_EXEC_SIGNAL</code> The ContainerSSH Kubernetes module is delivering a signal. <code>KUBERNETES_EXEC_SIGNAL_FAILED</code> The ContainerSSH Kubernetes module failed to deliver a signal. <code>KUBERNETES_EXEC_SIGNAL_FAILED_NO_AGENT</code> The ContainerSSH Kubernetes module failed to deliver a signal because guest agent support is disabled. <code>KUBERNETES_EXEC_SIGNAL_SUCCESSFUL</code> The ContainerSSH Kubernetes module successfully delivered the requested signal. <code>KUBERNETES_EXIT_CODE_FAILED</code> The ContainerSSH Kubernetes module has failed to fetch the exit code of the program. <code>KUBERNETES_GUEST_AGENT_DISABLED</code> The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the pod image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. <code>KUBERNETES_PID_RECEIVED</code> The ContainerSSH Kubernetes module has received a PID from the Kubernetes guest agent. <code>KUBERNETES_POD_ATTACH</code> The ContainerSSH Kubernetes module is attaching to a pod in session mode. <code>KUBERNETES_POD_CREATE</code> The ContainerSSH Kubernetes module is creating a pod. <code>KUBERNETES_POD_CREATE_FAILED</code> The ContainerSSH Kubernetes module failed to create a pod. This may be a temporary and retried or a permanent error message. Check the log message for details. <code>KUBERNETES_POD_REMOVE</code> The ContainerSSH Kubernetes module is removing a pod. <code>KUBERNETES_POD_REMOVE_FAILED</code> The ContainerSSH Kubernetes module could not remove the pod. This message may be temporary and retried or permanent. Check the log message for details. <code>KUBERNETES_POD_REMOVE_SUCCESSFUL</code> The ContainerSSH Kubernetes module has successfully removed the pod. <code>KUBERNETES_POD_SHUTTING_DOWN</code> The ContainerSSH Kubernetes module is shutting down a pod. <code>KUBERNETES_POD_WAIT</code> The ContainerSSH Kubernetes module is waiting for the pod to come up. <code>KUBERNETES_POD_WAIT_FAILED</code> The ContainerSSH Kubernetes module failed to wait for the pod to come up. Check the error message for details. <code>KUBERNETES_PROGRAM_ALREADY_RUNNING</code> The ContainerSSH Kubernetes module can't execute the request because the program is already running. This is a client error. <code>KUBERNETES_PROGRAM_NOT_RUNNING</code> This message indicates that the user requested an action that can only be performed when a program is running, but there is currently no program running. <code>KUBERNETES_SIGNAL_FAILED_EXITED</code> The ContainerSSH Kubernetes module can't deliver a signal because the program already exited. <code>KUBERNETES_SIGNAL_FAILED_NO_PID</code> The ContainerSSH Kubernetes module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. <code>KUBERNETES_SUBSYSTEM_NOT_SUPPORTED</code> The ContainerSSH Kubernetes module is not configured to run the requested subsystem. <code>KUBERUN_DEPRECATED</code> This message indicates that you are still using the deprecated KubeRun backend. This backend doesn't support all safety and functionality improvements and will be removed in the future. Please read the deprecation notice for a migration guide <code>KUBERUN_EXEC_DISABLED</code> This message indicates that the user tried to execute a program, but program execution is disabled in the legacy KubeRun configuration. <code>KUBERUN_INSECURE</code> This message indicates that you are using Kubernetes in the \"insecure\" mode where certificate verification is disabled. This is a major security flaw, has been deprecated and is removed in the new Kubernetes backend. Please change your configuration to properly validates the server certificates."},{"location":"reference/codes/#log","title":"Log","text":"Code Explanation <code>LOG_FILE_OPEN_FAILED</code> ContainerSSH failed to open the specified log file. <code>LOG_ROTATE_FAILED</code> ContainerSSH cannot rotate the logs as requested because of an underlying error. <code>LOG_WRITE_FAILED</code> ContainerSSH cannot write to the specified log file. This usually happens because the underlying filesystem is full or the log is located on a non-local storage (e.g. NFS), which is not supported. <code>TEST</code> This is message that should only be seen in unit and component tests, never in production. <code>UNKNOWN_ERROR</code> This is an untyped error. If you see this in a log that is a bug and should be reported."},{"location":"reference/codes/#metrics","title":"Metrics","text":"Code Explanation <code>METRICS_AVAILABLE</code> The metrics service is now online and ready for service."},{"location":"reference/codes/#security","title":"Security","text":"Code Explanation <code>SECURITY_ENV_REJECTED</code> ContainerSSH rejected setting the environment variable because it does not pass the security settings. <code>SECURITY_EXEC_FAILED_SETENV</code> Program execution failed in conjunction with the forceCommand option because ContainerSSH could not set the <code>SSH_ORIGINAL_COMMAND</code> environment variable on the backend. <code>SECURITY_EXEC_FORCING_COMMAND</code> ContainerSSH is replacing the command passed from the client (if any) to the specified command and is setting the <code>SSH_ORIGINAL_COMMAND</code> environment variable. <code>SECURITY_EXEC_REJECTED</code> A program execution request has been rejected because it doesn't conform to the security settings. <code>SECURITY_SHELL_REJECTED</code> ContainerSSH rejected launching a shell due to the security settings. <code>SECURITY_SIGNAL_REJECTED</code> ContainerSSH rejected delivering a signal because it does not pass the security settings. <code>SECURITY_SUBSYSTEM_REJECTED</code> ContainerSSH rejected the subsystem because it does pass the security settings. <code>SECURITY_TTY_REJECTED</code> ContainerSSH rejected the pseudoterminal request because of the security settings."},{"location":"reference/codes/#service","title":"Service","text":"Code Explanation <code>SERVICE_CRASHED</code> A ContainerSSH has stopped improperly. <code>SERVICE_POOL_RUNNING</code> All ContainerSSH services are now running. <code>SERVICE_POOL_STARTING</code> All ContainerSSH services are starting. <code>SERVICE_POOL_STOPPED</code> ContainerSSH has stopped all services. <code>SERVICE_POOL_STOPPING</code> ContainerSSH is stopping all services. <code>SERVICE_RUNNING</code> A ContainerSSH service is now running <code>SERVICE_STARTING</code> ContainerSSH is starting a component service <code>SERVICE_STOPPED</code> A ContainerSSH service has stopped. <code>SERVICE_STOPPING</code> A ContainerSSH service is now stopping."},{"location":"reference/codes/#ssh","title":"SSH","text":"Code Explanation <code>SSH_ALREADY_RUNNING</code> The SSH server is already running and has been started again. This is a bug, please report it. <code>SSH_AUTH_FAILED</code> The user has provided invalid credentials. <code>SSH_AUTH_SUCCESSFUL</code> The user has provided valid credentials and is now authenticated. <code>SSH_AUTH_UNAVAILABLE</code> The user has requested an authentication method that is currently unavailable. <code>SSH_AVAILABLE</code> The SSH service is now online and ready for service. <code>SSH_BACKEND_REJECTED_HANDSHAKE</code> The backend has rejected the connecting user after successful authentication. <code>SSH_CHANNEL_REQUEST</code> The user has send a new channel-specific request. <code>SSH_CHANNEL_REQUEST_FAILED</code> ContainerSSH couldn't fulfil the channel-specific request. <code>SSH_CHANNEL_REQUEST_SUCCESSFUL</code> ContainerSSH has successfully processed the channel-specific request. <code>SSH_CONNECTED</code> A user has connected over SSH. <code>SSH_DECODE_FAILED</code> ContainerSSH failed to decode something from the user. This is either a bug in ContainerSSH or in the connecting client. <code>SSH_DISCONNECTED</code> An SSH connection has been severed. <code>SSH_EXIT</code> ContainerSSH is sending the exit code of the program to the user. <code>SSH_EXIT_CODE_FAILED</code> ContainerSSH failed to obtain and send the exit code of the program to the user. <code>SSH_EXIT_SIGNAL</code> ContainerSSH is sending the exit signal from an abnormally exited program to the user. <code>SSH_HANDSHAKE_FAILED</code> The connecting party failed to establish a secure SSH connection. This is most likely due to invalid credentials or a backend error. <code>SSH_HANDSHAKE_SUCCESSFUL</code> The user has provided valid credentials and has now established an SSH connection. <code>SSH_LISTEN_CLOSE_FAILED</code> ContainerSSH failed to close the listen socket. <code>SSH_NEW_CHANNEL</code> A user has established a new SSH channel. (Not connection!) <code>SSH_NEW_CHANNEL_REJECTED</code> The user has requested a new channel to be opened, but was rejected. <code>SSH_REPLY_SEND_FAILED</code> ContainerSSH couldn't send the reply to a request to the user. This is usually the case if the user suddenly disconnects. <code>SSH_START_FAILED</code> ContainerSSH failed to start the SSH service. This is usually because of invalid configuration. <code>SSH_UNSUPPORTED_CHANNEL_TYPE</code> The user requested a channel type that ContainerSSH doesn't support (e.g. TCP/IP forwarding). <code>SSH_UNSUPPORTED_GLOBAL_REQUEST</code> The users client has send a global request ContainerSSH does not support. This is nothing to worry about."},{"location":"reference/configserver/","title":"Configuration server","text":"Configuration Server <p>ContainerSSH has the ability to configure the backend, and the launched container dynamically based on the username and/or IP address. To do this ContainerSSH calls out to a configuration server if configured.</p>"},{"location":"reference/configserver/#configuration","title":"Configuration","text":"<p>The configserver webhook can be configured in the main configuration using the following structure:</p> <pre><code>configserver:\n  &lt;options&gt;\n</code></pre> <p>The options here are described on the HTTP and TLS page. If no <code>url</code> is provided the configuration webhook is disabled.</p>"},{"location":"reference/configserver/#the-configuration-webhook","title":"The configuration webhook","text":"<p>The configuration webhook is a simple JSON <code>POST</code> request to which the server must respond with a JSON response.</p> <p>Note</p> <p>We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code.</p> <p>Tip</p> <p>We provide a Go library to create a configuration server.</p> <p>The config server will receive a request in following format:</p> <pre><code>{\n  \"username\": \"username\",\n  \"authenticatedUsername\": \"username obtained during authentication\",\n  \"remoteAddress\": \"127.0.0.1:1234\",\n  \"connectionId\": \"An opaque ID for the SSH connection\",\n  \"clientVersion\": \"SSH client version string\",\n  \"metadata\": {\n    \"metadata_name\": {\n      \"value\": \"metadata_value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"environment\": {\n    \"env_variable_name\": {\n      \"value\": \"env variable value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"files\": {\n    \"/path/to/file\": {\n      \"value\": \"base64-encoded contents of the file\",\n      \"sensitive\": true|false\n    }\n  }\n}\n</code></pre> <p>The configuration server will have to respond with the following response accompanied with the content type of <code>application/json</code>. </p> <pre><code>{\n  \"config\": {\n    // Provide a partial configuration here \n  },\n  \"metadata\": {\n    \"metadata_name\": {\n      \"value\": \"metadata_value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"environment\": {\n    \"env_variable_name\": {\n      \"value\": \"env variable value\",\n      \"sensitive\": true|false\n    }\n  },\n  \"files\": {\n    \"/path/to/file\": {\n      \"value\": \"base64-encoded contents of the file\",\n      \"sensitive\": true|false\n    }\n  }\n}\n</code></pre> <p>The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running <code>./containerssh --dump-config</code>. The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file.</p> <p>Currently only the following options can be set from the configuration server:</p> <ul> <li>Backend</li> <li>Docker</li> <li>Kubernetes</li> <li>Security</li> </ul> <p>Tip</p> <p>We provide a Go library to implement a config server.</p>"},{"location":"reference/docker/","title":"Docker","text":"The Docker backend <p>The Docker backend should work with any Docker Engine version starting with 1.6 thanks to the version negotiation present. We fix issues starting with Docker version 18.02.</p> <p>Tip</p> <p>This is the documentation for the Docker backend. For deploying ContainerSSH inside Docker please see the installation guide.</p>"},{"location":"reference/docker/#the-base-configuration-structure","title":"The base configuration structure","text":"<p>In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server:</p> <pre><code>backend: docker\ndocker:\n  connection:\n    &lt;connection configuration here&gt;\n  execution:\n    &lt;execution configuration here&gt;\n  timeouts:\n    &lt;timeouts configuration here&gt;\n</code></pre>"},{"location":"reference/docker/#configuring-connection-parameters","title":"Configuring connection parameters","text":"<p>The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is <code>unix:///var/run/docker.sock</code>, on Windows <code>npipe:////./pipe/docker_engine</code>. ContainerSSH must have permissions to access the Docker socket.</p> <p>The Docker socket location can be changed with the <code>host</code> option:</p> <pre><code>docker:\n  connection:\n    host: 127.0.0.1:2375\n</code></pre> <p>However, exposing Docker without certificate authentication is dangerous. It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options:</p> <pre><code>docker:\n  connection:\n    host: &lt;ip and port of the Docker engine&gt;\n    cert: |\n      -----BEGIN CERTIFICATE-----\n      &lt;client certificate here&gt;\n      -----END CERTIFICATE-----\n    key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      &lt;client key here&gt;\n      -----END RSA PRIVATE KEY-----\n    cacert: |\n      -----BEGIN CERTIFICATE-----\n      &lt;CA certificate here&gt;\n      -----END CERTIFICATE-----\n</code></pre>"},{"location":"reference/docker/#configuring-container-execution","title":"Configuring container execution","text":"<p>Container execution options can be specified as follows:</p> <pre><code>docker:\n  execution:\n    container:\n      image: containerssh/containerssh\n      &lt;other container options&gt;\n    host:\n      &lt;host options&gt;\n    network:\n      &lt;network options&gt;\n    platform:\n      &lt;platform options&gt;\n    containerName: \"&lt;container name here&gt;\"\n    &lt;ContainerSSH-specific options here&gt;\n</code></pre> <p>The <code>container</code>, <code>host</code>, <code>network</code>, and <code>platform</code> options contain settings described in the Docker API.</p> <p>The <code>containerName</code> option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases.</p>"},{"location":"reference/docker/#basic-container-configuration","title":"Basic container configuration","text":"<p>The basic configuration options are as follows:</p> <pre><code>docker:\n  execution:\n    container:\n      image: containerssh/containerssh\n      env:\n        - VAR=value\n      # cmd is only respected in session mode, see below.\n      cmd: \n        - /run/this/command\n      user: ubuntu\n</code></pre>"},{"location":"reference/docker/#mounting-volumes","title":"Mounting volumes","text":"<p>Volumes can be mounted in 3 ways:</p> <ol> <li>Using bind mounts</li> <li>Using mounts</li> <li>Using tmpfs</li> </ol>"},{"location":"reference/docker/#bind-mounts","title":"Bind mounts","text":"<p>Bind mounts mount a directory from the host into the container. The syntax is fairly simple:</p> <pre><code>docker:\n  execution:\n    host:\n      binds:\n        - \"/path-on-the-host:/path-in-the-container[:options]\"\n</code></pre> <p>Instead of the host path you can also specify a volume name. The following options are suported:</p> <code>nocopy</code> If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. <code>ro|rw</code> Set volume to read-only or read-write. <code>z</code> Apply the <code>shared</code> SELinux label to the volume allowing multiple containers to write the same volume. <code>Z</code> Apply the <code>private unshared</code> SELinux label to the volume allowing only the current container to use it. <code>[r]shared, [r]slave, [r]private</code> Sets the mount propagation behavior of the volume."},{"location":"reference/docker/#mounts","title":"Mounts","text":"<p>The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows:</p> <pre><code>docker:\n  execution:\n    host:\n      mounts:\n        - target: /path/in/container\n          source: &lt;volume name, host path, etc&gt;\n          type: &lt;bind|volume|tmpfs|npipe&gt;\n          readonly: &lt;true|false&gt;\n          consistency: &lt;default|consistent|cached|delegated&gt;\n          # For bind type only:\n          bindoptions:\n            propagation: &lt;private|rprivate|shared|rshared|slave|rslave&gt;\n            #Disable recursive bind mount\n            nonrecursive: &lt;true|false&gt;\n          # For volume type only:\n          volumeoptions: \n            # Disable copying files from the image to the volume\n            nocopy: &lt;true|false&gt; \n            labels:\n              - key: value\n            driverconfig:\n              name: drivername\n              options:\n                &lt;driveroption&gt;: &lt;drivervalue&gt;\n          # For tmpfs type only:\n          tmpfsoptions:\n            sizebytes: &lt;size in bytes&gt;\n            mode: 0755\n</code></pre>"},{"location":"reference/docker/#tmpfs","title":"tmpfs","text":"<p>The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed.</p> <pre><code>docker:\n  execution:\n    host:\n      tmpfs:\n        /container/directory: &lt;options&gt;\n</code></pre> <p>The detailed options for tmpfs can be found on the tmpfs man page.</p>"},{"location":"reference/docker/#other-options","title":"Other options","text":"<p>Apart from the <code>container</code>, <code>host</code>, <code>network</code>, <code>platform</code> and <code>containerName</code> options ContainerSSH has the following options for execution. These should not be changed unless required.</p> Name Type Description <code>mode</code> <code>string</code> Specify <code>connection</code> to launch one container per SSH connection or <code>session</code> to run one container per SSH session (multiple containers per connection). In connection mode the container is started with the <code>idleCommand</code> as the first program and every session is launched similar to how <code>docker exec</code> runs programs. In session mode the command is launched directly. <code>idleCommand</code> <code>[]string</code> Specifies the command to run as the first process in the container in <code>connection</code> mode. Parameters must be provided as separate items in the array. Has no effect in <code>session</code> mode. <code>shellCommand</code> <code>[]string</code> Specifies the command to run as a shell in <code>connection</code> mode. Parameters must be provided as separate items in the array. Has no effect in <code>session</code> mode. <code>agentPath</code> <code>string</code> Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. <code>disableAgent</code> <code>bool</code> Disable the ContainerSSH guest agent. This will disable several functions and is not recommended. <code>subsystems</code> <code>map[string]string</code> Specifies a map of subsystem names to executables. It is recommended to set at least the <code>sftp</code> subsystem as many users will want to use it. <code>imagePullPolicy</code> <code>Never,IfNotPresent,Always</code> Specifies when to pull the container image. Defaults to <code>IfNotPresent</code>, which pulls the image when it is not locally present or if the image has no tag/has the <code>latest</code> tag. It is recommended that you provide a custom, versioned image name to prevent pulling the image at every connection."},{"location":"reference/docker/#configuring-timeouts","title":"Configuring timeouts","text":"<p>The <code>timeouts</code> section has the following options. All options can use time units (e.g. <code>60s</code>) and default to nanoseconds without time units.</p> Name Description <code>containerStart</code> The time to wait for the container to start. <code>containerStop</code> The time to wait for the container to stop. <code>commandStart</code> The time to wait for the command to start in <code>connection</code> mode. <code>signal</code> The time to wait to deliver a signal to a process. <code>window</code> The time to wait to deliver a window size change. <code>http</code> The time to wait for the underlying HTTP calls to complete."},{"location":"reference/docker/#securing-docker","title":"Securing Docker","text":"<p>Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine.</p> <p>For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server.</p>"},{"location":"reference/docker/#securing-the-docker-socket","title":"Securing the Docker socket","text":"<p>Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation.</p> <p>The certificates can be provided for ContainerSSH using the following fields:</p> <pre><code>docker:\n  connection:\n    host: &lt;ip and port of the Docker engine&gt;\n    cert: |\n      -----BEGIN CERTIFICATE-----\n      &lt;client certificate here&gt;\n      -----END CERTIFICATE-----\n    key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      &lt;client key here&gt;\n      -----END RSA PRIVATE KEY-----\n    cacert: |\n      -----BEGIN CERTIFICATE-----\n      &lt;CA certificate here&gt;\n      -----END CERTIFICATE-----\n</code></pre> <p>Danger</p> <p>Never expose the Docker socket on TCP without configuring certificates! </p>"},{"location":"reference/docker/#preventing-root-escalation","title":"Preventing root escalation","text":"<p>Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid <code>1000</code> as follows:</p> <pre><code>docker:\n  execution:\n    container:\n      user: 1000\n</code></pre> <p>If root is required inside the container user namespace mapping. There are a few steps required to make this happen:</p> <p>First, you need to create the files called <code>/etc/subuid</code> and <code>/etc/subgid</code>. These files have the following format:</p> <pre><code>&lt;username/uid&gt;:&lt;startuid/gid&gt;:&lt;uid/gid count&gt;\n</code></pre> <p>For example:</p> <pre><code>1000:1000:65536\n</code></pre> <p>In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs.</p> <p>Tip</p> <p>Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. </p> <p>Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping.</p> <p>To configure the Docker daemon you can pass the following parameter to <code>dockerd</code>:</p> <pre><code>dockerd --userns-remap=\"&lt;UID&gt;:&lt;GID&gt;\"\n</code></pre> <p>Alternatively, you can configure this per-container in ContainerSSH:</p> <pre><code>docker:\n  execution:\n    host:\n      usernsmode: &lt;UID&gt;:&lt;GID&gt;\n</code></pre> <p>In both cases the passed UID and GID must map to the entries in <code>/etc/subuid</code> and <code>/etc/subgid</code>.</p> <p>Warning</p> <p>Using a large number of mappings (10k or more) will cause a performance penalty.</p>"},{"location":"reference/docker/#preventing-storage-exhaustion","title":"Preventing storage exhaustion","text":"<p>A malicious user could cause the Docker host to run out of disk space with a simple attack:</p> <pre><code>cat /dev/zero &gt; ~/zerofill\n</code></pre> <p>There are two cases here:</p> <p>If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver.</p> <p>If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this:</p> <pre><code>docker:\n  execution:\n    host:\n      storageopt:\n        size: 524288000\n</code></pre> <p>However, make sure to test if this works on your Docker configuration. When using <code>overlayfs2</code> on an <code>ext4</code> filesystem this does not work, you may have to switch to <code>xfs</code> or move to <code>devicemapper</code> to make use of this option.</p> <p>Some directories, such as <code>/tmp</code> or <code>/run</code> can also be put on tmpfs to store in memory. This can be configured as follows:</p> <pre><code>docker:\n  execution:\n    host:\n      tmpfs:\n        /tmp: rw,noexec,nosuid,size=65536k\n        /run: rw,noexec,nosuid,size=65536k\n</code></pre> <p>To prevent storage exhaustion it is recommended to set the root FS to be read only:</p> <pre><code>docker:\n  execution:\n    host:\n      readonlyrootfs: true\n</code></pre> <p>Danger</p> <p>If you are using the auditlog make sure you put the local directory on a separate disk than the <code>/var/lib/docker</code> directory even if you don't use storage limits to prevent the audit log from getting corrupted.</p>"},{"location":"reference/docker/#preventing-memory-exhaustion","title":"Preventing memory exhaustion","text":"<p>Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration:</p> <pre><code>docker:\n  execution:\n    host:\n      resources:\n        memory: 26214400\n        # Soft limit\n        memoryreservation: 20000000\n</code></pre> <p>The memory is specified in bytes.</p> <p>In addition it is recommended to enable cgroup swap limit support. This allows for limiting the totoal memory + swap usage:</p> <pre><code>docker:\n  execution:\n    host:\n      resources:\n        memoryswap: 26214400\n        # Tune how likely the container is to be swapped out\n        memoryswappiness: 90\n</code></pre> <p>If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options:</p> <pre><code>docker:\n  execution:\n    host:\n      # Tune how likely the OOM killer is to kill this container\n      oomscoreadj: 500\n      resources:\n          # Disable OOM killer for this container\n          oomkilldisable: true\n</code></pre>"},{"location":"reference/docker/#preventing-cpu-exhaustion","title":"Preventing CPU exhaustion","text":"<p>A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options:</p> <pre><code>docker:\n  execution:\n    host:\n      resources:\n        # Limit which cores the container should run on\n        cpusetcpus: 1-3,5\n        # Limit which Memory nodes the container should run on (For NUMA systems)\n        cpusetmems: 1-3,5\n        # CPU scheduling period in microseconds\n        cpuperiod: 10000\n        # CPU quota to allocate tho the container\n        cpuquota: 1000\n        # As above, but for realtime tasks (guaranteed CPU time)\n        cpurealtimeperiod: 10000\n        cpurealtimequota: 1000\n</code></pre>"},{"location":"reference/docker/#preventing-process-exhaustion","title":"Preventing process exhaustion","text":"<p>You can also limit the number of processes that can be launched inside the container:</p> <pre><code>docker:\n  execution:\n    host:\n      resources:\n        pidslimit: 1000\n</code></pre>"},{"location":"reference/docker/#limiting-network-access","title":"Limiting network access","text":"<p>In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container:</p> <pre><code>docker:\n  execution:\n    container:\n      networkdisabled: true\n</code></pre>"},{"location":"reference/docker/#limiting-disk-io","title":"Limiting disk I/O","text":"<p>Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure:</p> <pre><code>docker:\n  execution:\n    host:\n      resources:\n        # Set relative weight against other containers\n        blkioweight: &lt;weight number&gt; \n        # Set relative weight against other containers\n        blkioweightdevice:\n          - path: &lt;device path&gt;\n            weight: &lt;weight number&gt;\n        # Read BPS\n        blkiodevicereadbps:\n          - path: &lt;device path&gt;\n            rate: &lt;bytes per second&gt;\n        # Write BPS\n        blkiodevicewritebps:\n          - path: &lt;device path&gt;\n            rate: &lt;bytes per second&gt;\n        # Read IOps\n        blkiodevicereadiops:\n          - path: &lt;device path&gt;\n            rate: &lt;IO per second&gt;\n        # Write IOps\n        blkiodevicewriteiops:\n          - path: &lt;device path&gt;\n            rate: &lt;IO per second&gt;\n</code></pre> <p>The device path has to be a path to a block device, e.g. <code>/dev/vda</code>. It does not work on filesystems or character devices.</p>"},{"location":"reference/features/","title":"Supported SSH features","text":"<p>This table contains the list of currently supported SSH features in ContainerSSH.</p> Feature Support RFC Shell execution RFC 4254 section 6.5 Command execution RFC 4254 section 6.5 Subsystem execution RFC 4254 section 6.5 Requesting a Pseudo-Terminal RFC 4254 section 6.2 Setting environment variables RFC 4254 section 6.4 Forwarding signals RFC 4254 section 6.9 Window dimension change RFC 4254 section 6.7 Return exit statuses RFC 4254 section 6.10 Return exit signals RFC 4254 section 6.10 TCP/IP port forwarding RFC 4254 section 7 X11 forwarding RFC 4254 section 6.2 SSH agent forwarding (OpenSSH extension: <code>auth-agent-req@openssh.com</code>) draft-ietf-secsh-agent-02 Keepalive (OpenSSH extension: <code>keepalive@openssh.com</code>) No more sessions (OpenSSH extension: <code>no-more-sessions@openssh.com</code>)"},{"location":"reference/forwarding/","title":"Connection Forwarding","text":"Connection Forwarding <p>This page details setting up connection forwarding for ContainerSSH. Connection forwarding works by having the ContainerSSH agent act as the proxy when a connection forwarding is placed. Connection forwarding comes in multiple flavours: You can ask for the listening end to be either on the client or inside the container, and respectively you can choose listen from a ip:port combo or a named/unix socket and send to either an ip:port combo or a named socket. Additionally, the direct forward option is also supported which enables the usage of the SOCKS proxy support in OpenSSH. Finally, X11 forwarding is also supported.</p>"},{"location":"reference/forwarding/#supported-clients","title":"Supported clients","text":"<p>We have tested the following clients and know them to work:</p> <ul> <li>OpenSSH</li> </ul>"},{"location":"reference/forwarding/#configuration","title":"Configuration","text":"<p>Forwarding is disabled by default, you can enable it via the security settings. Currently only enable/disable directives are supported, no filtering rules. In order for specific ports to be forwarded, or a specific forwarding function please consult the documentation of your SSH Client. </p> <p>In order to enable all forwarding functionality the following configuration can be used:</p> <pre><code>security:\n    forwarding:\n        reverseForwardingMode: enable\n        forwardingMode: enable\n        socketForwardingMode: enable\n        socketListenMode: enable\n        x11ForwardingMode: enable\n</code></pre> <ul> <li>The <code>reverseForwardingMode</code> setting how to treat reverse port forwarding requests, connections from the container to the client.</li> <li>The <code>forwardingMode</code> setting configures how to treat port forwarding requests from the client to the container. Enabling this setting also allows using ContainerSSH as a SOCKs proxy.</li> <li>The <code>socketForwardingMode</code> setting configures how to treat connection requests from the client to a unix socket in the container.</li> <li>The <code>socketListenMode</code> setting configures how to treat requests to listen for connections to a unix socket in the container.</li> <li>The <code>x11ForwardingMode</code> setting configures how to treat X11 forwarding requests from the container to the client</li> </ul>"},{"location":"reference/hardening/","title":"Hardening Guide","text":"Hardening ContainerSSH <p>ContainerSSH is built to secure its inner workings as much as possible. You can take several steps to secure it further.</p>"},{"location":"reference/hardening/#running-containerssh","title":"Running ContainerSSH","text":"<p>ContainerSSH should be run as an unprivileged user (e.g. root) as it does not need access to any privileged resources. When running it from the default container image <code>containerssh/containerssh</code> this is the default.</p> <p>When running it outside a container you should keep the default bind port of 2222. On Linux you can then use iptables to redirect port 22 to the unprivileged port:</p> <pre><code>iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222\n</code></pre> <p>When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman.</p> <p>Tip</p> <p>Don't forget to add this rule to your persistent firewall configuration.</p>"},{"location":"reference/hardening/#securing-authentication","title":"Securing authentication","text":""},{"location":"reference/hardening/#authentication-server-connection","title":"Authentication server connection","text":"<p>ContainerSSH talks to an authentication server over HTTP. There are two potential attacks here:</p> <ol> <li>If an attacker can intercept the connection between ContainerSSH and the authentication server the attacker can read the passwords for password authentication.</li> <li>If an attacker can send requests to the authentication server they can brute force SSH passwords.</li> </ol> <p>Therefore, the connection between ContainerSSH and the authentication server should be secured in the following 3 ways:</p> <ol> <li>Implement firewalls such that only ContainerSSH can access the authentication server.</li> <li>Only use HTTPS with certificate verification to access the authentication server and disable the HTTP port.</li> <li>Deploy client certificates to prevent unauthorized access to the authentication server.</li> </ol> <p>To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the authentication chapter.</p>"},{"location":"reference/hardening/#rate-limiting","title":"Rate limiting","text":"<p>ContainerSSH contains no rate limiting for authentication across connections, this is the job of the authentication server. The number of authentication attempts within a connection is limited to 6.</p> <p>The authentication server must take care to do rate limiting right: within a single connection multiple authentication attempts may be made and if the authentication server returns a non-200 response code ContainerSSH will retry connections.</p> <p>It is recommended that the authentication server use the <code>connectionId</code> field to distinguish between SSH connections as this field is guaranteed to be unique for a connection.</p>"},{"location":"reference/hardening/#client-credential-security","title":"Client credential security","text":"<p>Passwords are vulnerable to being stolen and cannot be transferred to hardware keys. For the most security it is recommended to disable password authentication and only use SSH keys.</p> <p>When storing SSH keys on the client computer they should be protected by a passphrase and limited permissions on the key file. </p> <p>If possible, however, SSH keys should be transferred to a hardware token like the Yubikey. The Yubikey should be configured to require a physical touch on every authentication and should be unlocked with a passcode to prevent unauthorized applications on the client accessing the key for connections.</p>"},{"location":"reference/hardening/#securing-the-configuration-server","title":"Securing the configuration server","text":"<p>ContainerSSH can optionally reach out to a configuration server to fetch dynamic backend configuration based on the username. The backend configuration may contain secrets, such as certificates for accessing Docker and Kubernetes, or application-specific secrets. Therefore, if an attacker can access the configuration server they can extract secrets from the returned configuration.</p> <p>This can be mitigated similar to the authentication server:</p> <ol> <li>Implement firewalls such that only ContainerSSH can access the configuration server.</li> <li>Only use HTTPS with certificate verification to access the configuration server and disable the HTTP port.</li> <li>Deploy client certificates to prevent unauthorized access to the configuration server.</li> </ol> <p>To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the configuration server chapter.</p>"},{"location":"reference/hardening/#limiting-ssh-requests","title":"Limiting SSH requests","text":"<p>The security module provides the ability to limit which requests are allowed from a client. As ContainerSSH is upgraded the default is to allow new features that will come in with future releases (e.g. TCP port forwarding).</p> <p>In order to secure ContainerSSH for future releases it is recommended to set the <code>defaultMode</code> to disable and enable the modes that you need. For subsystems specifically we recommend filtering and allowing only the <code>sftp</code> subsystem as future ContainerSSH versions may support more subsystems.</p> <pre><code>security:\n  defaultMode: disable\n  env:\n    mode: enable\n  command:\n    mode: enable\n  shell:\n    mode: enable\n  subsystem:\n    mode: filter\n    allow:\n     - sftp\n  tty:\n    mode: enable\n  signal:\n    mode: enable\n</code></pre>"},{"location":"reference/hardening/#securing-docker","title":"Securing Docker","text":"<p>Docker-specific settings for security are described in the Docker documentation. </p>"},{"location":"reference/hardening/#securing-kubernetes","title":"Securing Kubernetes","text":"<p>Kubernetes-specific settings for security are described in the Kubernetes documentation.</p>"},{"location":"reference/health/","title":"Health Check","text":"Health check endpoint <p>The health check endpoint is an HTTP server that returns <code>ok</code> and a 200 status code only if all ContainerSSH services are running. This can be used to integrate ContainerSSH with a load balancer. In any other case it'll return \"not ok\" and a 503 status code.</p> <p>The health check endpoint has the following options:</p> <pre><code>health:\n  enable: true\n  listen: 0.0.0.0:7000\n</code></pre> <p>By default, the health check endpoint is disabled. The default listen port is 7000. Further configuration options can be found on the HTTP and TLS page.</p>"},{"location":"reference/http/","title":"HTTP and TLS configuration","text":"<p>ContainerSSH can act as a HTTP server in the following roles:</p> <ul> <li>Metrics server</li> <li>OAuth2 redirect page</li> </ul> <p>It can also act as a HTTP client in the following scenarios:</p> <ul> <li>Authentication webhook</li> <li>Configuration webhook</li> </ul> <p>This page describes how to configure ContainerSSH for secure HTTP communication in these roles.</p>"},{"location":"reference/http/#http-server-configuration","title":"HTTP server configuration","text":"<p>All HTTP servers in ContainerSSH have the following options. They may have additional options depending on their context, see the individual module documentation for details.</p> Option Type Description <code>listen</code> <code>string</code> IP and port to listen on. <code>clientcacert</code> <code>string</code> CA certificate in PEM format or filename that contains the CA certificate used for authenticating connecting clients.  See the Mutual TLS authentication section below. <code>cert</code> <code>string</code> Client certificate in PEM format or filename that contains the server certificate. <code>key</code> <code>string</code> Private key in PEM format or filename that contains the server certificate. <code>tlsVersion</code> <code>[]string</code> Minimum TLS version to support. See the TLS version section below. <code>curve</code> <code>[]string</code> Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. <code>cipher</code> <code>[]string,string</code> Which cipher suites to support. See the Cipher suites section below."},{"location":"reference/http/#http-client-configuration","title":"HTTP client configuration","text":"<p>All HTTP clients have the following options. They may have additional options depending on their context, see the individual module documentation for details.</p> Name Type Description <code>url</code> <code>string</code> HTTP URL of the server to call. <code>timeout</code> <code>string</code> Timeout for the call. Can be provided with time units (e.g. <code>6s</code>), defaults to nanoseconds if provided without a time unit. <code>cacert</code> <code>string</code> CA certificate in PEM format or filename that contains the CA certificate. This is field is required for <code>https://</code> URL's on Windows because of Golang issue #16736 <code>cert</code> <code>string</code> Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. See the Mutual TLS authentication section below. <code>key</code> <code>string</code> Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server.  See the Mutual TLS authentication section below. <code>tlsVersion</code> <code>[]string</code> Minimum TLS version to support. See the TLS version section below. <code>curve</code> <code>[]string</code> Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. <code>cipher</code> <code>[]string,string</code> Which cipher suites to support. See the Cipher suites section below. <code>allowRedirects</code> <code>bool</code> Allow following HTTP redirects. Defaults to false."},{"location":"reference/http/#tls-version","title":"TLS version","text":"<p>The minimum supported TLS version can be configured using the <code>tlsVersion</code> option. It defaults to <code>1.3</code> and also supports <code>1.2</code>. Versions lower than <code>1.2</code> are not supported. Server certificates must use Subject Alternative Names (SAN's) for proper server verification.</p>"},{"location":"reference/http/#elliptic-curve-algorithms","title":"Elliptic curve algorithms","text":"<p>The elliptic curve algorithms can be specified in the <code>curve</code> option. We support and default to the following options:</p> <ul> <li><code>x25519</code></li> <li><code>secp256r1</code></li> <li><code>secp384r1</code></li> <li><code>secp521r1</code></li> </ul>"},{"location":"reference/http/#cipher-suites","title":"Cipher suites","text":"<p>The following cipher suites are supported in ContainerSSH:</p> Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 <p>Tip</p> <p>Cipher suites can be provided as a list or as a colon (<code>:</code>) separated string.</p>"},{"location":"reference/http/#mutual-tls-authentication","title":"Mutual TLS authentication","text":"<p>If ContainerSSH is acting as a HTTP client it can authenticate itself with the HTTPS server using mutual TLS authentication. Conversely, when ContainerSSH is acting as a HTTPS server it can authenticate clients using mutual TLS authentication.</p> <p>To create a CA infrasturcture for this authentication we recommend using cfssl. First we need to create the CA certificates:</p> <pre><code>cat &gt; ca-config.json &lt;&lt;EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"8760h\"\n    },\n    \"profiles\": {\n      \"containerssh\": {\n        \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],\n        \"expiry\": \"8760h\"\n      }\n    }\n  }\n}\nEOF\n\ncat &gt; ca-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"ContainerSSH CA\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 4096\n  },\n  \"names\": [\n    {\n      \"C\": \"Your Country Code\",\n      \"L\": \"Your Locality\",\n      \"O\": \"Your Company\",\n      \"OU\": \"\",\n      \"ST\": \"Your State\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca\n</code></pre> <p>The resulting ca.pem file can be used on the server side as a CA certificate for clients. If ContainerSSH is the server the certificate can be added in the <code>clientcacert</code> field.</p> <p>Then we can create the client certificate:</p> <pre><code>cat &gt; containerssh-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"ContainerSSH\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"Your Country Code\",\n      \"L\": \"Your Locality\",\n      \"O\": \"Your Company\",\n      \"OU\": \"\",\n      \"ST\": \"Your State\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=containerssh \\\n  containerssh-csr.json | cfssljson -bare containerssh\n</code></pre> <p>The resulting <code>containerssh.pem</code> and <code>containerssh-key.pem</code> can be used in the connecting client. If ContainerSSH is the client these files can be added to the <code>cert</code> and <code>key</code> fields, respectively.</p>"},{"location":"reference/image/","title":"Creating Guest Images","text":"Building a container image for ContainerSSH <p>ContainerSSH can run any Linux container image. However, it is strongly recommended that you install the ContainerSSH guest agent into the image to make all features available.</p> <p>If you wish to use SFTP you have to add an SFTP server (<code>apt install openssh-sftp-server</code> on Ubuntu) to the container image and configure the path of the SFTP server correctly in your config.yaml. The sample image <code>containerssh/containerssh-guest-image</code> contains an SFTP server.</p>"},{"location":"reference/image/#integrating-the-guest-agent","title":"Integrating the guest agent","text":"Using the base image (recommended)Installing on Debian/UbuntuInstalling the binaries <p>This method uses the <code>containerssh/agent</code> container image as part of a multistage build:</p> <pre><code>FROM containerssh/agent AS agent\n\nFROM your-base-image\nCOPY --from=agent /usr/bin/containerssh-agent /usr/bin/containerssh-agent\n# Your other build commands here\n</code></pre> <p>We have an experimental Debian repository containing the agent package. Once you have set up the repository you can install the agent like this:</p> <pre><code>apt-get install containerssh-agent\n</code></pre> <p>To use this method go to the latest release from the releases section and verify it against our https://containerssh.io/gpg.txt key (<code>3EE5B012FA7B400CD952601E4689F1F0F358FABA</code>).</p> <p>On an Ubuntu image build this would involve the following steps:</p> <pre><code>ARG AGENT_GPG_FINGERPRINT=3EE5B012FA7B400CD952601E4689F1F0F358FABA\nARG AGENT_GPG_SOURCE=https://containerssh.io/gpg.txt\n\nRUN echo \"\\e[1;32mInstalling ContainerSSH guest agent...\\e[0m\" &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::='--force-confold' update &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::='--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages install gpg &amp;&amp; \\\n    wget -q -O - https://api.github.com/repos/containerssh/agent/releases/latest | grep browser_download_url | grep -e \"agent_.*_linux_amd64.deb\" | awk ' { print $2 } ' | sed -e 's/\"//g' &gt; /tmp/assets.txt &amp;&amp; \\\n    wget -q -O /tmp/agent.deb $(cat /tmp/assets.txt |grep -v .sig) &amp;&amp; \\\n    wget -q -O /tmp/agent.deb.sig $(cat /tmp/assets.txt |grep .sig) &amp;&amp; \\\n    wget -q -O - $AGENT_GPG_SOURCE | gpg --import &amp;&amp; \\\n    echo -e \"5\\ny\\n\" | gpg --command-fd 0 --batch --expert --edit-key $AGENT_GPG_FINGERPRINT trust &amp;&amp; \\\n    test $(gpg --status-fd=1 --verify /tmp/agent.deb.sig /tmp/agent.deb | grep VALIDSIG | grep $AGENT_GPG_FINGERPRINT | wc -l) -eq 1 &amp;&amp; \\\n    dpkg -i /tmp/agent.deb &amp;&amp; \\\n    rm -rf /tmp/* &amp;&amp; \\\n    rm -rf ~/.gnupg &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::='--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages remove gpg &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::='--force-confold' -y clean &amp;&amp; \\\n    /usr/bin/containerssh-agent -h\n</code></pre> <p>Warning</p> <p>The release signing process is experimental and is likely to change in the future.</p> <p>Guest image support is enabled by default in the Docker and Kubernetes backends, but can be disabled as shown below. </p> DockerKubernetes <pre><code>docker:\n  execution:\n    disableAgent: true\n</code></pre> <pre><code>kubernetes:\n  pod:\n    disableAgent: true\n</code></pre>"},{"location":"reference/installation/","title":"Installation","text":"Installation StandaloneDockerKubernetes <p>ContainerSSH can be deployed outside of a container. On our downloads page we provide binaries for Linux, Windows, and MacOS. We also provide DEB, RPM and APK (Alpine Linux) packages.</p> <p>Before running ContainerSSH you will need to create a <code>config.yaml</code> file that tells ContainerSSH where to find the SSH host key and the authentication server. The minimum configuration file looks like this:</p> <pre><code>ssh:\n  hostkeys:\n    - /path/to/your/host.key\nauth:\n  password:\n    method: webhook\n    webhook:\n      url: http://your-auth-server/\n</code></pre> <p>Tip</p> <p>You can generate a new host key using <code>openssl genrsa</code>. Please don't use <code>ssh-keygen</code> as it regenerates OpenSSH-specific keys.</p> <p>Tip</p> <p>Details about the authentication server are described in the Authentication section.</p> <p>ContainerSSH can then be started by running <code>./containerssh --config /path/to/your/config.yaml</code></p> <p>When deploying in Docker you must first prepare a configuration file that tells ContainerSSH where to find the SSH host key and the authentication server. The minimum configuration file looks like this:</p> <pre><code>ssh:\n  hostkeys:\n    - /var/run/secrets/host.key\nauth:\n  password:\n    method: webhook\n    webhook:\n      url: http://your-auth-server/\n</code></pre> <p>Tip</p> <p>You can generate a new host key using <code>openssl genrsa</code></p> <p>Tip</p> <p>Details about the authentication server are described in the Authentication section.</p> <p>You can then run ContainerSSH with the following command line:</p> <pre><code>docker run -d \\\n  -v /srv/containerssh/config.yaml:/etc/containerssh/config.yaml \\\n  -v /srv/containerssh/host.key:/var/run/secrets/host.key \\\n  -p 2222:2222 \\\n  containerssh/containerssh:0.4\n</code></pre> <p>When running ContainerSSH inside a Kubernetes cluster you must first create a <code>Secret</code> that contains the host key.</p> <pre><code>openssl genrsa | kubectl create secret generic containerssh-hostkey --from-file=host.key=/dev/stdin\n</code></pre> <p>Next, you can create a ConfigMap to hold the ContainerSSH configuration:</p> <pre><code>( cat &lt;&lt; EOF \nssh:\n  hostkeys:\n    - /etc/containerssh/host.key\nauth:\n  password:\n    method: webhook\n    webhook:\n      url: http://your-auth-server/\nEOF\n) | kubectl create configmap containerssh-config --from-file=config.yaml=/dev/stdin\n</code></pre> <p>Tip</p> <p>Details about the authentication server are described in the Authentication section.</p> <p>Then you can create a deployment to run ContainerSSH:</p> <pre><code>( cat &lt;&lt; EOF \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: containerssh\n  labels:\n    app: containerssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: containerssh\n  template:\n    metadata:\n      labels:\n        app: containerssh\n    spec:\n      containers:\n      - name: containerssh\n        image: containerssh/containerssh:0.4\n        ports:\n        - containerPort: 2222\n        volumeMounts:\n        - name: hostkey\n          mountPath: /etc/containerssh/host.key\n          subPath: host.key\n          readOnly: true\n        - name: config\n          mountPath: /etc/containerssh/config.yaml\n          subPath: config.yaml\n          readOnly: true\n      volumes:\n      - name: hostkey\n        secret:\n          secretName: containerssh-hostkey\n      - name: config\n        configMap:\n          name: containerssh-config\nEOF\n) | kubectl apply -f -\n</code></pre> <p>Finally, you can create a service to expose the SSH port. You can customize this to create a loadbalancer or nodeport to make SSH publicly available. See <code>kubectl expose --help</code> for details.  </p> <pre><code>kubectl expose deployment containerssh \\\n    --port=2222 --target-port=2222 \\\n    --name=containerssh\n</code></pre> <p>Note</p> <p>This still does not configure ContainerSSH to use Kubernetes as a container backend. This is described in detail in the Kubernetes backend section.</p>"},{"location":"reference/kubernetes/","title":"Kubernetes","text":"The Kubernetes backend <p>The Kubernetes backend runs and is tested against all currently actively maintained Kubernetes versions. For ContainerSSH version 0.4.1 these are: 1.21, 1.20, and 1.19.</p> <p>Tip</p> <p>This is the documentation for the Kubernetes backend. For deploying ContainerSSH inside Kubernetes please see the installation guide.</p>"},{"location":"reference/kubernetes/#the-base-configuration-structure","title":"The base configuration structure","text":"<p>In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server:</p> <pre><code>backend: kubernetes\nkubernetes:\n  connection:\n    &lt;connection configuration here&gt;\n  pod:\n    &lt;pod configuration here&gt;\n  timeouts:\n    &lt;timeouts configuration here&gt;\n</code></pre>"},{"location":"reference/kubernetes/#configuring-connection-parameters","title":"Configuring connection parameters","text":"<p>In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods:</p> <ul> <li>Username and password (HTTP basic auth).</li> <li>x509 client certificates.</li> <li>Bearer token.</li> </ul> <p>These options should be specified like this:</p> <pre><code>kubernetes:\n  connection:\n    host: &lt;...&gt;\n    &lt;...&gt;\n</code></pre> <p>Tip</p> <p>See the Securing Kubernetes section below for a detailed walkthrough for provisioning limited service accounts for ContainerSSH.</p>"},{"location":"reference/kubernetes/#base-configuration","title":"Base configuration","text":"Name Type Description <code>host</code> <code>string</code> The hostname or ip + the port of the Kubernetes API server. Set this to <code>kubernetes.default.svc</code> to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. <code>path</code> <code>string</code> This is the API path of the Kubernetes API. Defaults to <code>/api</code> and you will typically not need to change this. <code>cacertFile</code> <code>string</code> Points to the file that contains the CA certificate in PEM format that signed the server certificate. <code>cacert</code> <code>string</code> Directly contains the CA certificate in PEM format that signed the server certificate. Set to <code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code> when running with a service account. <code>serverName</code> <code>string</code> Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. <code>qps</code> <code>float32</code> Indicates a maximum queries per second from this client. <code>burst</code> <code>int</code> Indicates the maximum burst for query throttling."},{"location":"reference/kubernetes/#http-basic-authentication-username-and-password","title":"HTTP basic authentication (username and password)","text":"Name Type Description <code>username</code> <code>string</code> Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) <code>password</code> <code>string</code> Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2)"},{"location":"reference/kubernetes/#x509-certificate-authentication","title":"x509 certificate authentication","text":"Name Type Description <code>certFile</code> <code>string</code> Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. <code>cert</code> <code>string</code> Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. <code>keyFile</code> <code>string</code> Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. <code>key</code> <code>string</code> Directly contains the client key for x509 authentication against the Kubernetes API in PEM format."},{"location":"reference/kubernetes/#bearer-token-authentication","title":"Bearer token authentication","text":"<p>This authentication method is primarily used with service accounts.</p> Name Type Description <code>bearerTokenFile</code> <code>string</code> Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> to use the service account when running ContainerSSH inside a Kubernetes cluster. <code>bearerToken</code> <code>string</code> Directly contains the bearer token for authenticating against the Kubernetes API."},{"location":"reference/kubernetes/#pod-configuration","title":"Pod configuration","text":"<p>The pod configuration contains the information which pod to run. The structure is very similar to the <code>Pod</code> object in Kubernetes, and we add a few extra options:</p> <pre><code>kubernetes:\n  pod:\n    metadata:\n      &lt;metadata configuration here&gt;\n    spec:\n      &lt;pod spec here&gt;\n    &lt;ContainerSSH-specific options here&gt;\n</code></pre> <p>Note</p> <p>Do not include the <code>apiVersion</code>, <code>kind</code>, or <code>status</code> types from the Kubernetes structure.</p> <p>Tip</p> <p>Did you know? You can get a full description of the Pod type by running <code>kubectl explain pod</code>, <code>kubectl explain pod.spec</code>, and <code>kubectl explain pod.metadata</code>.</p>"},{"location":"reference/kubernetes/#basic-pod-configuration","title":"Basic pod configuration","text":"<p>ContainerSSH defaults to running pods in the <code>default</code> namespace with the <code>containerssh/containerssh-guest-image</code> container image. You can change these settings with the following options:</p> <pre><code>kubernetes:\n  pod:\n    metadata:\n      namespace: default\n    spec:\n      containers:\n        - name: shell\n          image: containerssh/containerssh-guest-image\n          env:\n           - name: VAR\n             value: Hello world!\n</code></pre>"},{"location":"reference/kubernetes/#running-multiple-containers","title":"Running multiple containers","text":"<p>When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the <code>consoleContainerNumber</code> option. This number is 0-indexed.</p> <pre><code>kubernetes:\n  pod:\n    consoleContainerNumber: 1\n    spec:\n      containers:\n        - name: container1\n          image: ...\n        - name: container2\n          image: ...\n</code></pre>"},{"location":"reference/kubernetes/#mounting-volumes","title":"Mounting volumes","text":"<p>In Kubernetes volumes of various types can be mounted into pods. This is done as follows:</p> <pre><code>kubernetes:\n  pod:\n    consoleContainerNumber: 1\n    spec:\n      volumes:\n        - name: &lt;volume name here&gt;\n          &lt;mount type here&gt;:\n            &lt;mount options here&gt;\n      containers:\n        - name: shell\n          image: &lt;image name here&gt;\n          volumeMounts:\n            - name: &lt;volume name here&gt;\n              mountPath: &lt;where to mount&gt;\n</code></pre> <p>For example, mounting a path from the host machine can be done as follows:</p> <pre><code>kubernetes:\n  pod:\n    consoleContainerNumber: 1\n    spec:\n      volumes:\n        - name: home\n          hostPath:\n            path: /home/ubuntu\n            type: Directory\n      containers:\n        - name: shell\n          image: containerssh/containerssh-guest-image\n          volumeMounts:\n            - name: home\n              mountPath: /home/ubuntu\n</code></pre> <p>Tip</p> <p>Use <code>kubectl explain pod.spec.volumes</code> for details on how to configure the volume driver for your storage.</p>"},{"location":"reference/kubernetes/#forcing-the-pod-to-run-on-a-specific-node","title":"Forcing the pod to run on a specific node","text":"<p>In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node.</p> <p>Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual.</p> <p>Binding a pod to a specific node on the other hand is rather simple:</p> <pre><code>kubernetes:\n  pod:\n    spec:\n      nodeName: &lt;insert node name here&gt;\n</code></pre>"},{"location":"reference/kubernetes/#other-options","title":"Other options","text":"<p>Apart from the <code>metadata</code> and <code>spec</code> options ContainerSSH has the following options on a Pod-level. These should not be changed unless required.</p> Name Type Description <code>consoleContainerNumber</code> <code>uint</code> Specifies the number of the container to attach to. Defaults to the first container. <code>mode</code> <code>string</code> Specify <code>connection</code> to launch one pod per SSH connection or <code>session</code> to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the <code>idleCommand</code> as the first program and every session is launched similar to how <code>kubectl exec</code> runs programs. In session mode the command is launched directly. <code>idleCommand</code> <code>[]string</code> Specifies the command to run as the first process in the container in <code>connection</code> mode. Parameters must be provided as separate items in the array. Has no effect in <code>session</code> mode. <code>shellCommand</code> <code>[]string</code> Specifies the command to run as a shell in <code>connection</code> mode. Parameters must be provided as separate items in the array. Has no effect in <code>session</code> mode. <code>agentPath</code> <code>string</code> Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. <code>disableAgent</code> <code>bool</code> Disable the ContainerSSH guest agent. This will disable several functions and is not recommended. <code>subsystems</code> <code>map[string]string</code> Specifies a map of subsystem names to executables. It is recommended to set at least the <code>sftp</code> subsystem as many users will want to use it."},{"location":"reference/kubernetes/#configuration-restrictions","title":"Configuration restrictions","text":"<ul> <li>In <code>connection</code> mode the <code>idleCommand</code> and <code>shellCommand</code> options are required.</li> <li>In <code>session</code> mode the restart policy must be empty or <code>Never</code>.</li> </ul>"},{"location":"reference/kubernetes/#configuring-timeouts","title":"Configuring timeouts","text":"<p>The <code>timeouts</code> section has the following options. All options can use time units (e.g. <code>60s</code>) and default to nanoseconds without time units.</p> Name Description <code>podStart</code> The time to wait for the pod to start. <code>podStop</code> The time to wait for the pod to stop. <code>commandStart</code> The time to wait for the command to start in <code>connection</code> mode. <code>signal</code> The time to wait to deliver a signal to a process. <code>window</code> The time to wait to deliver a window size change. <code>http</code> The time to wait for the underlying HTTP calls to complete."},{"location":"reference/kubernetes/#securing-kubernetes","title":"Securing Kubernetes","text":"<p>Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment</p>"},{"location":"reference/kubernetes/#creating-a-service-account","title":"Creating a service account","text":"<p>When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches.</p> <p>The setup below assumes you are creating a service account in the <code>default</code> namespace and the ContainerSSH pods will run in the <code>containerssh-guests</code> namespace</p> <p>First, we need to create the service account. The following fragment can be applied with <code>kubectl apply -f</code>:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: containerssh\nautomountServiceAccountToken: false\n</code></pre> <p>Then we create the <code>role</code> and <code>rolebinding</code> resources in the <code>containerssh-guests</code> namespace to allow the service accounts to create pods:</p> <pre><code>kubectl create role containerssh \\\n  -n containerssh-guests \\\n  --verb=\"*\" \\\n  --resource=pods \\\n  --resource=pods/logs \\\n  --resource=pods/exec\nkubectl create rolebinding containerssh \\\n  -n containerssh-guests \\\n  --serviceaccount=containerssh:containerssh\n</code></pre> <p>Let's test if the permissions are correct:</p> <pre><code>$ kubectl auth can-i create pod --as containerssh\nno\n$ kubectl auth can-i create pod --namespace containerssh-guests --as containerssh\nyes\n</code></pre> <p>Docker Desktop</p> <p>Docker Desktop Kubernetes contains a cluster role binding called <code>docker-for-desktop-binding</code> that allows all service accounts to perform every action. To secure your Docker Desktop installation you will need to delete this CRB. </p>"},{"location":"reference/kubernetes/#deploying-inside-of-kubernetes","title":"Deploying inside of Kubernetes","text":"<p>When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment:</p> <pre><code>kubernetes:\n  connection:\n    host: ...\n    cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre>"},{"location":"reference/kubernetes/#deploying-outside-of-kubernetes","title":"Deploying outside of Kubernetes","text":"<p>Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself:</p> <pre><code>kubectl describe serviceaccount containerssh\n</code></pre> <p>This command will output the name of the secret for this service account, which can then be extracted:</p> <pre><code>kubectl get secret containerssh-token-2jrnc -o yaml\n</code></pre> <p>The output will look as follows:</p> <pre><code>apiVersion: v1\ndata:\n  ca.crt: &lt;base64-encoded CA certificate here&gt;\n  namespace: ZGVmYXVsdA==\n  token: &lt;base64-encoded bearer token here&gt;\nkind: Secret\n</code></pre> <p>Base64-decode both the <code>ca.crt</code> and the <code>token</code> fields and insert them into your ContainerSSH config as follows:</p> <pre><code>kubernetes:\n  connection:\n    bearerToken: &lt;insert token here&gt;\n    cacert: |\n      &lt;insert ca.crt here&gt;\n</code></pre>"},{"location":"reference/kubernetes/#preventing-root-escalation","title":"Preventing root escalation","text":"<p>Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting:</p> <pre><code>kubernetes:\n  pod:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n</code></pre> <p>However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID:</p> <pre><code>kubernetes:\n  pod:\n    spec:\n      securityContext:\n        runAsUser: 1000\n</code></pre>"},{"location":"reference/kubernetes/#preventing-storage-exhaustion","title":"Preventing storage exhaustion","text":"<p>A malicious user could cause the Kubernetes host to run out of disk space with a simple attack:</p> <pre><code>cat /dev/zero &gt; ~/zerofill\n</code></pre> <p>There are two cases here:</p> <p>If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver.</p> <p>If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The  <code>readOnlyRootFilesystem</code> PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk.</p>"},{"location":"reference/kubernetes/#preventing-memory-exhaustion","title":"Preventing memory exhaustion","text":"<p>Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration:</p> <pre><code>kubernetes:\n  pod:\n    spec:\n      resources:\n        limits:\n          memory: \"128Mi\"\n</code></pre> <p>You can read more about memory requests and limits in the Kubernetes documentation.</p>"},{"location":"reference/kubernetes/#preventing-cpu-exhaustion","title":"Preventing CPU exhaustion","text":"<p>A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options:</p> <pre><code>kubernetes:\n  pod:\n    spec:\n      resources:\n        limits:\n          cpu: \"500m\"\n</code></pre> <p>In this setting <code>1000m</code> corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation.</p>"},{"location":"reference/kubernetes/#limiting-network-access","title":"Limiting network access","text":"<p>Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies. These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: containerssh-guest-policy\n  namespace: containerssh-guests\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress: []\n  egress: []\n</code></pre>"},{"location":"reference/logging/","title":"Logging","text":"Logging <p>ContainerSSH comes with configurable logging facilities. You can configure the following options:</p> <ol> <li>The minimum log level to filter unnecessary log messages</li> <li>The log format</li> <li>The log destination</li> </ol>"},{"location":"reference/logging/#configuring-the-minimum-log-level","title":"Configuring the minimum log level","text":"<p>You can configure the minimum log level by adding the following to your configuration:</p> <pre><code>log:\n  level: \"info\"\n</code></pre> <p>Tip</p> <p>You can configure the log level on a per-user basis using the configuration server.</p> <p>The supported levels are in accordance with the Syslog standard:</p> <ul> <li><code>debug</code></li> <li><code>info</code></li> <li><code>notice</code></li> <li><code>warning</code></li> <li><code>error</code></li> <li><code>crit</code></li> <li><code>alert</code></li> <li><code>emerg</code></li> </ul>"},{"location":"reference/logging/#configuring-the-log-format","title":"Configuring the log format","text":"<p>ContainerSSH can log in both text and newline-delimited JSON (<code>ljson</code>) format. You can change the format with the following setting:</p> <pre><code>log:\n  format: \"ljson\"\n</code></pre>"},{"location":"reference/logging/#the-json-log-format","title":"The JSON log format","text":"<p>The LJSON format outputs a JSON-formatted string per log message. For file and stdout these JSON messages are separated by a newline from each other.</p> <p>When writing to the <code>stdout</code> or <code>file</code> destinations the format is the following:</p> <pre><code>{\n  \"timestamp\":\"Timestamp in RFC3339 format\",\n  \"level\":\"the log level\",\n  \"code\":\"ERROR_CODE_HERE\",\n  \"message\":\"the message (optional)\",\n  \"details\": {\n    \"the detail object if any (optional)\"\n  }\n}\n</code></pre> <p>When writing to syslog the format is the same, but does not contain the <code>timestamp</code> and <code>level</code> fields as they are redundant. The error codes are documented in the troubleshooting section.</p>"},{"location":"reference/logging/#the-text-log-format","title":"The text log format","text":"<p>When writing to the <code>stdout</code> or <code>file</code> destinations the text log format has the following fields delimited by a tab (<code>\\t</code>) character:</p> <pre><code>TIMESTAMP\\tLEVEL\\tMESSAGE\n</code></pre> <p>The <code>TIMESTAMP</code> will be formatted according to RFC3339, while the <code>LEVEL</code> will be the text-representation of the log level. The <code>MESSAGE</code> field will contain the text representation of the message.</p> <p>Warning</p> <p>The text message is not intended for machine processing and may change across versions. If you intend to do machine processing please use the <code>details</code> field from the <code>ljson</code> format.</p>"},{"location":"reference/logging/#setting-the-output","title":"Setting the output","text":"<p>The output configuration of ContainerSSH is the following:</p> <pre><code>log:\n  destination: \"stdout|file|syslog\"\n  &lt;other destination options&gt;\n</code></pre>"},{"location":"reference/logging/#writing-to-the-stdout","title":"Writing to the stdout","text":"<p>The stdout destination is the simplest: it will write error messages to the standard output of ContainerSSH. This is the default logging method.</p> <pre><code>log:\n  destination: \"stdout\"\n</code></pre>"},{"location":"reference/logging/#writing-to-a-file","title":"Writing to a file","text":"<p>You can write to a specific file using the following options:</p> <pre><code>log:\n  destination: \"file\"\n  file: \"/var/log/containerssh/containerssh.log\"\n</code></pre> <p>ContainerSSH will write to the specified file in the specified format.</p> <p>Tip</p> <p>ContainerSSH supports log rotation. You can trigger a log rotation by sending a HUP (hangup) signal to ContainerSSH.</p>"},{"location":"reference/logging/#writing-to-syslog","title":"Writing to syslog","text":"<p>ContainerSSH supports writing to syslog via a Unix socket or UDP. TCP syslog is not supported due to the complexity of maintaining a stable connection and buffering messages between disconnects.</p> <p>Syslog can be configured as follows:</p> <pre><code>log:\n  destination: syslog\n  syslog:\n    # Change to IP and port for UDP\n    destination: /dev/log\n    # See below for supported names\n    facility: auth\n    # Program name to log as\n    tag: ContainerSSH\n    # Log PID with program name\n    pid: false\n</code></pre> <p>The following facilities are supported:</p> <ul> <li><code>kern</code></li> <li><code>user</code></li> <li><code>mail</code></li> <li><code>daemon</code></li> <li><code>auth</code></li> <li><code>syslog</code></li> <li><code>lpr</code></li> <li><code>news</code></li> <li><code>uucp</code></li> <li><code>cron</code></li> <li><code>authpriv</code></li> <li><code>ftp</code></li> <li><code>ntp</code></li> <li><code>logaudit</code></li> <li><code>logalert</code></li> <li><code>clock</code></li> <li><code>local0..7</code></li> </ul> <p>Warning</p> <p>ContainerSSH can log very long messages with lots of details. Please make sure to bump your maximum line length to at least 4096 characters in your Syslog server to avoid message truncation. (rsyslog and syslog-ng have higher default values, so you don't need to change the configuration if you are using these syslog servers.)</p>"},{"location":"reference/metrics/","title":"Metrics","text":"Metrics <p>ContainerSSH contains a Prometheus-compatible metrics server which can be enabled using the following configuration:</p> <pre><code>metrics:\n  &lt;options here&gt;\n</code></pre> <p>The metrics server has the following options:</p> Option Type Description <code>enable</code> <code>bool</code> Enable metrics server. Defaults to false. <code>path</code> <code>string</code> HTTP path to serve metrics on. Defaults to <code>/metrics</code>. <p>Additionally, all options in the HTTP server section on the HTTP and TLS page are available. The metrics server defaults to port 9100.</p> <p>Tip</p> <p>For an example on configuring Prometheus with mutual TLS authentication see the Prometheus documentation.</p>"},{"location":"reference/metrics/#available-metrics","title":"Available metrics","text":"<p>You can configure Prometheus to grab the following metrics:</p> <code>containerssh_config_server_requests_total</code> Number of requests to the configuration server since start. <code>containerssh_config_server_failures_total</code> Number of failed requests to the configuration server since start. <code>containerssh_backend_requests_total</code> Number of requests to the backend (docker, kubernetes etc) since start. <code>containerssh_backend_errors_total</code> Number of failed requests to the backend (docker, kubernetes etc) since start. <code>containerssh_auth_server_requests_total</code> Number of requests to the authentication server since start. <code>containerssh_auth_server_failures_total</code> Number of failed requests to the authentication server since start. <code>containerssh_auth_success_total</code> Number of successful authentications since start. Contains labels for <code>authtype</code> (<code>password</code>, <code>pubkey</code> etc) and <code>country</code> (see below). <code>containerssh_auth_failures_total</code> Number of failed authentications since start. Contains labels for <code>authtype</code> (<code>password</code>, <code>pubkey</code> etc) and <code>country</code> (see below). <code>containerssh_ssh_connections_total</code> Number of SSH connections since start. Contains a label for <code>country</code> (see below). <code>containerssh_ssh_current_connections</code> Number of currently active SSH connections. Contains a label for <code>country</code> (see below). <code>containerssh_ssh_successful_handshakes_total</code> Number of successful SSH handshakes since start. Contains a label for <code>country</code> (see below). <code>containerssh_ssh_failed_handshakes_total</code> Number of failed SSH handshakes since start. Contains a label for <code>country</code> (see below)."},{"location":"reference/metrics/#country-identification","title":"Country identification","text":"<p>Country identification works using GeoIP2 or GeoLite2 from MaxMind. This database needs to be provided to ContainerSSH externally due to licensing concerns.</p> <p>The default path for the GeoIP database is <code>/var/lib/GeoIP/GeoIP2-Country.mmdb</code>, but you can change that using the following configuration snippet:</p> <pre><code>geoip:\n  provider: \"maxmind\"\n  maxmind-geoip2-file: '/var/lib/GeoIP/GeoIP2-Country.mmdb'\n</code></pre>"},{"location":"reference/security/","title":"Restrictions","text":"Security configuration <p>The security module provides the ability to limit or force the behavior of SSH. It can be configured using the following structure:</p> <pre><code>security:\n  maxSessions: 10\n  forceCommand: \"/run/this/command\"\n  defaultMode: enable|filter|disable\n  env:\n    mode: enable|filter|disable\n    allow:\n      - ENV_VARIABLE_NAME\n    deny:\n      - ENV_VARIABLE_NAME\n  command:\n    mode: enable|filter|disable\n    allow:\n      - /allow/this/command\n  shell:\n    mode: enable|disable\n  subsystem:\n    mode: enable|filter|disable\n    allow:\n      - &lt;enable-this-subsystem&gt;\n    deny:\n      - &lt;disable-this-subsystem&gt;\n  forwarding:\n    reverseForwardingMode: enable|disable\n    forwardingMode: enable|disable\n    socketForwardingMode: enable|disable\n    socketListenMode: enable|disable\n    x11ForwardingMode: enable|disable\n  tty:\n    mode: enable|disable\n  signal:\n    mode: enable|filter|disable\n    allow:\n      - TERM\n    deny:\n      - KILL\n</code></pre>"},{"location":"reference/security/#maximum-sessions","title":"Maximum sessions","text":"<p>The <code>maxSessions</code> option lets you limit the number of parallel sessions a client can open. When this number of sessions is reached further session requests are rejected until a session is closed. The recommended value for this option is <code>10</code>. </p>"},{"location":"reference/security/#forcing-commands","title":"Forcing commands","text":"<p>The <code>forceCommand</code> option lets you force the execution of a command even when the client has specified a different command to be run. This turns all shell and subsystem requests into command execution requests to run the specified command. The original command will be available in the <code>SSH_ORIGINAL_COMMAND</code> environment variable.</p>"},{"location":"reference/security/#filtering-requests","title":"Filtering requests","text":"<p>SSH allows a client to request a multitude of things. The security module allows you to either enable, filter, or deny requests.</p> <ul> <li><code>allow</code> will allow all requests except the ones specified in the <code>deny</code> list.</li> <li><code>filter</code> will only allow requests specified in the <code>allow</code> list.</li> <li><code>deny</code> denies all requests.</li> </ul> <p>You can configure the settings either individually, or using the <code>defaultMode</code> setting. It is strongly recommended to set a default mode so future ContainerSSH versions adding new features don't accidentally allow something you don't want to enable.</p>"},{"location":"reference/security/#environment-variable-filtering","title":"Environment variable filtering","text":"<p>Using the <code>env</code> option you can filter which environment variables the client can set. In <code>enable</code> mode you can deny specific environment variables by specifying disallowed variables in the <code>deny</code> list. In <code>filter</code> mode you can specify allowed variables in the <code>allow</code> list. If you want to completely disable setting environment variables you can set the mode to <code>disable</code>.</p>"},{"location":"reference/security/#command-execution","title":"Command execution","text":"<p>A client can explicitly request running a specific command by specifying it in the command line:</p> <pre><code>ssh yourserver.com run-this-program\n</code></pre> <p>In <code>enable</code> mode command execution is enabled with no filtering. There is no <code>deny</code> option as it is trivially easy to work around a simple matching.</p> <p>In <code>filter</code> mode only the commands that are specified in the <code>allow</code> list are executed. The match must be exact.</p> <p>In <code>deny</code> mode all command execution is disabled.</p>"},{"location":"reference/security/#shell-execution","title":"Shell execution","text":"<p>When not specifying a command to the SSH client by default a shell is launched. You can only set the <code>enable</code> and <code>disable</code> modes. The <code>filter</code> mode is valid, but is equal to the <code>disable</code> mode.</p>"},{"location":"reference/security/#subsystem-execution","title":"Subsystem execution","text":"<p>SSH clients can also execute well-known subsystems, such as <code>sftp</code>. The server then decides which binary to execute for the requested subsystem.</p> <p>When set to <code>enable</code> all subsystems except the ones in the <code>deny</code> list. In <code>filter</code> mode only subsystems in the <code>allow</code> list are allowed. In <code>deny</code> mode no subsystem execution is allowed.</p>"},{"location":"reference/security/#ttypty-requests","title":"TTY/PTY requests","text":"<p>When a client wants to use the SSH server interactively they can send a <code>PTY</code> request to the server before executing the program.</p> <p>The only security options for TTY are <code>enable</code> and <code>disable</code>. <code>filter</code> mode is not explicitly invalid, but behaves like <code>deny</code>.</p>"},{"location":"reference/security/#signals","title":"Signals","text":"<p>Although not used very often, SSH clients can request signals to be delivered to the running program. In <code>enable</code> mode all signals except for the ones listed in the <code>deny</code> list are allowed. In <code>filter</code> mode only the signals in the <code>allow</code> list are allowed. In <code>disable</code> mode no signal delivery is allowed.</p> <p>Warning</p> <p>Signal names have to be specified without the <code>SIG</code> prefix.</p> <p>ContainerSSH supports the following signals:</p> <ul> <li><code>ABRT</code></li> <li><code>ALRM</code></li> <li><code>FPE</code></li> <li><code>HUP</code></li> <li><code>ILL</code></li> <li><code>INT</code></li> <li><code>KILL</code></li> <li><code>PIPE</code></li> <li><code>QUIT</code></li> <li><code>SEGV</code></li> <li><code>TERM</code></li> <li><code>USR1</code></li> <li><code>USR2</code></li> </ul>"},{"location":"reference/ssh/","title":"Configuration","text":"SSH configuration <p>SSH is the main service of ContainerSSH. It has the following configuration structure:</p> <pre><code>ssh:\n  &lt;options&gt;\n</code></pre> <p>The options are as follows:</p> Name Type Description <code>listen</code> <code>string</code> IP and port pair to bind the SSH service to. Defaults to <code>0.0.0.0:2222</code> <code>serverVersion</code> <code>string</code> Server version string presented to any connecting client. Must start with <code>SSH-2.0-</code>. Defaults to <code>SSH-2.0-ContainerSSH</code>. <code>ciphers</code> <code>[]string</code> List of ciphers the server should support. See the Ciphers section below. <code>kex</code> <code>[]string</code> List of key exchange algorithms the server should support. See the Key exchange section below. <code>macs</code> <code>[]string</code> List of MAC algorithms the server should support. See the MAC section below. <code>banner</code> <code>string</code> The banner text to presented to any connecting client. <code>hostkeys</code> <code>[]string</code> List of host keys in PEM format, or file names to read the key from. Generate with <code>openssl genrsa</code> <code>clientAliveInterval</code> <code>time.Duration</code> (<code>string</code>) Time interval between keepAlive messages containerssh sends to the client. Defaults to <code>0</code>, disabled. Example value:<code>30s</code> <code>clientAliveCountMax</code> <code>int</code> Number of missed keepAlive messages before a client is considered disconnected and the connection is closed"},{"location":"reference/ssh/#configuring-the-server-version","title":"Configuring the server version","text":"<p>The SSH server version is presented to any connecting client in plain text upon connection. It has the following format:</p> <pre><code>SSH-2.0-softwareversion &lt;SP&gt; comments\n</code></pre> <p>The <code>softwareversion</code> can only contain printable US-ASCII characters without whitespace and minus (<code>-</code>) signs. The <code>comments</code> field is optional and is separated from the <code>softwareversion</code> with a single space. The maximum length of the version string is 255 characters.   </p>"},{"location":"reference/ssh/#configuring-a-banner","title":"Configuring a banner","text":"<p>SSH offers the ability to output a message to the clients before they enter passwords. This can be configured in the <code>banner</code> option. The banner can contain multiple lines. </p>"},{"location":"reference/ssh/#ciphers","title":"Ciphers","text":"<p>ContainerSSH supports the following ciphers. The defaults are configured based on Mozilla Modern suite.</p> Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID"},{"location":"reference/ssh/#key-exchange","title":"Key exchange","text":"<p>ContainerSSH supports the following key exchange algorithms. The defaults are configured based on Mozilla Modern suite.</p> Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1"},{"location":"reference/ssh/#mac","title":"MAC","text":"<p>ContainerSSH supports the following MAC algorithms. The defaults are configured based on Mozilla Modern suite.</p> Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96"},{"location":"reference/sshproxy/","title":"SSH proxy","text":"The SSH proxy backend <p>The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend. This allows for using the audit log to inspect SSH traffic, or to dynamically forwarding connections using the configuration server.</p>"},{"location":"reference/sshproxy/#the-base-configuration-structure","title":"The base configuration structure","text":"<p>The minimum configuration is the following:</p> <pre><code>backend: sshproxy\nsshproxy:\n  # Add the backend server here\n  server: 127.0.0.1\n  # Set the following option to true to reuse the connecting user's username.\n  usernamePassThrough: true\n  # Or specify a username manually\n  username: root\n  # Specify the password\n  password: changeme\n  # Or the private key. This can reference a file or be added directly.\n  privateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n  # Provide all fingerprints of the backing SSH server's host keys:\n  allowedHostKeyFingerprints:\n    - SHA256:...\n</code></pre> <p>Tip</p> <p>You can obtain the fingerprints of OpenSSH host keys by running the following script: <pre><code>for i in /etc/ssh/ssh_host_*.pub; do ssh-keygen -l -f $i; done | cut -d ' ' -f 2\n</code></pre></p> <p>Warning</p> <p>ContainerSSH does not support passing through passwords or public key authentication to the backing server. We recommend setting up private-public key authentication with the backing server.</p>"},{"location":"reference/sshproxy/#configuration-options","title":"Configuration options","text":"Option Type Description <code>server</code> <code>string</code> Host name or IP address of the backing SSH server. Required. <code>port</code> <code>uint16</code> Port number of the backing SSH service. Defaults to 22. <code>usernamePassThrough</code> <code>bool</code> Take username from the connecting client. <code>username</code> <code>string</code> Explicitly set the username to use for the backing connection. Required if <code>usernamePassThrough</code> is <code>false</code>. <code>password</code> <code>string</code> Password to use to authenticate with the backing SSH server. <code>privateKey</code> <code>string</code> Private key to use to authenticate with the backing SSH server. Can be a reference to a file or the private key in PEM or OpenSSH format. <code>allowedHostKeyFingerprints</code> <code>[]string</code> List of SHA256 fingerprints of the backing SSH server. <code>ciphers</code> <code>[]string</code> List of SSH ciphers to use. See Ciphers below. <code>kex</code> <code>[]string</code> List of key exchange algorithms to use. See Key exchange algorithms below. <code>macs</code> <code>[]string</code> List of MAC algorithms to use. See MAC algorithms below. <code>hostKeyAlgorithms</code> <code>[]string</code> List of host key algorithms to request from the backing server. See Host key algorithms below. <code>timeout</code> <code>string</code> Timeout for connecting / retrying the SSH connection. <code>clientVersion</code> <code>string</code> Client version string to send to the backing server. Must be in the format of <code>SSH-protoversion-softwareversion SPACE comments</code>. See RFC 4235 section 4.2. Protocol Version Exchange for details. The trailing CR and LF characters should not be added to this string."},{"location":"reference/sshproxy/#ciphers","title":"Ciphers","text":"<p>ContainerSSH supports the following ciphers for contacting the backing server.  The defaults are configured based on Mozilla Modern suite.</p> Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID"},{"location":"reference/sshproxy/#key-exchange-algorithms","title":"Key exchange algorithms","text":"<p>ContainerSSH supports the following key exchange algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite.</p> Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1"},{"location":"reference/sshproxy/#mac-algorithms","title":"MAC algorithms","text":"<p>ContainerSSH supports the following MAC algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite.</p> Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96"},{"location":"reference/sshproxy/#host-key-algorithms","title":"Host key algorithms","text":"<p>ContainerSSH supports the following host key algorithms for verifying the backing server identity.</p> Algorithm Default ssh-rsa-cert-v01@openssh.com ssh-dss-cert-v01@openssh.com ecdsa-sha2-nistp256-cert-v01@openssh.com ecdsa-sha2-nistp384-cert-v01@openssh.com ecdsa-sha2-nistp521-cert-v01@openssh.com ssh-ed25519-cert-v01@openssh.com ssh-rsa ssh-dss ssh-ed25519"},{"location":"reference/troubleshooting/","title":"Troubleshooting guide","text":"Troubleshooting ContainerSSH <p>When ContainerSSH doesn't work several components can be involved. Is it ContainerSSH? Can it connect to the authentication server? How about the configuration server? Is the backend working?</p> <p>This guide will lead you through the steps of debugging issues with ContainerSSH.</p>"},{"location":"reference/troubleshooting/#turning-on-debug-logging","title":"Turning on debug logging","text":"<p>The first step in determining any potential issues is to enable debug logging. This will output a slew of log messages into the designated log destination that you can use to determine what's going wrong.</p> <p>If you are trying to debug production failures with lots of traffic it can sometimes be hard to read these logs. That's why a useful field in all connection-related messages is the <code>connectionId</code> field. This can be used to tie related log messages together.</p> <p>If your configuration server is flexible enough you can pass the <code>debug</code> log level on a per-user basis to increase the log verbosity for a single user only.</p> <p>Each message has a unique code. The list of codes is documented in the codes section.</p>"},{"location":"reference/troubleshooting/#connecting-in-debug-mode","title":"Connecting in debug mode","text":"<p>Most SSH clients have a debug mode. In the command line SSH you can add the <code>-vvv</code> flag to get a lot more verbose output:</p> <pre><code>ssh youruser@youserver.com -vvv\n</code></pre>"},{"location":"reference/troubleshooting/#turning-on-audit-logging","title":"Turning on audit logging","text":"<p>Another useful tool to turn on is audit logging. Audit logging can be another helpful tool when trying to figure out what a certain user is doing. It can record every single interaction that happens over SSH. More importantly, audit logs are also tied to the <code>connectionId</code> that is also present in the logs above.</p>"},{"location":"reference/troubleshooting/#debugging-webhook-server-failures","title":"Debugging webhook server failures","text":"<p>Apart from turning on logging you can also use network monitoring to debug authentication and configuration webhook failures. Even if the connection itself is encrypted, using a tool like tcpdump or Wireshark can give you useful clues if the connection is even established correctly or if something is failing on a connection level.</p>"},{"location":"reference/troubleshooting/#debugging-container-backend-failures","title":"Debugging container backend failures","text":"<p>When it comes to interacting with container backend you can also use logs as well as network monitoring to determine basic failures. Furthermore, both Docker and Kubernetes provide the ability to monitor events that are happening to get an idea of what's going on.</p> DockerKubernetes <pre><code>docker events\n</code></pre> <pre><code>kubectl get events --watch\n</code></pre>"},{"location":"reference/troubleshooting/#debugging-with-strace","title":"Debugging with strace","text":"<p>If none of the above steps help it is time to unpack the big tools. strace is a Linux utility that lets you list all system calls ContainerSSH is doing. This is not the easiest log to read, but the following snippet should help:</p> <pre><code>strace \\\n  -e trace=open,read,write,readv,writev,recv,recvfrom,send,sendto \\\n  -s 999 \\\n  -p CONTAINERSSH-PID-HERE\n</code></pre>"},{"location":"reference/troubleshooting/#if-all-else-fails-ask-for-help","title":"If all else fails: ask for help","text":"<p>If all else fails we are here for you. Please collect the following items:</p> <ol> <li>The debug logs from above.</li> <li>Your configuration file without any sensitive details (credentials, IPs).</li> </ol> <p>Please also prepare the following items if you can, but don't submit them as they may contain sensitive credentials:</p> <ol> <li>Audit logs (if you have them).</li> <li>Any pcap files from tcpdump or Wireshark you may have.</li> <li>Any strace outputs you may have.</li> <li>Any Docker or Kubernetes events you may have recorded.</li> </ol> <p>You can raise your question in one of the following channels:</p> <ul> <li>As a GitHub issue.</li> <li>As a discussion post.</li> </ul> <p>Please link the debug logs and configuration from a GitHub Gist or a Pastebin.</p> <p>Don't worry about submitting duplicate issues, just make sure to describe your issue in as much detail as possible.</p>"},{"location":"reference/api/","title":"About the API","text":"<p>ContainerSSH itself does not have an API apart from exposing the metrics. However, it requires two external APIs: the authentication server for user authentication and the config server for supplying a user-specific container configuration.</p> <p>We are providing an OpenAPI document for both.</p> <p>Access the OpenAPI docs \u00bb</p>"},{"location":"usecases/debugging/","title":"Debugging a production environment","text":""},{"location":"usecases/debugging/#problem","title":"Problem","text":"<p>Granting developer access to a production system is often problematic or even impossible if accurate record-keeping is required.</p> <ol> <li>Access to production systems should be restricted on an as-needed basis, not all developers should have access to it.</li> <li>Auditing changes to a production system is a must.</li> </ol>"},{"location":"usecases/debugging/#what-a-traditional-setup-looks-like","title":"What a traditional setup looks like","text":"<ol> <li>Only a limited number of system administrators have access.</li> <li>Changes can only be made using a CI system.</li> <li>Debugging involves several people.</li> <li>Audit logging is limited or non-existent.</li> <li>Sometimes, PAM is used for dynamic user databases, which increases complexity.</li> </ol>"},{"location":"usecases/debugging/#how-containerssh-helps","title":"How ContainerSSH helps","text":"<ol> <li>Dynamically allow users access based on automation from your ticketing system or web interface.</li> <li>Audit log every SSH command and file uploads accurately.</li> <li>Create narrowly-scoped containers with read only access.</li> <li>No PAM modifications needed, no access to the host operating system.</li> </ol>"},{"location":"usecases/debugging/#how-to-build-it","title":"How to build it","text":"<p>The goal is that developers get time-limited access to the production environment. In our case, we will configure ContainerSSH to authenticate against OpenPolicyAgent or your custom authentication server. </p> <p>First, install ContainerSSH with a base setup. Configure the backend to Docker or Kubernetes as desired. Please read the \"Securing Docker\" or \"Securing Kubernetes\" sections for your environment.</p> <p>When a developer requests access via a ticket or web interface, use automation to create a user entry in the OPA JSON file, or a database for the current access. (You can use external data with OPA to use an external database.) Now, configure ContainerSSH to send authentication requests to your OPA instance or custom server.</p> <p>Now, make sure that the container image contains all the debugging tools you need for your system, such as a database client. See the image creation reference manual for details.</p> <p>Next, configure audit logging in ContainerSSH. This will record every interaction your developers make.</p> <p>Optionally, you can also add a dynamic configuration server to provision dynamic user credentials for the current access, or create a per-user container configuration, such as mounting their home volume.</p>"},{"location":"usecases/honeypots/","title":"Honeypots","text":"<p>When left undefended, SSH can be a large attack surface towards the internet. If you leave an SSH server open to the Internet, bots will try to brute force their way in within minutes. Why not build a honeypot?</p> <p>Honeypots can provide valuable early warning: log the IP addresses of connection attempts and dynamically firewall them. Collect credentials attackers are trying to use and match them against your user database to root out weak passwords. Collect logs of what attackers are doing in a containerized environment. ContainerSSH version 0.4 and up offer comprehensive audit logging to record everything the attacker is doing.</p> <p>ContainerSSH can do all that. When a user connects, ContainerSSH reaches out to your authentication server where you can log IP addresses and credentials.</p> <p>If you allow attackers to connect, ContainerSSH fetches a dynamic container configuration from your configuration server. You can specify what environment and on which Docker or Kubernetes setup to run your honeypot. Restrict attackers to a set amount of resources or a networkless environment.</p> <p>Get started \u00bb Read the guide \u00bb</p>"},{"location":"usecases/lab/","title":"Dynamic lab environment","text":""},{"location":"usecases/lab/#problem","title":"Problem","text":"<p>In a lab environment you want to give SSH access to several people: students, contractors, etc. There are several requirements:</p> <ol> <li>Authentication: Each user should be able to log in with their own username and password or SSH key, and only get access to their own environment.</li> <li>Resource restriction: The environment should be resource-restricted, one user should not be able to use up all system resources.</li> <li>Cleanup: When a user logs out the environment should be cleaned up.</li> <li>Monitoring: The activities of the user should be monitored.</li> </ol>"},{"location":"usecases/lab/#how-a-traditional-setup-looks-like","title":"How a traditional setup looks like","text":"<ol> <li>Authentication: Authentication is done by creating system users, either directly or via a PAM integration. SSH key-based authentication is difficult or impossible to manage, depending on the requirements.</li> <li>Resource restriction: Resource restrictions are difficult or even impossible to implement.</li> <li>Cleanup: The environment is not cleaned up after a user, the servers need to be reinstalled frequently.</li> <li>Monitoring: Basic login monitoring is provided by the system, more advanced logging is difficult.</li> </ol>"},{"location":"usecases/lab/#how-containerssh-helps","title":"How ContainerSSH helps","text":"<ol> <li>Authentication: ContainerSSH natively talks to an external authentication provider via webhooks.</li> <li>Resource restriction: Each user container can be configured with resource restrictions for CPU, memory, disk IO, and network, depending on the capabilities of the backend (Docker, Kubernetes, etc).</li> <li>Cleanup: ContainerSSH launches ephemeral containers that are removed when the user logs out. Persistent volumes can be mounted for each user dynamically.</li> <li>Monitoring: ContainerSSH has a detailed audit log that is able to record everything that goes on via SSH, including file transfers.</li> </ol>"},{"location":"usecases/lab/#how-to-build-it","title":"How to build it","text":"<p>As a first step, decide on a backend: Kubernetes is more scalable, but requires more work to get going. Docker is less scalable, but provides more capabilities for resource restriction out of the box.</p> <p>Next, you will need to build an authentication server. This server is a simple webhook that authenticates your users. The simplest method is using Open Policy Agent, but  you can roll your own too. Check the OPA example for an example on how to set this up. We have a full reference manual on building and configuring the authentication webhook.</p> <p>Finally, you will have to build a configuration server. This is also a webhook, and lets you configure the container for each user. You can also do this with OPA, or write your own. Most importantly, you will want to mount the home directory of each user for persistent data storage. Optionally, you may also want to create a container image and add your own tools. See the config webook and building your own image reference manuals for details.</p> <p>You may also want to add resource restrictions to the configuration webhook. See the Kubernetes and Docker reference manuals for detailed security guides.</p> <p>Once you have all components, please follow the installation reference manual on deploying ContainerSSH.</p>"},{"location":"usecases/learning/","title":"Learning environments","text":"<p>Providing console access to your students can be a hurdle. No matter if you want to provide access to a Linux environment, databases, or something else, you will have to create users, make sure they have everything they need, and clean up when they are done.</p> <p>Containers can provide an easier solution: launch a specific environment for a student and simply remove the container when they are done. However, manually provisioning containers can still be tedious and continuously running a large number of containers can be resource-intensive.</p> <p>ContainerSSH provides a vital role here: it can dynamically launch containers as needed. When users connect via SSH, ContainerSSH reaches out to your authentication server to verify user credentials and then contacts your configuration server to fetch the customized container configuration for your user. When your user disconnects, ContainerSSH removes their container and leaves no trace behind.</p> <p>Get started \u00bb</p>"},{"location":"usecases/security/","title":"High security environments","text":"<p>Do you need to provide secure access to a console environment and highly sensitive credentials to users? Key management systems like HashiCorp Vault can change credentials frequently to counteract credential leakage or theft by users. However, educating your users to use the key management system can be time-consuming. </p> <p>ContainerSSH provides a user-friendly solution. When your users connect to the SSH server it reaches out to an authentication server provided by you. This lets you authenticate them against your own user database using passwords or SSH keys.</p> <p>When authenticated successfully, ContainerSSH contacts your configuration server to get the configuration for your user. The configuration server can expose short lived credentials from the key management system in the container environment. Even if your users steal or leak the credentials, they are only valid for a short time.</p> <p>The audit log can record any action taken by a user and upload it to an S3-compatible object storage.</p> <p>Get started \u00bb</p>"},{"location":"usecases/webhosting/","title":"ContainerSSH for web hosting","text":"<p>Providing SSH access in a web hosting environment is tricky. Users may run unexpected scripts that consume lots of resources. They may have permission issues if they are not able to SSH with the same user as the webserver, which in turn presents security issues.</p> <p>Containers present a good solution for this problem: you can run a container as the same user as the web server, but keep them in isolation from the actual production environment. You can use NFS mounts to isolate them from the production servers. You can even mount folders based on an advanced permission matrix.</p> <p>However, running an SSH server per user is very cost-intensive in an industry where individual customers don't pay much. That's where ContainerSSH fills an important role: when users connect via SSH, ContainerSSH reaches out to your authentication server to verify user credentials and then contacts your configuration server to fetch the customized container configuration for your user. When your user disconnects, ContainerSSH removes their container and leaves no trace behind.</p> <p>ContainerSSH also supports SFTP, which provides secure file transfers. It can replace the old and arguably broken FTP, so you no longer have to worry about that either.</p> <p>If you are running multiple servers, you can even provide dynamic Docker connect strings and credentials to connect the server where the user is located.</p> <p>Get started \u00bb</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""}]}