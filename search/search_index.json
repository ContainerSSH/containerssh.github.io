{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Offering web hosting? ContainerSSH lets you offer full SSH access to your users. Clients are dropped in containers where they can only access their own environment. Authenticate against your existing user database and mount directories based on your existing permission matrix. Read more \u00bb Teaching the cloud? With ContainerSSH students can connect to an on-demand environment that you can customize with your own tools and credentials. On disconnect the environment is cleaned up. This is perfect for Linux or cloud learning environments. Read more \u00bb Building a honeypot? If you want to understand what attackers do once they breach SSH you can use ContainerSSH to drop them into an isolated environment . You can store their entire audit trail on an S3-compatible storage for later analysis. This includes SFTP file uploads! Read more \u00bb Building a jump host? ContainerSSH is being used to provide dynamic console access to an environment with sensitive credentials . Webhooks let you dynamically provision credentials in conjunction with secret management systems such as Hashicorp Vault. Read more \u00bb How does it work? \u00b6 The user opens an SSH connection to ContainerSSH. ContainerSSH calls the authentication server with the user's username and password/pubkey to check if it is valid. ContainerSSH calls the config server to obtain backend location and configuration (if configured). ContainerSSH calls the container backend to launch the container with the specified configuration. All input from the user is sent directly to the backend, output from the container is sent to the user. Watch as Video \ud83d\ude80 Get started \u00bb Demo \u00b6","title":"Home"},{"location":"#how-does-it-work","text":"The user opens an SSH connection to ContainerSSH. ContainerSSH calls the authentication server with the user's username and password/pubkey to check if it is valid. ContainerSSH calls the config server to obtain backend location and configuration (if configured). ContainerSSH calls the container backend to launch the container with the specified configuration. All input from the user is sent directly to the backend, output from the container is sent to the user. Watch as Video \ud83d\ude80 Get started \u00bb","title":"How does it work?"},{"location":"#demo","text":"","title":"Demo"},{"location":"about/","text":"About ContainerSSH Who makes ContainerSSH? Why? \u00b6 ContainerSSH is a fully open source community-driven project . It is made with \u2764\ufe0f by the following group of volunteers. Janos Pasztor Sanja Bonic Bence Santha Montgomery Edwards\u2074\u2074\u2078 Richard Kovacs Note: this list is opt-in for privacy reasons. If you wish to be listed on this page please add your name here . Companies \u00b6 We use services and tools from the following companies for free: Docker Docker has accepted ContainerSSH into their Docker Open Source Program . The open source program exempts ContainerSSH from the Docker Hub rate limits , which is important to us because we are using it to host the default guest image . GitHub GitHub provides the core development infrastructure for ContainerSSH , as well as the continuous integration system and the hosting for this website. Snyk Snyk monitors our Go dependencies and container root images for known security vulnerabilities and alerts us when we need to rebuild any of them. Terraform Cloud We use the Terraform Cloud by HashiCorp to automate the configuration of our GitHub organization.","title":"About ContainerSSH"},{"location":"about/#who-makes-containerssh-why","text":"ContainerSSH is a fully open source community-driven project . It is made with \u2764\ufe0f by the following group of volunteers. Janos Pasztor Sanja Bonic Bence Santha Montgomery Edwards\u2074\u2074\u2078 Richard Kovacs Note: this list is opt-in for privacy reasons. If you wish to be listed on this page please add your name here .","title":"Who makes ContainerSSH? Why?"},{"location":"about/#companies","text":"We use services and tools from the following companies for free:","title":"Companies"},{"location":"about/imprint/","text":"Imprint ContainerSSH is being developed by a community of volunteers . For a postal address suitable for legal service please contact handshake@containerssh.io . The address will be handed out to registered law firms and government agencies only.","title":"Imprint"},{"location":"about/license/","text":"License This website and the ContainerSSH source code is distributed under the MIT-0 license (see below) with the following exceptions: To keep things fair the ContainerSSH name, logos, and graphic assets located in the branding repository may only be used in accordance with the ContainerSSH brand license . Company marks such as GitHub, Docker, Twitter, etc. Trademarks are properties of their respective owners and may only be used according to their trademark guidelines. Some decorative images are licensed from Unsplash . MkDocs is distributed under the BSD license. Material for MkDocs is distributed under the MIT license. ContainerSSH binaries contain open source components under the following licenses: MIT, MIT-0 BSD Apache 2.0 Mozilla ISC Details of these licenses can be found in the NOTICE.md file published with each release or by running containerssh --license . MIT No Attribution License (MIT-0) \u00b6 Copyright (c) 2020 ContainerSSH contributors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/license/#mit-no-attribution-license-mit-0","text":"Copyright (c) 2020 ContainerSSH contributors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT No Attribution License (MIT-0)"},{"location":"about/packages/","text":"ContainerSSH maintains an experimental package repository at packages.containerssh.io . This page describes how to add the repository to your operating system. Warning The package repository is experimental and is likely to change in the future. Debian/Ubuntu First, you need to add the tools needed for adding a custom repository: sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Next, you should add our GnuPG key as a trusted key for packages: curl -fsSL https://packages.containerssh.io/debian/gpg | sudo apt-key add - Verify that you now have the correct fingerprint: sudo apt-key fingerprint F358FABA Add our repository: sudo add-apt-repository \\ \"deb [arch=amd64] https://packages.containerssh.io/debian ./\" Finally, refresh the package list: sudo apt-get update Now you can install the ContainerSSH packages.","title":"ContainerSSH packages"},{"location":"about/privacy/","text":"Privacy policy \u00b6 This is the privacy policy for the website containerssh.io . Cookies \u00b6 This website uses no cookies. Personal data collected by this website \u00b6 This website does not directly collect personal data. Third party providers \u00b6 This website is hosted on GitHub pages. As such, when loading this website, your IP address will be available and may be stored by GitHub in their access logs, along with the information which page specifically you loaded.","title":"Privacy Policy"},{"location":"about/privacy/#privacy-policy","text":"This is the privacy policy for the website containerssh.io .","title":"Privacy policy"},{"location":"about/privacy/#cookies","text":"This website uses no cookies.","title":"Cookies"},{"location":"about/privacy/#personal-data-collected-by-this-website","text":"This website does not directly collect personal data.","title":"Personal data collected by this website"},{"location":"about/privacy/#third-party-providers","text":"This website is hosted on GitHub pages. As such, when loading this website, your IP address will be available and may be stored by GitHub in their access logs, along with the information which page specifically you loaded.","title":"Third party providers"},{"location":"about/security/","text":"Security ContainerSSH is a security-relevant piece of software. If you have found any security issues you wish to report to us please mail handshake@containerssh.io . We are also in the process of signing all our binaries, which is expected to complete during 2021. Our GPG key \u00b6 You may use the following GPG key to encrypt messages to us. This key is always available at https://containerssh.io/gpg.txt . -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF/kX1IBEACyNunb2fpRr9tHKojGaAFu8be7UGYebIJUtg220IpSDzxSY/hq sklhoiICzz3uOaSwxdSShfCrHfcS3yDNkr79Wb5DgdZKreY041R4YptrR7NkX3ZD B3a7rk1LLWy6GmYHTNYbDvMOgTVAATg2fiUrewwLJ8gtVWQRGfFVI1TkJZ1vqUBq lLGqEEvdIWlHHigAgWQSf9UpGwsXYmSecY30Wv3MxBJeVtIvg5CxWewTUjuHKvzE mIJWDT28BPkAjjxpoku/KuDuXgwpbE9MNu+PN7RVYCMxyzcaFft842NzOIrFF0Q1 EH0XEYkoQ/hlONDBxngI/NMRWFtYiw+feyXvnuAs82FkjYAcz4HSrZt1Fedk5U1i UpnEwHrW00eSWmR3+d4AUzYmTP4XNrTgtzPvVWDJ2engM56jSjSZbDstbn08WJRx IbCj4ILREROguM5bpZVbh2+Gmij4bN7aDVvuud7mXR/eqDtnZ1r/+ydMrjriEhnn UpWlGGRM+SWvdrfZcZMUpqKW0dKuzxSIzM0ZFiUO20kSAHhBt2qUxvWOPzy2rLo0 MqyqxMkDmWxdF+Wo6TvYtFC9KOfZ5QBsVQExPCcb3RFUN6jQ584z+RKvSrIAmDw1 nOGwz19KvdRlWClb8JNSNmUXe6paw3MKPEPY8sDJPL0ivNigoQPEDHo29QARAQAB tEJDb250YWluZXJTU0ggKGh0dHBzOi8vY29udGFpbmVyc3NoLmlvKSA8aGFuZHNo YWtlQGNvbnRhaW5lcnNzaC5pbz6JAk4EEwEIADgWIQQ+5bAS+ntADNlSYB5GifHw 81j6ugUCX+RfUgIbAwULCQgHAgYVCgkICwIEFgIDAQIeAQIXgAAKCRBGifHw81j6 uhi8D/9r2WJFUA1tX8LrzrM1ELkRNjqkWVzFGWL/s9JBccpfXVvCiWf0ldFnaSRF EwBD18HwhG581Gt4LXr8K3gnpVIL976YrzhUh32cSQu4Cunvv2nIXcjW56SNbY+d +tfK/zWX8S492/gZfPr8fPQxK6OBEw+V7+92nzpE0M41TdH6g8f5iZSRS0PbtkcE 046G4KXmVvyktLc8zPlYd3ukt/xm4SoVhRGTccqzcLlP52FV1qhIZNm782Fb6eOn r1kNC9FO7NWc1uEBfWWHbanhLrZXt8lQ6eP+0U3JLW3cEtyP1HtMdmzjuCfMLDFU yEIkI36EMG6c1kFeqije6cdGLCZKaAc7AZLnfwhKidy/0NGjSmKlCKOU76UrKgIR Q8i4HH/IfE+OO/9BHZtLgohB/L8UeqytrWTUm3nkHkQev7h1/pcIGcZLW73JsHlk Bv97N/X9h0EcC7BSWKsobT6TQtH7iRerrR0WJ4pEte7GNimaWsGLES94G19oHppE jXsulmjW3ip16Vn6rS3VZQFXQ3I9HcOLh9bxwsBx9bSbsufoNSMmxm5m2oBg8cHG LMeKKmRSuaPVOI7oKZtIj1pqYRjTnwrysR1YwnvTvxoaOfpirMk8ZVPm+3LwBw2A y6MW4d/sov6VKXwwVycA+ITa4xff5KZec41/RgVCe4a1G5a8irkCDQRf5F+AARAA 5FuMzgwy5STDCysB38ddi7hp34hD5IIXgxs3g7K+bXUFesj+aYR1accQ9X8CqGC7 oKw0vB8Y97OaPLsswX70XHgRr99qK4iV8G8mM/DQx++gNqCfpmS7yYrUqIrTXyxq q6kEhAHWQ6d4LZ/MMQAh0SKsEyvqa+RYp6CRYOZ4GrIHaE/Ax3jlp1zPscdolCkf M2Zk/853ku3DvobRDWN+4EQQCdTa/2FFIHthzF76EDj3phr8QS7IsYZQi/KSiTtz 6K2j7vEMxjk3s9hhB2ovkkQaEASKftc2c+PztGMGsQKOWQmInoMQKjGjZu7PuYD7 dTJ4gDrLYA6OjPy+uRhZw2DLl0HDY8bDY9eMdIWW3LNB7Q2WcigMsRtJDMlL1Xkc xtKFy63opiJ75QXGdgtuYnQbRBAPKVz+sksMXyuitE2ST2dn2vIeqQe5/30xZuBb yNp3YyoD70wnJpJ4WzAeZiB86e7t36+4QeUWO4NibIkoip0UmRWqdGZNTY6AWaMM xLMpiI8AJ/tu+prXJa4Nk3OAoBnD+yo4zSzSly3xhSWARPmzA7P+rsQzMp7PJDQY 1I7pnykEpaFDg0WXh/Lyarp4lkaB1etRve8WNFINtu8xwTB/PtcMyE4InU7g9Dm4 JmOLFah18TROyVLIhxWERQ7KnUNdMNtXX8WwSL7JOYsAEQEAAYkEbAQYAQgAIBYh BD7lsBL6e0AM2VJgHkaJ8fDzWPq6BQJf5F+AAhsCAkAJEEaJ8fDzWPq6wXQgBBkB CAAdFiEE9dc2FgMqG1zmlCp4YcU1CuyR4skFAl/kX4AACgkQYcU1CuyR4slvARAA 4D6+phqJsDuDo28s/vB6dzbHkJe/iZiok75zy8gCTzNP9xgAI8PYcUMLgaTVQ2aj HiMQPyCfmnGjuK4oE5oXFzEWkYO4SOpangLAFRIEIu6PhwJJB5tFVEtj6fzS9FAb kUbfuePYUzTbuSgKP8SOkkBnPZftZMYY1jcPDq/wQ4pNdkDV5wyN2YHOxRocAibt cP2aOG6agMgKJitSJ/llVIJcw90epcgMMSXCSDH9J3el/AjGTE2LwFrymnSkIgDO c81TawYxKKYevhwuDOQto7b0DBDGPz6D7nJisDmx1BHhKwnfUc+nkuaD2LPiJ9S5 Mbp4jHraVXfd8IM9W4Xb1jtYDlHSmMRwSjA73Coij5F4uh+As4l6jRwDzG+5ZYsN duXEGrychdnIT9sREHJJ8UUVBZayh4kOnRmem0MTAgEJGifFMv1mr/ySdOYCN3f9 lOibMsPbloha1g3MczMkA4TdCNDfC28P3v1cEgMxtdHsUNN5BblvMorQ4zM1JXPL 8A5/ZKFTTufjyhs+AW1BNKzxoYM7N1HFu4kPg8V2o59Y5Z5kD5pehf4+d5tyE5Y5 UeoS3SxeWazmJERaZsrwjJxqjgP/zbda53PUSg9naCsf+10e7+hSl0fK+1UYcMtd CcHosuh9x5LZHP/rgACkthYDt583boTATubpM2rJWVVyVw/+PRfx4PX81X+2S/Am rVR9QebRAtI0v5+5ZnewTLWJdhsg1KoTnc21OMKU8YbRVvXDlv9QjOe3W8/sl3H0 uWEBAAWFylDiZa0UERhmT23fRfRHnWDjXv8IXslBS2W1MAmrLXb4RUAM3D0MwwZK IbnEC1W74QleJ3TO7WIDZraS8GlGJImdUy1BARN4rdBihx0GHM4Gs3gM+11Rel+u jgy2Zfqw/niD0V8WKPzY7tdraVOWFrB/aly3rQlLyPGEiZkzqlsyQiP6+pyBwRv0 On6Juo/gzpfT78afvXBZr/oUCl6eBWzelLkArtQV++bwPfDTODdQ4vPBFVgLL7s5 cHLKMDtKxV13kn+fC4cuMg8aWpWVtZ4VpSnkYY3XhCirVrz/w0mLB5fa5PmBvH6+ ktDfrnw2+P9yChqb1QmFWdGsSrVuxVjPfY4GxsE5tvEdVGW4wodyEduWLUGEASQ7 w4ZGIKhLx9RScC4Z84YxY0fwPx6foMfthmSfDc7LsjGN3bBWDh7l8nRxVgViBTNb rlgKQqnNr/1eoRdtNsSkRiXNhpOEHiRorLHKL5DsiIDBsTYO97DqU3j4zJTz1gPz DWnJgkKlavyYE2FM8Xfb04uN6AVRRKDmR/GLaJyCF12AbnEpd45zeDLJ5TuiWy+v WD0QH8rr8tLoFvovcHVU0GFmggS5Ag0EX+RgTAEQALkyge/1WC6ZVtmkvGkGTGyT afBSRWc5T0hTQ1+3DejVBSmrzh+N4OOWd2ay9aEfw+gtR9W9b+2vcMP47XG0SGhL o3rfW3bQ+KPiu+nBFvE1LxQp/C+exvmOxITMEGIEQdxXZRFpCGIpzCUPj/+2PknD VhlYjGK8cwVhODtlJ8KCjObv5ypCGhNfuN2m5F/GE0pBWBjyv1z+ohVK6xdjL//E 8BaK0ZIEudj40aSD86gAkpiaW9Wh05jFvZPcjej+N2G12prOP4OIfuzIqCiot0bW tvI+EirJbzGvLvcLb1G304Je6Om9is47MV9++0X3fFhKkiwOAl5OLfqNaqCZx2I1 21C3Pll1Q7O4/ZoIHUzhr9dmHwDIuGMzO4auILjSJIXBw9A0brDkzaMDIFSPOohW ryp0amH3DLuxUA0hXGsrM9JD0w33EUjxieSam92sHzDofpWkgRfcqJN85vVC1Qc8 7CIITYVO1uxHT+XzX2x/sDBBwH28XRZaNf3HggwMggGiHQBxclB4QltKJiDf1VJN IOoLd09uwLbrbnDbvahzQEty3ETCU+HCW1GCGhYWQW5Pos2jHTUXVf9T3ZegIKfI oohfh9CVl/9ZQMjWjE0wd6/gntz+FlMHPy8qMs1Yh8C8Rddk7uRKxG46Z6ht6l33 QebjIvKdmLd76Om7jTFlABEBAAGJBGwEGAEIACAWIQQ+5bAS+ntADNlSYB5GifHw 81j6ugUCX+RgbQIbLgJAwXQgBBkBCAAdFiEEgaQXWNyEMhcn+lRfXzeeAnpsK6sF Al/kYEwACgkQXzeeAnpsK6ukhhAAkC+J9HIxrfKaNTBmKHNbD9p1+ZegY8otejNK Kw3QDWBSCZFmouQAXcylGsz63LyC7lUr9eIbEffn4W8XOlGHsp9VjYMb6VmEPeCn 4gnXRfPjNbGtcrOQ/dsla/U5TmQuLgsAYKfpf1BMC1yXiuIJUSMmoefQ1gQ7o8+c sb31id8R2RIQ4EcYRzeaGnBcy1tLt/Aq2YkjC3KRGh/fbtGmI3xYpE2aW4RX/p1b RKfkmQt0xPiK9ujy4Jk1e5O4sTlvaCd2PgxgQQxg/oP6yA5AQNlAsc9n23jATmAa ENbX24EPWAozubfYfFFslrQTR6GifQQpyTsj3DAeNaYdsnAZM8LSkQ1+/puMt7mO EmQPtNHOn5EIHwMpdp4nXBjVLVRXMBwZrE7izQzkwP0cz8Ws1ZlxEwrD6QgBFxN1 0yg0JPOPDfD1k6JaJe6/e4tYBiRQTaVoaVICGxuujlNcRC0A/Fk+tLEaQU2x2mq5 grXsrAiz1SHxP9KynCgqd3VvIXJRShs1icL34JZMBBQ9BGFm2dMhvqDVzFGmK0CU G/s3Yjpw+FlTq+f9qLHexeMODgN1UFhLQ4jA+XE/DOWeRu7LKNKKh45P1ieVr0zd YIjlPILtz6eYOxmJKrM+RI05OWj6ntiCtZPTg8yKMtzLdSqrmyk6OUlkThNUva4s IKP1XdAJEEaJ8fDzWPq6gzcP/ikf5T0sGyTvO2hqEBZPCJ7xF/6MxL/lvJsFbJdd M+ZBS4S7o9HePn0EOLeqwTi7q6iHf3QkOD54F62u4HAo78/ZT4HsWkaPRnKtMzFg CLbkSx/5aZ/ltA+XKEYx4kA3dNotoLQFwj/bk1m1tWM3PqrJhoX+J7XIxgMfuzbb BKehakiaTjBJ6GDG2tn/xMNssPZJWgGh+cJcPzGRpM1DtuFcZH1eCIwYA17KYg8X DppowLObcL/iSurpdNvlFzjIUV7wWTD1sa7dHP+u9UoQoy/hozUYmDOhSWAGWeml OC7IPULIfCH17p0o5pEvWjot2iasUgco0b/MIcN+PuT8GNt2gU4J0i+XHeiXE20A 6L65f7CQcCRqlfIJMXQ7XwVlbGGrndBgCNiD7i3ddxkYGZv7Iy4UPVjJxlQoRbqR JsOMLPdVmExcbHc00HesNzbuVYsvzSrKgHgA0QMxCK03JWhJiad0vKWQOlDCkucA V7Fzj+Ogg8w4ZI8qsdefEtQ+y414yyGkIdngJopT40KNlkpLx9RAl5sZTtlMx189 zIC7fu1wQD0xxYDiyOm9FxJRyo70kCBJNgxj2NSREOeDFs0rlyaoZENFh/75boVG rHnNV/NmnHbEeqxAdDF4QGXhf+F899lmxTm0vRGsSsGYGFvWQaKxBOPVb8XvPdve 1IxE =/NjK -----END PGP PUBLIC KEY BLOCK-----","title":"Security"},{"location":"about/security/#our-gpg-key","text":"You may use the following GPG key to encrypt messages to us. This key is always available at https://containerssh.io/gpg.txt . -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF/kX1IBEACyNunb2fpRr9tHKojGaAFu8be7UGYebIJUtg220IpSDzxSY/hq sklhoiICzz3uOaSwxdSShfCrHfcS3yDNkr79Wb5DgdZKreY041R4YptrR7NkX3ZD B3a7rk1LLWy6GmYHTNYbDvMOgTVAATg2fiUrewwLJ8gtVWQRGfFVI1TkJZ1vqUBq lLGqEEvdIWlHHigAgWQSf9UpGwsXYmSecY30Wv3MxBJeVtIvg5CxWewTUjuHKvzE mIJWDT28BPkAjjxpoku/KuDuXgwpbE9MNu+PN7RVYCMxyzcaFft842NzOIrFF0Q1 EH0XEYkoQ/hlONDBxngI/NMRWFtYiw+feyXvnuAs82FkjYAcz4HSrZt1Fedk5U1i UpnEwHrW00eSWmR3+d4AUzYmTP4XNrTgtzPvVWDJ2engM56jSjSZbDstbn08WJRx IbCj4ILREROguM5bpZVbh2+Gmij4bN7aDVvuud7mXR/eqDtnZ1r/+ydMrjriEhnn UpWlGGRM+SWvdrfZcZMUpqKW0dKuzxSIzM0ZFiUO20kSAHhBt2qUxvWOPzy2rLo0 MqyqxMkDmWxdF+Wo6TvYtFC9KOfZ5QBsVQExPCcb3RFUN6jQ584z+RKvSrIAmDw1 nOGwz19KvdRlWClb8JNSNmUXe6paw3MKPEPY8sDJPL0ivNigoQPEDHo29QARAQAB tEJDb250YWluZXJTU0ggKGh0dHBzOi8vY29udGFpbmVyc3NoLmlvKSA8aGFuZHNo YWtlQGNvbnRhaW5lcnNzaC5pbz6JAk4EEwEIADgWIQQ+5bAS+ntADNlSYB5GifHw 81j6ugUCX+RfUgIbAwULCQgHAgYVCgkICwIEFgIDAQIeAQIXgAAKCRBGifHw81j6 uhi8D/9r2WJFUA1tX8LrzrM1ELkRNjqkWVzFGWL/s9JBccpfXVvCiWf0ldFnaSRF EwBD18HwhG581Gt4LXr8K3gnpVIL976YrzhUh32cSQu4Cunvv2nIXcjW56SNbY+d +tfK/zWX8S492/gZfPr8fPQxK6OBEw+V7+92nzpE0M41TdH6g8f5iZSRS0PbtkcE 046G4KXmVvyktLc8zPlYd3ukt/xm4SoVhRGTccqzcLlP52FV1qhIZNm782Fb6eOn r1kNC9FO7NWc1uEBfWWHbanhLrZXt8lQ6eP+0U3JLW3cEtyP1HtMdmzjuCfMLDFU yEIkI36EMG6c1kFeqije6cdGLCZKaAc7AZLnfwhKidy/0NGjSmKlCKOU76UrKgIR Q8i4HH/IfE+OO/9BHZtLgohB/L8UeqytrWTUm3nkHkQev7h1/pcIGcZLW73JsHlk Bv97N/X9h0EcC7BSWKsobT6TQtH7iRerrR0WJ4pEte7GNimaWsGLES94G19oHppE jXsulmjW3ip16Vn6rS3VZQFXQ3I9HcOLh9bxwsBx9bSbsufoNSMmxm5m2oBg8cHG LMeKKmRSuaPVOI7oKZtIj1pqYRjTnwrysR1YwnvTvxoaOfpirMk8ZVPm+3LwBw2A y6MW4d/sov6VKXwwVycA+ITa4xff5KZec41/RgVCe4a1G5a8irkCDQRf5F+AARAA 5FuMzgwy5STDCysB38ddi7hp34hD5IIXgxs3g7K+bXUFesj+aYR1accQ9X8CqGC7 oKw0vB8Y97OaPLsswX70XHgRr99qK4iV8G8mM/DQx++gNqCfpmS7yYrUqIrTXyxq q6kEhAHWQ6d4LZ/MMQAh0SKsEyvqa+RYp6CRYOZ4GrIHaE/Ax3jlp1zPscdolCkf M2Zk/853ku3DvobRDWN+4EQQCdTa/2FFIHthzF76EDj3phr8QS7IsYZQi/KSiTtz 6K2j7vEMxjk3s9hhB2ovkkQaEASKftc2c+PztGMGsQKOWQmInoMQKjGjZu7PuYD7 dTJ4gDrLYA6OjPy+uRhZw2DLl0HDY8bDY9eMdIWW3LNB7Q2WcigMsRtJDMlL1Xkc xtKFy63opiJ75QXGdgtuYnQbRBAPKVz+sksMXyuitE2ST2dn2vIeqQe5/30xZuBb yNp3YyoD70wnJpJ4WzAeZiB86e7t36+4QeUWO4NibIkoip0UmRWqdGZNTY6AWaMM xLMpiI8AJ/tu+prXJa4Nk3OAoBnD+yo4zSzSly3xhSWARPmzA7P+rsQzMp7PJDQY 1I7pnykEpaFDg0WXh/Lyarp4lkaB1etRve8WNFINtu8xwTB/PtcMyE4InU7g9Dm4 JmOLFah18TROyVLIhxWERQ7KnUNdMNtXX8WwSL7JOYsAEQEAAYkEbAQYAQgAIBYh BD7lsBL6e0AM2VJgHkaJ8fDzWPq6BQJf5F+AAhsCAkAJEEaJ8fDzWPq6wXQgBBkB CAAdFiEE9dc2FgMqG1zmlCp4YcU1CuyR4skFAl/kX4AACgkQYcU1CuyR4slvARAA 4D6+phqJsDuDo28s/vB6dzbHkJe/iZiok75zy8gCTzNP9xgAI8PYcUMLgaTVQ2aj HiMQPyCfmnGjuK4oE5oXFzEWkYO4SOpangLAFRIEIu6PhwJJB5tFVEtj6fzS9FAb kUbfuePYUzTbuSgKP8SOkkBnPZftZMYY1jcPDq/wQ4pNdkDV5wyN2YHOxRocAibt cP2aOG6agMgKJitSJ/llVIJcw90epcgMMSXCSDH9J3el/AjGTE2LwFrymnSkIgDO c81TawYxKKYevhwuDOQto7b0DBDGPz6D7nJisDmx1BHhKwnfUc+nkuaD2LPiJ9S5 Mbp4jHraVXfd8IM9W4Xb1jtYDlHSmMRwSjA73Coij5F4uh+As4l6jRwDzG+5ZYsN duXEGrychdnIT9sREHJJ8UUVBZayh4kOnRmem0MTAgEJGifFMv1mr/ySdOYCN3f9 lOibMsPbloha1g3MczMkA4TdCNDfC28P3v1cEgMxtdHsUNN5BblvMorQ4zM1JXPL 8A5/ZKFTTufjyhs+AW1BNKzxoYM7N1HFu4kPg8V2o59Y5Z5kD5pehf4+d5tyE5Y5 UeoS3SxeWazmJERaZsrwjJxqjgP/zbda53PUSg9naCsf+10e7+hSl0fK+1UYcMtd CcHosuh9x5LZHP/rgACkthYDt583boTATubpM2rJWVVyVw/+PRfx4PX81X+2S/Am rVR9QebRAtI0v5+5ZnewTLWJdhsg1KoTnc21OMKU8YbRVvXDlv9QjOe3W8/sl3H0 uWEBAAWFylDiZa0UERhmT23fRfRHnWDjXv8IXslBS2W1MAmrLXb4RUAM3D0MwwZK IbnEC1W74QleJ3TO7WIDZraS8GlGJImdUy1BARN4rdBihx0GHM4Gs3gM+11Rel+u jgy2Zfqw/niD0V8WKPzY7tdraVOWFrB/aly3rQlLyPGEiZkzqlsyQiP6+pyBwRv0 On6Juo/gzpfT78afvXBZr/oUCl6eBWzelLkArtQV++bwPfDTODdQ4vPBFVgLL7s5 cHLKMDtKxV13kn+fC4cuMg8aWpWVtZ4VpSnkYY3XhCirVrz/w0mLB5fa5PmBvH6+ ktDfrnw2+P9yChqb1QmFWdGsSrVuxVjPfY4GxsE5tvEdVGW4wodyEduWLUGEASQ7 w4ZGIKhLx9RScC4Z84YxY0fwPx6foMfthmSfDc7LsjGN3bBWDh7l8nRxVgViBTNb rlgKQqnNr/1eoRdtNsSkRiXNhpOEHiRorLHKL5DsiIDBsTYO97DqU3j4zJTz1gPz DWnJgkKlavyYE2FM8Xfb04uN6AVRRKDmR/GLaJyCF12AbnEpd45zeDLJ5TuiWy+v WD0QH8rr8tLoFvovcHVU0GFmggS5Ag0EX+RgTAEQALkyge/1WC6ZVtmkvGkGTGyT afBSRWc5T0hTQ1+3DejVBSmrzh+N4OOWd2ay9aEfw+gtR9W9b+2vcMP47XG0SGhL o3rfW3bQ+KPiu+nBFvE1LxQp/C+exvmOxITMEGIEQdxXZRFpCGIpzCUPj/+2PknD VhlYjGK8cwVhODtlJ8KCjObv5ypCGhNfuN2m5F/GE0pBWBjyv1z+ohVK6xdjL//E 8BaK0ZIEudj40aSD86gAkpiaW9Wh05jFvZPcjej+N2G12prOP4OIfuzIqCiot0bW tvI+EirJbzGvLvcLb1G304Je6Om9is47MV9++0X3fFhKkiwOAl5OLfqNaqCZx2I1 21C3Pll1Q7O4/ZoIHUzhr9dmHwDIuGMzO4auILjSJIXBw9A0brDkzaMDIFSPOohW ryp0amH3DLuxUA0hXGsrM9JD0w33EUjxieSam92sHzDofpWkgRfcqJN85vVC1Qc8 7CIITYVO1uxHT+XzX2x/sDBBwH28XRZaNf3HggwMggGiHQBxclB4QltKJiDf1VJN IOoLd09uwLbrbnDbvahzQEty3ETCU+HCW1GCGhYWQW5Pos2jHTUXVf9T3ZegIKfI oohfh9CVl/9ZQMjWjE0wd6/gntz+FlMHPy8qMs1Yh8C8Rddk7uRKxG46Z6ht6l33 QebjIvKdmLd76Om7jTFlABEBAAGJBGwEGAEIACAWIQQ+5bAS+ntADNlSYB5GifHw 81j6ugUCX+RgbQIbLgJAwXQgBBkBCAAdFiEEgaQXWNyEMhcn+lRfXzeeAnpsK6sF Al/kYEwACgkQXzeeAnpsK6ukhhAAkC+J9HIxrfKaNTBmKHNbD9p1+ZegY8otejNK Kw3QDWBSCZFmouQAXcylGsz63LyC7lUr9eIbEffn4W8XOlGHsp9VjYMb6VmEPeCn 4gnXRfPjNbGtcrOQ/dsla/U5TmQuLgsAYKfpf1BMC1yXiuIJUSMmoefQ1gQ7o8+c sb31id8R2RIQ4EcYRzeaGnBcy1tLt/Aq2YkjC3KRGh/fbtGmI3xYpE2aW4RX/p1b RKfkmQt0xPiK9ujy4Jk1e5O4sTlvaCd2PgxgQQxg/oP6yA5AQNlAsc9n23jATmAa ENbX24EPWAozubfYfFFslrQTR6GifQQpyTsj3DAeNaYdsnAZM8LSkQ1+/puMt7mO EmQPtNHOn5EIHwMpdp4nXBjVLVRXMBwZrE7izQzkwP0cz8Ws1ZlxEwrD6QgBFxN1 0yg0JPOPDfD1k6JaJe6/e4tYBiRQTaVoaVICGxuujlNcRC0A/Fk+tLEaQU2x2mq5 grXsrAiz1SHxP9KynCgqd3VvIXJRShs1icL34JZMBBQ9BGFm2dMhvqDVzFGmK0CU G/s3Yjpw+FlTq+f9qLHexeMODgN1UFhLQ4jA+XE/DOWeRu7LKNKKh45P1ieVr0zd YIjlPILtz6eYOxmJKrM+RI05OWj6ntiCtZPTg8yKMtzLdSqrmyk6OUlkThNUva4s IKP1XdAJEEaJ8fDzWPq6gzcP/ikf5T0sGyTvO2hqEBZPCJ7xF/6MxL/lvJsFbJdd M+ZBS4S7o9HePn0EOLeqwTi7q6iHf3QkOD54F62u4HAo78/ZT4HsWkaPRnKtMzFg CLbkSx/5aZ/ltA+XKEYx4kA3dNotoLQFwj/bk1m1tWM3PqrJhoX+J7XIxgMfuzbb BKehakiaTjBJ6GDG2tn/xMNssPZJWgGh+cJcPzGRpM1DtuFcZH1eCIwYA17KYg8X DppowLObcL/iSurpdNvlFzjIUV7wWTD1sa7dHP+u9UoQoy/hozUYmDOhSWAGWeml OC7IPULIfCH17p0o5pEvWjot2iasUgco0b/MIcN+PuT8GNt2gU4J0i+XHeiXE20A 6L65f7CQcCRqlfIJMXQ7XwVlbGGrndBgCNiD7i3ddxkYGZv7Iy4UPVjJxlQoRbqR JsOMLPdVmExcbHc00HesNzbuVYsvzSrKgHgA0QMxCK03JWhJiad0vKWQOlDCkucA V7Fzj+Ogg8w4ZI8qsdefEtQ+y414yyGkIdngJopT40KNlkpLx9RAl5sZTtlMx189 zIC7fu1wQD0xxYDiyOm9FxJRyo70kCBJNgxj2NSREOeDFs0rlyaoZENFh/75boVG rHnNV/NmnHbEeqxAdDF4QGXhf+F899lmxTm0vRGsSsGYGFvWQaKxBOPVb8XvPdve 1IxE =/NjK -----END PGP PUBLIC KEY BLOCK-----","title":"Our GPG key"},{"location":"blog/2020/11/25/the-road-to-0-4/","text":"The road to ContainerSSH 0.4: modularized structure, audit logging, and more \u00b6 November 25, 2020 After a rapid rush of releases this summer we have announced that version 0.4.0 would have a long-awaited feature: detailed audit logging . This feature would allow for a forensic reconstruction of an SSH session. The use cases for this are diverse: from building honeypots to securing a corporate environment. We even published a preview release for test driving this feature. We even implemented an automatic upload for the audit logs to an S3-compatible object storage. So, what happened? Why isn't 0.4.0 released yet? The delay has everything to do with maintainability . The PR-1 implementation of the audit logging was built right into ContainerSSH causing a deluge of code changes. While it technically worked, it blew up the code in size and made features extremely hard to test. Look at this code, for example . The actual authentication code dwarfs in comparison to the audit logging parts. You could say, the code violates the Single Responsibility Principle . There is no way we could retrofit component-level tests into this. When we began work on ContainerSSH we knew that the code quality was prototype-level at best and we'd have to overhaul large parts before the 1.0 release. In essence, version 0.4 became the release to make that overhaul happen. We started pulling out large parts of the codebase into independent libraries and started retrofitting them with unittests. Needless to say, moving from only having a few integration tests to writing unittests for each component unearthed a slew of bugs, which were promptly fixed. Starting ContainerSSH with a prototyping approach wasn't a bad decision, though: it helped us getting something working fairly quickly and kept motivation high. This is especially important with a purely open source project. Pulling everything apart into separate libraries also gave us a couple of additional advantages. We created developer documentation for each library , and we now also have the ability to extend ContainerSSH in a significant way. ContainerSSH now has clean APIs to handle SSH events . These clean APIs allow developers to plug in additional functionality without breaking any existing features. For example, the audit log functionality is integrated in a separate repository with this approach . Using a layered approach gives us quite a few options. One idea we are toying with is to create an SSH proxy that forwards connections to a backend SSH server. This would allow users to deploy ContainerSSH as a pure audit logging facility. Another idea is to build in PAM authentication and enable a direct shell on the host , which would enable ContainerSSH to function as a replacement for OpenSSH with the added functionality of audit logging. If you like these ideas please comment on the issues linked above and let us know about your use case . So, when is 0.4 coming out? We don't know yet. Our plan is early next year , but our focus in this release is stability.","title":" The Road to ContainerSSH 0.4"},{"location":"blog/2020/11/25/the-road-to-0-4/#the-road-to-containerssh-04-modularized-structure-audit-logging-and-more","text":"November 25, 2020 After a rapid rush of releases this summer we have announced that version 0.4.0 would have a long-awaited feature: detailed audit logging . This feature would allow for a forensic reconstruction of an SSH session. The use cases for this are diverse: from building honeypots to securing a corporate environment. We even published a preview release for test driving this feature. We even implemented an automatic upload for the audit logs to an S3-compatible object storage. So, what happened? Why isn't 0.4.0 released yet? The delay has everything to do with maintainability . The PR-1 implementation of the audit logging was built right into ContainerSSH causing a deluge of code changes. While it technically worked, it blew up the code in size and made features extremely hard to test. Look at this code, for example . The actual authentication code dwarfs in comparison to the audit logging parts. You could say, the code violates the Single Responsibility Principle . There is no way we could retrofit component-level tests into this. When we began work on ContainerSSH we knew that the code quality was prototype-level at best and we'd have to overhaul large parts before the 1.0 release. In essence, version 0.4 became the release to make that overhaul happen. We started pulling out large parts of the codebase into independent libraries and started retrofitting them with unittests. Needless to say, moving from only having a few integration tests to writing unittests for each component unearthed a slew of bugs, which were promptly fixed. Starting ContainerSSH with a prototyping approach wasn't a bad decision, though: it helped us getting something working fairly quickly and kept motivation high. This is especially important with a purely open source project. Pulling everything apart into separate libraries also gave us a couple of additional advantages. We created developer documentation for each library , and we now also have the ability to extend ContainerSSH in a significant way. ContainerSSH now has clean APIs to handle SSH events . These clean APIs allow developers to plug in additional functionality without breaking any existing features. For example, the audit log functionality is integrated in a separate repository with this approach . Using a layered approach gives us quite a few options. One idea we are toying with is to create an SSH proxy that forwards connections to a backend SSH server. This would allow users to deploy ContainerSSH as a pure audit logging facility. Another idea is to build in PAM authentication and enable a direct shell on the host , which would enable ContainerSSH to function as a replacement for OpenSSH with the added functionality of audit logging. If you like these ideas please comment on the issues linked above and let us know about your use case . So, when is 0.4 coming out? We don't know yet. Our plan is early next year , but our focus in this release is stability.","title":"The road to ContainerSSH 0.4: modularized structure, audit logging, and more"},{"location":"blog/2020/12/24/the-agent/","text":"Announcing the ContainerSSH Guest Agent \u00b6 December 24, 2020 ContainerSSH is an integration project between the SSH library and the Docker and Kubernetes API. However, neither the Docker nor the Kubernetes API have been designed to host some of the more intricate SSH specific features. For example, the Kubernetes \"attach\" API does not allow for retrieving the output of the command running in the container that happened before attaching reliably,and neither Docker nor Kubernetes allow sending signals to commands running in an \"exec\", etc. We won't go into details on these various issues, suffice it to say, some of them break the expectations you would have for a classic SSH server. There are two paths ahead of us: either try to send pull requests to the Docker and Kubernetes projects to patch these features in, or add a guest agent to the container images that enable these extra features. Sending in patches to enable all the functionality would be a very long process and chances are that our patches wouldn't be accepted as they add additional functionality that is, admittedly, fringe for most users. Therefore, we opted to build a guest agent . The ContainerSSH guest agent is a binary containing only minimal functionality and no external dependencies that can easily be added to any container image as a single binary. We have already added it to the default containerssh/containerssh-guest-image and we encourage users who built their own image to include the agent as well and keep it updated. Please see github.com/containerssh/agent for details. That being said, the guest image is and will be optional. It will be a feature that needs to be explicitly enabled in the configuration. Guest agent support will arrive in ContainerSSH 0.4 after the holidays. Merry Christmas and Happy Holidays!","title":" Announcing the ContainerSSH Guest Agent"},{"location":"blog/2020/12/24/the-agent/#announcing-the-containerssh-guest-agent","text":"December 24, 2020 ContainerSSH is an integration project between the SSH library and the Docker and Kubernetes API. However, neither the Docker nor the Kubernetes API have been designed to host some of the more intricate SSH specific features. For example, the Kubernetes \"attach\" API does not allow for retrieving the output of the command running in the container that happened before attaching reliably,and neither Docker nor Kubernetes allow sending signals to commands running in an \"exec\", etc. We won't go into details on these various issues, suffice it to say, some of them break the expectations you would have for a classic SSH server. There are two paths ahead of us: either try to send pull requests to the Docker and Kubernetes projects to patch these features in, or add a guest agent to the container images that enable these extra features. Sending in patches to enable all the functionality would be a very long process and chances are that our patches wouldn't be accepted as they add additional functionality that is, admittedly, fringe for most users. Therefore, we opted to build a guest agent . The ContainerSSH guest agent is a binary containing only minimal functionality and no external dependencies that can easily be added to any container image as a single binary. We have already added it to the default containerssh/containerssh-guest-image and we encourage users who built their own image to include the agent as well and keep it updated. Please see github.com/containerssh/agent for details. That being said, the guest image is and will be optional. It will be a feature that needs to be explicitly enabled in the configuration. Guest agent support will arrive in ContainerSSH 0.4 after the holidays. Merry Christmas and Happy Holidays!","title":"Announcing the ContainerSSH Guest Agent"},{"location":"blog/2021/03/19/we-messed-up/","text":"We broke your images \ud83d\ude22 \u00b6 March 19, 2021 Two days ago, on March 17, 2021 around 4:30 PM UTC we pushed a change to our build system that broke the container images we published on the Docker Hub . This change resulted in the following error when running the container: Cannot start service containerssh: OCI runtime create failed: container_linux.go:349: starting container process caused \"exec: \\\"/containerssh\\\": permission denied\": unknown To make matters worse, this did not only affect the most recent image, it broke all container images. The issue was reported an hour later and fixed on around noon UTC on the 18th of March, 2021 . If you are affected by this issue you can pull the fixed ContainerSSH image by pulling it: Docker docker pull containerssh/containerssh:<version> Podman podman pull containerssh/containerssh:<version> Kubernetes Please set the imagePullPolicy in your pod spec to Always or switch to the image containerssh/containerssh:<version>-20200318 The <version> tag in this case should be replaced with your ContainerSSH version (e.g. 0.3.1 ). There is no way around it: we messed up. Pretty badly at that, we potentially broke your production environment without an easy way to roll back to a previous version. This should not happen, not even in a pre-1.0 version, especially not with something as trivial as a permission mistake. These images should have never made it to the Docker Hub, our testing procedures (obviously lacking) should have caught this and we are determined to fix them . We are very sorry for the inconvenience and the potential outage this issue has caused. Please reach out to us If you need to reach out quickly you can tag or DM us in Twitter or tag the @ContainerSSH role on the Debugged.it Discord to notify the core maintainers. Versioning in the future \u00b6 Going forward the ContainerSSH images will be versioned in two parts: the ContainerSSH version and the image build date. For example, the image tag 0.3.1-20200318 points to the image built from ContainerSSH version 0.3.1 on the 18th of March, 2020. The tag 0.3.1 points to the latest built from ContainerSSH version 0.3.1. The tag 0.3 points to the latest ContainerSSH version in the 0.3 series, and so on. You can find the full list of available tags on the Docker hub . Why are we doing this? This is necessary because container base images (Alpine Linux in our case) get security updates much more frequently than we release ContainerSSH versions. Why did this happen (post mortem) \u00b6 With the upcoming 0.4 release we are drawing closer to creating a first stable version of ContainerSSH (1.0). As part of the effort to secure the container images we publish we are now using Snyk , graciously made available to open source projects for free. As we scanned our container images built late last year we realized that there were multiple vulnerable libraries in them. These vulnerabilities did not affect ContainerSSH, but it highlights the need to update the images we release much more frequently than we release new ContainerSSH versions. It is also unreasonable to ask system administrators to jump through the hoops of switching to a new ContainerSSH version just because there is a new container image available. Therefore, we decided to version the container images separately from ContainerSSH as described above. However, our existing build system ( Goreleaser , an excellent tool) does not have the capability to manage images in this manner. Therefore, we had to come up with a new build system. We were looking at several ones, but none of them could fulfill the need for a cross-platform build system that is easy for developers to run on their potentially non-Linux machines. Therefore, we decided to code the relatively simple process of building a container image in Go . This tool downloads the Linux .tar.gz files from the GitHub releases as specified in the configuration and unpacks them. This is where the critical mistake happened: the unpacking code used os.Create() instead of os.OpenFile() . This resulted in the permissions not being set on the files extracted from the archive. The Dockerfile moved from the Goreleaser build system also did not contain the required chmod instruction. How did this make it into production? It clearly never should have, and that's a hole in our testing procedures. It is not enough for us to test the ContainerSSH code, we should have tested the built images before pushing them, no matter what build tool was used. Mistakes can happen, we need to make sure big ones don't make it to the images you are using. To fix this issue we will institute automated tests in the build tool that test the functionality of ContainerSSH end-to-end by opening a real SSH connection and executing a real command. This will not be a comprehensive test suite since those are covered by lower level tests, but we will test if the SSH connection can be established, the containers are started, and the output of an executed command is as expected. To sum it up, here's what we learned: Create automated end to end tests for container images before they are pushed. Don't rely on a previously-working Dockerfile in different circumstances. Always explicitly set permissions for binaries in the Dockerfile , don't rely on filesystem permissions. Make sure you keep permissions across the whole toolchain. Again, we are very sorry we messed up this badly, we will do everything we can to not repeat this mistake.","title":" We messed up"},{"location":"blog/2021/03/19/we-messed-up/#we-broke-your-images","text":"March 19, 2021 Two days ago, on March 17, 2021 around 4:30 PM UTC we pushed a change to our build system that broke the container images we published on the Docker Hub . This change resulted in the following error when running the container: Cannot start service containerssh: OCI runtime create failed: container_linux.go:349: starting container process caused \"exec: \\\"/containerssh\\\": permission denied\": unknown To make matters worse, this did not only affect the most recent image, it broke all container images. The issue was reported an hour later and fixed on around noon UTC on the 18th of March, 2021 . If you are affected by this issue you can pull the fixed ContainerSSH image by pulling it: Docker docker pull containerssh/containerssh:<version> Podman podman pull containerssh/containerssh:<version> Kubernetes Please set the imagePullPolicy in your pod spec to Always or switch to the image containerssh/containerssh:<version>-20200318 The <version> tag in this case should be replaced with your ContainerSSH version (e.g. 0.3.1 ). There is no way around it: we messed up. Pretty badly at that, we potentially broke your production environment without an easy way to roll back to a previous version. This should not happen, not even in a pre-1.0 version, especially not with something as trivial as a permission mistake. These images should have never made it to the Docker Hub, our testing procedures (obviously lacking) should have caught this and we are determined to fix them . We are very sorry for the inconvenience and the potential outage this issue has caused. Please reach out to us If you need to reach out quickly you can tag or DM us in Twitter or tag the @ContainerSSH role on the Debugged.it Discord to notify the core maintainers.","title":"We broke your images \ud83d\ude22"},{"location":"blog/2021/03/19/we-messed-up/#versioning-in-the-future","text":"Going forward the ContainerSSH images will be versioned in two parts: the ContainerSSH version and the image build date. For example, the image tag 0.3.1-20200318 points to the image built from ContainerSSH version 0.3.1 on the 18th of March, 2020. The tag 0.3.1 points to the latest built from ContainerSSH version 0.3.1. The tag 0.3 points to the latest ContainerSSH version in the 0.3 series, and so on. You can find the full list of available tags on the Docker hub . Why are we doing this? This is necessary because container base images (Alpine Linux in our case) get security updates much more frequently than we release ContainerSSH versions.","title":"Versioning in the future"},{"location":"blog/2021/03/19/we-messed-up/#why-did-this-happen-post-mortem","text":"With the upcoming 0.4 release we are drawing closer to creating a first stable version of ContainerSSH (1.0). As part of the effort to secure the container images we publish we are now using Snyk , graciously made available to open source projects for free. As we scanned our container images built late last year we realized that there were multiple vulnerable libraries in them. These vulnerabilities did not affect ContainerSSH, but it highlights the need to update the images we release much more frequently than we release new ContainerSSH versions. It is also unreasonable to ask system administrators to jump through the hoops of switching to a new ContainerSSH version just because there is a new container image available. Therefore, we decided to version the container images separately from ContainerSSH as described above. However, our existing build system ( Goreleaser , an excellent tool) does not have the capability to manage images in this manner. Therefore, we had to come up with a new build system. We were looking at several ones, but none of them could fulfill the need for a cross-platform build system that is easy for developers to run on their potentially non-Linux machines. Therefore, we decided to code the relatively simple process of building a container image in Go . This tool downloads the Linux .tar.gz files from the GitHub releases as specified in the configuration and unpacks them. This is where the critical mistake happened: the unpacking code used os.Create() instead of os.OpenFile() . This resulted in the permissions not being set on the files extracted from the archive. The Dockerfile moved from the Goreleaser build system also did not contain the required chmod instruction. How did this make it into production? It clearly never should have, and that's a hole in our testing procedures. It is not enough for us to test the ContainerSSH code, we should have tested the built images before pushing them, no matter what build tool was used. Mistakes can happen, we need to make sure big ones don't make it to the images you are using. To fix this issue we will institute automated tests in the build tool that test the functionality of ContainerSSH end-to-end by opening a real SSH connection and executing a real command. This will not be a comprehensive test suite since those are covered by lower level tests, but we will test if the SSH connection can be established, the containers are started, and the output of an executed command is as expected. To sum it up, here's what we learned: Create automated end to end tests for container images before they are pushed. Don't rely on a previously-working Dockerfile in different circumstances. Always explicitly set permissions for binaries in the Dockerfile , don't rely on filesystem permissions. Make sure you keep permissions across the whole toolchain. Again, we are very sorry we messed up this badly, we will do everything we can to not repeat this mistake.","title":"Why did this happen (post mortem)"},{"location":"blog/2021/04/01/containerssh-0-4/","text":"ContainerSSH 0.4: Audit & Proxy \u00b6 April 1, 2021 In November last year we were optimistic that we'd be launching 0.4 early 2021 with the new audit logging feature . Now it is finally time: we are very proud to announce the immediate availability of ContainerSSH 0.4: Audit & Proxy . TL;DR \u00b6 What we added: Audit logging Improved logging New SSH proxy backend New Kubernetes backend New Docker backend New security layer The guest agent What we deprecated: The KubeRun backend The DockerRun backend The Listen config option The sessionId parameter in the auth/config webhook What we removed: The pubKeyBase64 field in the auth webhook Moving from GET to POST in webhooks Audit logging \u00b6 The biggest feature of this release is no doubt the audit logging feature. This audit log records everything a user does, on a byte-by-byte basis . That includes things like SFTP uploads, which may slip through your net if you just record typed commands. The audit logging feature can also upload the stored audit logs to an S3-compatible object storage for long-term archival. In the future we plan to release a tool to visually inspect the stored audit logs, and the planned SSH proxy feature will allow you to use it with traditional SSH servers. For details check out the audit logging reference manual . New SSH proxy backend \u00b6 This new backend forwards connections to a second SSH server instead of starting containers. This makes it possible to use ContainerSSH in two roles: As a way to dynamically authenticate SSH users or dynamically route users to SSH backends. As a way to audit SSH connections after decryption. For details check out the logging reference manual . Better logging \u00b6 One of the reasons why this release took so long to complete was the addition of a comprehensive logging interface . We added hundreds of log messages , most of them on the debug level that allow you to trace exactly what ContainerSSH is doing. Each of the log messages contains a unique code you can use to identify what's wrong with your setup. These codes are documented in the codes list . We also added options for log targets: you can now log to a file, a syslog server via /dev/log or UDP, or to the standard output. For details check out the logging reference manual . New Kubernetes backend \u00b6 Since we wanted to support more use cases we added a completely new backend for Kubernetes to replace the now-deprecated KubeRun backend. The KubeRun backend will remain available until the next release. This new backend supports the new agent we announced back in December to support all the SSH features Kubernetes doesn't normally support. For example, the agent will fix a long-standing issue with Kubernetes where the user would not see the shell because it was written to the standard output before ContainerSSH has attached to the pod. In addition, ContainerSSH supports multiple execution models . The original execution from KubeRun would run one pod per session (multiple pods per SSH connection). The new (default) execution mode now creates one pod per SSH connection and uses the kubectl exec functionality to start the programs for the individual sessions. This also paves the way for future development where we will have (semi) persistent pods. For details check the Kubernetes backend reference manual and the KubeRun deprecation notice . New Docker backend \u00b6 Similar to the Kubernetes backend above this release also adds a new backend for Docker and Podman, deprecating the DockerRun backend. The old DockerRun backend will remain available until the next release. As with Kubernetes the new backend supports the new agent we announced back in December to support all the SSH features Kubernetes doesn't normally support, mainly signal delivery. In addition, ContainerSSH supports multiple execution models . The original execution from DockerRun would run one container per session (multiple pods per SSH connection). The new (default) execution mode now creates one container per SSH connection and uses the docker exec functionality to start the programs for the individual sessions. This also paves the way for future development where we will have (semi) persistent containers. For details check the Docker backend reference manual and the DockerRun deprecation notice . Security filters \u00b6 This release also adds a set of security filters that can be used for fine-grained control over what SSH interactions to allow or block. For example, you could limit the user to a set of environment variables, only allow running certain programs, etc. For details check the security reference manual . SSH key format \u00b6 In the previous releases we transmitted the SSH key to the authentication server in the OpenSSH wire format. This was not easy to implement so in this release we switch to the more popular authorized key format, which is transmitted in the publicKey field. For details check the deprecation notice Reference manual \u00b6 Alongside of this release we are also adding a comprehensive reference manual which describes in great detail how to set up and configure ContainerSSH. Future plans \u00b6 This release is a 90% rewrite of the ContainerSSH codebase which splits it into modules . This presents a basis for exciting new features, such as SSH port forwarding, SSH single sign-on via a web interface (OAuth2/OIDC), web client, launching VMs instead of containers, etc. For details on the planned features please check our development dashboard .","title":" ContainerSSH 0.4: Audit & Proxy"},{"location":"blog/2021/04/01/containerssh-0-4/#containerssh-04-audit-proxy","text":"April 1, 2021 In November last year we were optimistic that we'd be launching 0.4 early 2021 with the new audit logging feature . Now it is finally time: we are very proud to announce the immediate availability of ContainerSSH 0.4: Audit & Proxy .","title":"ContainerSSH 0.4: Audit &amp; Proxy"},{"location":"blog/2021/04/01/containerssh-0-4/#tldr","text":"What we added: Audit logging Improved logging New SSH proxy backend New Kubernetes backend New Docker backend New security layer The guest agent What we deprecated: The KubeRun backend The DockerRun backend The Listen config option The sessionId parameter in the auth/config webhook What we removed: The pubKeyBase64 field in the auth webhook Moving from GET to POST in webhooks","title":"TL;DR"},{"location":"blog/2021/04/01/containerssh-0-4/#audit-logging","text":"The biggest feature of this release is no doubt the audit logging feature. This audit log records everything a user does, on a byte-by-byte basis . That includes things like SFTP uploads, which may slip through your net if you just record typed commands. The audit logging feature can also upload the stored audit logs to an S3-compatible object storage for long-term archival. In the future we plan to release a tool to visually inspect the stored audit logs, and the planned SSH proxy feature will allow you to use it with traditional SSH servers. For details check out the audit logging reference manual .","title":"Audit logging"},{"location":"blog/2021/04/01/containerssh-0-4/#new-ssh-proxy-backend","text":"This new backend forwards connections to a second SSH server instead of starting containers. This makes it possible to use ContainerSSH in two roles: As a way to dynamically authenticate SSH users or dynamically route users to SSH backends. As a way to audit SSH connections after decryption. For details check out the logging reference manual .","title":"New SSH proxy backend"},{"location":"blog/2021/04/01/containerssh-0-4/#better-logging","text":"One of the reasons why this release took so long to complete was the addition of a comprehensive logging interface . We added hundreds of log messages , most of them on the debug level that allow you to trace exactly what ContainerSSH is doing. Each of the log messages contains a unique code you can use to identify what's wrong with your setup. These codes are documented in the codes list . We also added options for log targets: you can now log to a file, a syslog server via /dev/log or UDP, or to the standard output. For details check out the logging reference manual .","title":"Better logging"},{"location":"blog/2021/04/01/containerssh-0-4/#new-kubernetes-backend","text":"Since we wanted to support more use cases we added a completely new backend for Kubernetes to replace the now-deprecated KubeRun backend. The KubeRun backend will remain available until the next release. This new backend supports the new agent we announced back in December to support all the SSH features Kubernetes doesn't normally support. For example, the agent will fix a long-standing issue with Kubernetes where the user would not see the shell because it was written to the standard output before ContainerSSH has attached to the pod. In addition, ContainerSSH supports multiple execution models . The original execution from KubeRun would run one pod per session (multiple pods per SSH connection). The new (default) execution mode now creates one pod per SSH connection and uses the kubectl exec functionality to start the programs for the individual sessions. This also paves the way for future development where we will have (semi) persistent pods. For details check the Kubernetes backend reference manual and the KubeRun deprecation notice .","title":"New Kubernetes backend"},{"location":"blog/2021/04/01/containerssh-0-4/#new-docker-backend","text":"Similar to the Kubernetes backend above this release also adds a new backend for Docker and Podman, deprecating the DockerRun backend. The old DockerRun backend will remain available until the next release. As with Kubernetes the new backend supports the new agent we announced back in December to support all the SSH features Kubernetes doesn't normally support, mainly signal delivery. In addition, ContainerSSH supports multiple execution models . The original execution from DockerRun would run one container per session (multiple pods per SSH connection). The new (default) execution mode now creates one container per SSH connection and uses the docker exec functionality to start the programs for the individual sessions. This also paves the way for future development where we will have (semi) persistent containers. For details check the Docker backend reference manual and the DockerRun deprecation notice .","title":"New Docker backend"},{"location":"blog/2021/04/01/containerssh-0-4/#security-filters","text":"This release also adds a set of security filters that can be used for fine-grained control over what SSH interactions to allow or block. For example, you could limit the user to a set of environment variables, only allow running certain programs, etc. For details check the security reference manual .","title":"Security filters"},{"location":"blog/2021/04/01/containerssh-0-4/#ssh-key-format","text":"In the previous releases we transmitted the SSH key to the authentication server in the OpenSSH wire format. This was not easy to implement so in this release we switch to the more popular authorized key format, which is transmitted in the publicKey field. For details check the deprecation notice","title":"SSH key format"},{"location":"blog/2021/04/01/containerssh-0-4/#reference-manual","text":"Alongside of this release we are also adding a comprehensive reference manual which describes in great detail how to set up and configure ContainerSSH.","title":"Reference manual"},{"location":"blog/2021/04/01/containerssh-0-4/#future-plans","text":"This release is a 90% rewrite of the ContainerSSH codebase which splits it into modules . This presents a basis for exciting new features, such as SSH port forwarding, SSH single sign-on via a web interface (OAuth2/OIDC), web client, launching VMs instead of containers, etc. For details on the planned features please check our development dashboard .","title":"Future plans"},{"location":"blog/2021/04/13/devlog-oauth2/","text":"DevLog: SSH authentication via OAuth2 \u00b6 April 13, 2021 Traditionally, SSH supports authentication via a number of methods. Typically, you'd use passwords or SSH keys to log in. However, other methods are also possible: keyboard-interactive can be used to ask the connecting user a series of questions. This can be used for two factor authentication, for example. GSSAPI authentication allows for using Kerberos tokens obtained, for example, by logging into a Windows domain to be used as SSH credentials. OAuth2: Why? \u00b6 When SSH was first invented by by Tatu Yl\u00f6nen in 1995 most terminal windows had the size of 80 by 25 characters. Netscape Navigator was barely a year old and systems like single sign-on weren't even on the horizon. The SSH protocol has, of course, evolved over the years, but even the most recent RFC's that are implemented are over 10 years old. Traditionally, large enterprises (telco providers, etc) relied on strong firewall rules to isolate their administrative (SSH) access from the Internet. In some cases central authentication was implemented, for example by means of authenticating from a central LDAP server, or via configuration management. This was rather the exception and implemented only when required by security standards such as PCI-DSS. As companies moved ouside their traditional network environment into the cloud in recent years access control became more and more of a problem. However, this is not a new problem. SAML was introduced in 2005, but proved to be an unwieldy XML beast, difficult to implement. Recently OpenID Connect (OIDC) became a very popular add-on to OAuth2 to manage single sign-on needs. (This is not to be confused with the traditional OpenID, the two are not related.) Microsoft Active Directory Federation Services offers OIDC support along with SAML 2.0, and Kubernetes also supports OIDC as an authentication method for administrator. What's missing? SSH. The method of accessing the servers running Kubernetes, or traditional workloads. Of course, you can use GSSAPI to provide automatic login capabilities using your Windows domain, but in the age of Bring Your Own Device that's no longer appropriate. OAuth2: How? \u00b6 Ok, so it's not 1995 any more, how can we get SSH to authenticate via a browser-based authentication flow? The key lies in the keyboard-interactive authentication method described in RFC 4256 . This method is supported by almost all SSH clients and gives the SSH server the ability to send the client a list of questions the client needs to answer. It also allows the server to send the client an instruction text. This instruction text can be used to show the client a link. From here it's fairly simple. Option one is the authorization code flow : client logs in to the web browser and must then copy the code back into their console. The SSH server checks their identity and that's it. See this 17 second video. Option two is the device code flow, where the user is sent to a link and must enter a code from the SSH console. Here we don't send a question, we simply poll the auth token endpoint for the code to be entered. See this 25 second video. The latter would also lend itself to displaying a QR code, but OpenSSH, unfortunately, limits the length of the instruction field to 255 characters and doesn't support UTF-8 either. Client support \u00b6 As of writing, we have tested the following clients: OpenSSH limits the instruction field to 255 characters and does not display non-ASCII characters. PuTTY displays the link, but breaks the link after 78 characters. WinSCP displays the instruction text, but the link is not clickable or copyable. The author of WinSCP has sent us a preview build which contains this feature. FileZilla exhibits a similar link breakage as PuTTY, but it also duplicates all & characters as well. This has been fixed in a nightly build . It also does not display the instructions when no questions are sent . This is required for the device flow. Termius does not display the instruction field on mobile at all, and does not make it possible to copy or click the link on desktop. This issue has been forwarded to their dev team. Bitvise does not make it possible to copy or click the link. They are addressing this as a bug. JuiceSSH (Android) does not display the instruction field on mobile at all. When? \u00b6 Soon. We don't have an exact release date, it's uncharted territory and we want to wait at least for the more popular SSH clients to release full support for this feature. We would like to thank everyone who helped this project with ideas, input and testing.","title":" DevLog: SSH authentication via OAuth2"},{"location":"blog/2021/04/13/devlog-oauth2/#devlog-ssh-authentication-via-oauth2","text":"April 13, 2021 Traditionally, SSH supports authentication via a number of methods. Typically, you'd use passwords or SSH keys to log in. However, other methods are also possible: keyboard-interactive can be used to ask the connecting user a series of questions. This can be used for two factor authentication, for example. GSSAPI authentication allows for using Kerberos tokens obtained, for example, by logging into a Windows domain to be used as SSH credentials.","title":"DevLog: SSH authentication via OAuth2"},{"location":"blog/2021/04/13/devlog-oauth2/#oauth2-why","text":"When SSH was first invented by by Tatu Yl\u00f6nen in 1995 most terminal windows had the size of 80 by 25 characters. Netscape Navigator was barely a year old and systems like single sign-on weren't even on the horizon. The SSH protocol has, of course, evolved over the years, but even the most recent RFC's that are implemented are over 10 years old. Traditionally, large enterprises (telco providers, etc) relied on strong firewall rules to isolate their administrative (SSH) access from the Internet. In some cases central authentication was implemented, for example by means of authenticating from a central LDAP server, or via configuration management. This was rather the exception and implemented only when required by security standards such as PCI-DSS. As companies moved ouside their traditional network environment into the cloud in recent years access control became more and more of a problem. However, this is not a new problem. SAML was introduced in 2005, but proved to be an unwieldy XML beast, difficult to implement. Recently OpenID Connect (OIDC) became a very popular add-on to OAuth2 to manage single sign-on needs. (This is not to be confused with the traditional OpenID, the two are not related.) Microsoft Active Directory Federation Services offers OIDC support along with SAML 2.0, and Kubernetes also supports OIDC as an authentication method for administrator. What's missing? SSH. The method of accessing the servers running Kubernetes, or traditional workloads. Of course, you can use GSSAPI to provide automatic login capabilities using your Windows domain, but in the age of Bring Your Own Device that's no longer appropriate.","title":"OAuth2: Why?"},{"location":"blog/2021/04/13/devlog-oauth2/#oauth2-how","text":"Ok, so it's not 1995 any more, how can we get SSH to authenticate via a browser-based authentication flow? The key lies in the keyboard-interactive authentication method described in RFC 4256 . This method is supported by almost all SSH clients and gives the SSH server the ability to send the client a list of questions the client needs to answer. It also allows the server to send the client an instruction text. This instruction text can be used to show the client a link. From here it's fairly simple. Option one is the authorization code flow : client logs in to the web browser and must then copy the code back into their console. The SSH server checks their identity and that's it. See this 17 second video. Option two is the device code flow, where the user is sent to a link and must enter a code from the SSH console. Here we don't send a question, we simply poll the auth token endpoint for the code to be entered. See this 25 second video. The latter would also lend itself to displaying a QR code, but OpenSSH, unfortunately, limits the length of the instruction field to 255 characters and doesn't support UTF-8 either.","title":"OAuth2: How?"},{"location":"blog/2021/04/13/devlog-oauth2/#client-support","text":"As of writing, we have tested the following clients: OpenSSH limits the instruction field to 255 characters and does not display non-ASCII characters. PuTTY displays the link, but breaks the link after 78 characters. WinSCP displays the instruction text, but the link is not clickable or copyable. The author of WinSCP has sent us a preview build which contains this feature. FileZilla exhibits a similar link breakage as PuTTY, but it also duplicates all & characters as well. This has been fixed in a nightly build . It also does not display the instructions when no questions are sent . This is required for the device flow. Termius does not display the instruction field on mobile at all, and does not make it possible to copy or click the link on desktop. This issue has been forwarded to their dev team. Bitvise does not make it possible to copy or click the link. They are addressing this as a bug. JuiceSSH (Android) does not display the instruction field on mobile at all.","title":"Client support"},{"location":"blog/2021/04/13/devlog-oauth2/#when","text":"Soon. We don't have an exact release date, it's uncharted territory and we want to wait at least for the more popular SSH clients to release full support for this feature. We would like to thank everyone who helped this project with ideas, input and testing.","title":"When?"},{"location":"blog/2021/05/26/containerssh-0-4-1/","text":"ContainerSSH 0.4.1: Bugfixing Audit & Proxy \u00b6 May 26, 2021 ContainerSSH 0.4.1 is now available and contains several bugfixes for the previous version. We encourage all users to upgrade. Changes in detail \u00b6 This release fixes 3 bugs that were introduced with the refactor to version 0.4.0. These are: #201: Incorrect JSON serialization/deserialization from the configuration server when using the Docker backend #209: Incorrect YAML deserialization when using the Kubernetes backend #167: Authentication server ignores password and pubkey options Thanks to GitHub users ne-bknn and tomcsi for reporting these issues. Incorrect JSON serialization/deserialization from the configuration server when using the Docker backend \u00b6 When refactoring ContainerSSH for version 0.4.0 we implemented the JSON serialization and deserialization for the Docker backend incorrectly as reported by GitHub user ne-bknn . The returned JSON from the configuration server had this structure: { \"docker\" : { \"execution\" : { \"Launch\" : { } } } } The Launch component is not supposed to be in this structure and should be inlined. The serialization is now fixed and the Launch component is removed. Incorrect YAML deserialization when using the Kubernetes backend \u00b6 Another serialization issue has been reported by GitHub user tomcsi . This issue has been present since version 0.3 where we added Kubernetes support. Kubernetes uses its own YAML serialization and deserialization library based on ghodss/yaml . This library doesn't add separate YAML tags to the configuration structures, but instead uses the JSON tags. This prevented using several Kubernetes configuration options, such as the hostPath volume type: backend : kubernetes kubernetes : pod : spec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory We have now introduced using the Kubernetes YAML decoding library for the Kubernetes and KubeRun backends only to facilitate proper serialization. Authentication server ignores password and pubkey options \u00b6 Another bug we discovered after the release was that the new version did't take into account the password or pubkey options in the authentication section. The authentication server could just reject those authentication methods, but in order to cut down on 404 entries in the logs we added these options. This release restores the aforementioned functionality. Upgrading to the new release \u00b6 If you haven't upgraded to version 0.4.0 yet please see the 0.4.0 announcement for details on what changed from version 0.3. If you have already upgraded to 0.4.0 we recommend testing the new release for you scenario before upgrading and scheduling a brief downtime as you upgrade both the auth-config servers and ContainerSSH itself.","title":" ContainerSSH 0.4.1: Bugfixing Audit & Proxy"},{"location":"blog/2021/05/26/containerssh-0-4-1/#containerssh-041-bugfixing-audit-proxy","text":"May 26, 2021 ContainerSSH 0.4.1 is now available and contains several bugfixes for the previous version. We encourage all users to upgrade.","title":"ContainerSSH 0.4.1: Bugfixing Audit &amp; Proxy"},{"location":"blog/2021/05/26/containerssh-0-4-1/#changes-in-detail","text":"This release fixes 3 bugs that were introduced with the refactor to version 0.4.0. These are: #201: Incorrect JSON serialization/deserialization from the configuration server when using the Docker backend #209: Incorrect YAML deserialization when using the Kubernetes backend #167: Authentication server ignores password and pubkey options Thanks to GitHub users ne-bknn and tomcsi for reporting these issues.","title":"Changes in detail"},{"location":"blog/2021/05/26/containerssh-0-4-1/#incorrect-json-serializationdeserialization-from-the-configuration-server-when-using-the-docker-backend","text":"When refactoring ContainerSSH for version 0.4.0 we implemented the JSON serialization and deserialization for the Docker backend incorrectly as reported by GitHub user ne-bknn . The returned JSON from the configuration server had this structure: { \"docker\" : { \"execution\" : { \"Launch\" : { } } } } The Launch component is not supposed to be in this structure and should be inlined. The serialization is now fixed and the Launch component is removed.","title":"Incorrect JSON serialization/deserialization from the configuration server when using the Docker backend"},{"location":"blog/2021/05/26/containerssh-0-4-1/#incorrect-yaml-deserialization-when-using-the-kubernetes-backend","text":"Another serialization issue has been reported by GitHub user tomcsi . This issue has been present since version 0.3 where we added Kubernetes support. Kubernetes uses its own YAML serialization and deserialization library based on ghodss/yaml . This library doesn't add separate YAML tags to the configuration structures, but instead uses the JSON tags. This prevented using several Kubernetes configuration options, such as the hostPath volume type: backend : kubernetes kubernetes : pod : spec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory We have now introduced using the Kubernetes YAML decoding library for the Kubernetes and KubeRun backends only to facilitate proper serialization.","title":"Incorrect YAML deserialization when using the Kubernetes backend"},{"location":"blog/2021/05/26/containerssh-0-4-1/#authentication-server-ignores-password-and-pubkey-options","text":"Another bug we discovered after the release was that the new version did't take into account the password or pubkey options in the authentication section. The authentication server could just reject those authentication methods, but in order to cut down on 404 entries in the logs we added these options. This release restores the aforementioned functionality.","title":"Authentication server ignores password and pubkey options"},{"location":"blog/2021/05/26/containerssh-0-4-1/#upgrading-to-the-new-release","text":"If you haven't upgraded to version 0.4.0 yet please see the 0.4.0 announcement for details on what changed from version 0.3. If you have already upgraded to 0.4.0 we recommend testing the new release for you scenario before upgrading and scheduling a brief downtime as you upgrade both the auth-config servers and ContainerSSH itself.","title":"Upgrading to the new release"},{"location":"deprecations/","text":"Deprecated and removed ContainerSSH features \u00b6 This page lists all features that have been removed or deprecated. You can click each one to learn more about how to transition from the deprecated or removed feature. Feature Deprecated Removed publicKeyBase64 in auth protocol \u2014 0.4 sessionId field in auth/config protocol 0.4 \u2014 listen option 0.4 \u2014 dockerrun backend 0.4 \u2014 kuberun backend 0.4 \u2014 Moving from GET to POST in webhooks \u2014 0.4","title":"Deprecations"},{"location":"deprecations/#deprecated-and-removed-containerssh-features","text":"This page lists all features that have been removed or deprecated. You can click each one to learn more about how to transition from the deprecated or removed feature. Feature Deprecated Removed publicKeyBase64 in auth protocol \u2014 0.4 sessionId field in auth/config protocol 0.4 \u2014 listen option 0.4 \u2014 dockerrun backend 0.4 \u2014 kuberun backend 0.4 \u2014 Moving from GET to POST in webhooks \u2014 0.4","title":"Deprecated and removed ContainerSSH features"},{"location":"deprecations/authconfigget/","text":"Moving from GET to POST in webhooks \u00b6 In ContainerSSH version 0.3.1 and before the authentication and configuration webhooks used the GET HTTP method for sending webhooks due to a mistake. This has been changed to a POST request in ContainerSSH 0.4. If your authentication or configuration server only supports GET please add support for the POST method.","title":"Moving from GET to POST in webhooks"},{"location":"deprecations/authconfigget/#moving-from-get-to-post-in-webhooks","text":"In ContainerSSH version 0.3.1 and before the authentication and configuration webhooks used the GET HTTP method for sending webhooks due to a mistake. This has been changed to a POST request in ContainerSSH 0.4. If your authentication or configuration server only supports GET please add support for the POST method.","title":"Moving from GET to POST in webhooks"},{"location":"deprecations/dockerrun/","text":"Deprecating the DockerRun backend ( since 0.4 ) \u00b6 In version 0.4 ContainerSSH received a generalized Docker backend and we are deprecating the dockerrun backend from version 0.3.1 and earlier. We are adding this new backend because we are changing several default values to options which could cause security problems if the old configuration was used. Version 0.4 still includes support for the dockerrun backend, but log a warning when used: You are using the dockerrun backend deprecated since ContainerSSH 0.4. This backend will be removed in the future. Please switch to the new docker backend as soon as possible. See https://containerssh.io/deprecations/dockerrun for details. This page explains how to switch to the new backend. Changing the configuration structure \u00b6 The new configuration is structured into 3 components: docker : connection : # These options were on the root level of the dockerrun configuration. host : cacert : cert : key : execution : # These options are moved here from the old dockerrun -> config option. container : # ... host : # ... network : # ... platform : # ... containername : \"\" # Subsystems that can be requested. subsystems : sftp : /usr/lib/openssh/sftp-server # the \"disableCommand\" option has been removed and is configured in the # \"security\" option. # Pick an image pull policy from \"Always\", \"IfNotPresent\" or \"Never\". See below. imagePullPolicy : \"IfNotPresent\" # Execution mode, see below. mode : connection # Idle command for the new \"connection\" mode, see below. idleCommand : - \"/bin/sh\" - \"-c\" - \"sleep infinity & PID=$!; trap \\\"kill $PID\\\" INT TERM; wait\" # Shell command for the new \"connection\" mode, see below. shellCommand : - \"/bin/bash\" # Path to the new ContainerSSH Guest Agent. agentPath : \"/usr/bin/containerssh-agent\" # Disable the ContainerSSH guest agent. disableAgent : true timeouts : # This section replaces the dockerrun -> config -> timeout option. # Timeout for a container to start. containerStart : 60s # Timeout for a container to stop. containerStop : 60s # Timeout for a shell or command to start. commandStart : 60s # Timeout for HTTP calls http : 15s # Timeout for signal requests signal : 60s # Timeout for window change requests window : 60s The new execution modes \u00b6 The new docker backend supports two execution modes: connection or session . The old dockerrun backend worked identical to the session mode, where each command execution within an SSH connection would cause a new container to be started. The new connection mode, on the other hand, starts a container with an idle command from the configuration and then uses the docker exec facility to launch commands. In connection mode the pods are launched with the command specified in docker \u2192 execution \u2192 idleCommand as a command. The purpose of this command is to keep the pod alive and wait for a TERM signal. Any commands (shell, etc.) will be launched similar to how you would use docker exec to run an additional command in the pod. When a shell is requested the docker \u2192 execution \u2192 shellCommand parameter is used. Warning The connection execution mode means that the CMD and ENTRYPOINT settings from the container image or the configuration are ignored. If you are switching from the dockerrun backend and used the CMD as a security measure it is strongly recommended that you configure the idleCommand and shellCommand options properly. The guest agent \u00b6 ContainerSSH 0.4 also includes support for the new ContainerSSH Guest Agent that enables support for various features the Docker API does not provide, such as sending signals to processes. The agent must be included into the guest image in order to work. When the agent is included it can be configured as follows: docker : execution : # Path to the new ContainerSSH Guest Agent. agentPath : \"/usr/bin/containerssh-agent\" # Disable the ContainerSSH guest agent. disableAgent : true Warning The agent is enabled by default, you should explicitly disable it if you want to run an image that doesn't have an integrated agent. Image pull policy \u00b6 The new docker backend also includes an option when to pull images. This option helps with the Docker Hub rate limits and is built to be similar to the Kubernetes option with the same name . Tip Docker has added ContainerSSH as an Open Source Community Application . Pulls to containerssh/containerssh and the default guest image containerssh/containerssh-guest-image are excluded from the rate limits. docker : execution : # Pick an image pull policy from \"Always\", \"IfNotPresent\" or \"Never\". See below. imagePullPolicy : \"IfNotPresent\" The following options are supported: Always Always pulls images. This is the same behavior as the dockerrun backend. IfNotPresent Pull image if it is not locally present, has no image tag, or has the :latest tag. Never Never pulls the image. If the image is not locally present the execution will fail. Removing the disableCommand option \u00b6 The disableCommand option was added to ContainerSSH to prevent connecting users to run a custom application. This filled a similar role to the ForceCommand option in OpenSSH: it prevented connecting users to launch custom commands. However, this command was separately implemented in the kuberun and in the dockerrun backend. This was not maintainable, so it was moved into the security module and can be configured as follows: security : command : mode : disable","title":"Deprecating the DockerRun backend"},{"location":"deprecations/dockerrun/#deprecating-the-dockerrun-backend-since-04","text":"In version 0.4 ContainerSSH received a generalized Docker backend and we are deprecating the dockerrun backend from version 0.3.1 and earlier. We are adding this new backend because we are changing several default values to options which could cause security problems if the old configuration was used. Version 0.4 still includes support for the dockerrun backend, but log a warning when used: You are using the dockerrun backend deprecated since ContainerSSH 0.4. This backend will be removed in the future. Please switch to the new docker backend as soon as possible. See https://containerssh.io/deprecations/dockerrun for details. This page explains how to switch to the new backend.","title":"Deprecating the DockerRun backend (since 0.4)"},{"location":"deprecations/dockerrun/#changing-the-configuration-structure","text":"The new configuration is structured into 3 components: docker : connection : # These options were on the root level of the dockerrun configuration. host : cacert : cert : key : execution : # These options are moved here from the old dockerrun -> config option. container : # ... host : # ... network : # ... platform : # ... containername : \"\" # Subsystems that can be requested. subsystems : sftp : /usr/lib/openssh/sftp-server # the \"disableCommand\" option has been removed and is configured in the # \"security\" option. # Pick an image pull policy from \"Always\", \"IfNotPresent\" or \"Never\". See below. imagePullPolicy : \"IfNotPresent\" # Execution mode, see below. mode : connection # Idle command for the new \"connection\" mode, see below. idleCommand : - \"/bin/sh\" - \"-c\" - \"sleep infinity & PID=$!; trap \\\"kill $PID\\\" INT TERM; wait\" # Shell command for the new \"connection\" mode, see below. shellCommand : - \"/bin/bash\" # Path to the new ContainerSSH Guest Agent. agentPath : \"/usr/bin/containerssh-agent\" # Disable the ContainerSSH guest agent. disableAgent : true timeouts : # This section replaces the dockerrun -> config -> timeout option. # Timeout for a container to start. containerStart : 60s # Timeout for a container to stop. containerStop : 60s # Timeout for a shell or command to start. commandStart : 60s # Timeout for HTTP calls http : 15s # Timeout for signal requests signal : 60s # Timeout for window change requests window : 60s","title":"Changing the configuration structure"},{"location":"deprecations/dockerrun/#the-new-execution-modes","text":"The new docker backend supports two execution modes: connection or session . The old dockerrun backend worked identical to the session mode, where each command execution within an SSH connection would cause a new container to be started. The new connection mode, on the other hand, starts a container with an idle command from the configuration and then uses the docker exec facility to launch commands. In connection mode the pods are launched with the command specified in docker \u2192 execution \u2192 idleCommand as a command. The purpose of this command is to keep the pod alive and wait for a TERM signal. Any commands (shell, etc.) will be launched similar to how you would use docker exec to run an additional command in the pod. When a shell is requested the docker \u2192 execution \u2192 shellCommand parameter is used. Warning The connection execution mode means that the CMD and ENTRYPOINT settings from the container image or the configuration are ignored. If you are switching from the dockerrun backend and used the CMD as a security measure it is strongly recommended that you configure the idleCommand and shellCommand options properly.","title":"The new execution modes"},{"location":"deprecations/dockerrun/#the-guest-agent","text":"ContainerSSH 0.4 also includes support for the new ContainerSSH Guest Agent that enables support for various features the Docker API does not provide, such as sending signals to processes. The agent must be included into the guest image in order to work. When the agent is included it can be configured as follows: docker : execution : # Path to the new ContainerSSH Guest Agent. agentPath : \"/usr/bin/containerssh-agent\" # Disable the ContainerSSH guest agent. disableAgent : true Warning The agent is enabled by default, you should explicitly disable it if you want to run an image that doesn't have an integrated agent.","title":"The guest agent"},{"location":"deprecations/dockerrun/#image-pull-policy","text":"The new docker backend also includes an option when to pull images. This option helps with the Docker Hub rate limits and is built to be similar to the Kubernetes option with the same name . Tip Docker has added ContainerSSH as an Open Source Community Application . Pulls to containerssh/containerssh and the default guest image containerssh/containerssh-guest-image are excluded from the rate limits. docker : execution : # Pick an image pull policy from \"Always\", \"IfNotPresent\" or \"Never\". See below. imagePullPolicy : \"IfNotPresent\" The following options are supported: Always Always pulls images. This is the same behavior as the dockerrun backend. IfNotPresent Pull image if it is not locally present, has no image tag, or has the :latest tag. Never Never pulls the image. If the image is not locally present the execution will fail.","title":"Image pull policy"},{"location":"deprecations/dockerrun/#removing-the-disablecommand-option","text":"The disableCommand option was added to ContainerSSH to prevent connecting users to run a custom application. This filled a similar role to the ForceCommand option in OpenSSH: it prevented connecting users to launch custom commands. However, this command was separately implemented in the kuberun and in the dockerrun backend. This was not maintainable, so it was moved into the security module and can be configured as follows: security : command : mode : disable","title":"Removing the disableCommand option"},{"location":"deprecations/kuberun/","text":"Deprecating the KubeRun backend ( since 0.4 ) \u00b6 In version 0.4 ContainerSSH received a generalized Kubernetes backend and we are deprecating the kuberun backend from version 0.3.1 and earlier. We are adding this new backend because we are changing several default values to options which could cause security problems if the old configuration was used. Version 0.4 still includes support for the kuberun backend, but logs a warning when used: You are using the kuberun backend deprecated since ContainerSSH 0.4. This backend will be removed in the future. Please switch to the new docker backend as soon as possible. See https://containerssh.io/deprecations/kuberun for details. This page explains how to switch to the new backend. Changing the configuration structure \u00b6 The new configuration structure is very similar to the old kuberun structure. The most important change is the relocated and more detailed timeouts section: kubernetes : timeouts : # Timeout for a container to start. podStart : 60s # Timeout for a container to stop. podStop : 60s # Timeout for a shell or command to start. commandStart : 60s # Timeout for HTTP calls http : 15s # Timeout for signal requests signal : 60s # Timeout for window change requests window : 60s This replaces the old kubernetes \u2192 connection \u2192 timeout option. The configuration now also moves the kubernetes \u2192 pod \u2192 namespace option to the new metadata section, which is can now be fully customized with Kubernetes pod metadata. The podSpec option was renamed spec to align with Kubernetes: kubernetes : pod : metadata : namespace : default generateName : myPodNamePrefix- labels : foo : bar # Rename podSpec spec : Please run kubectl explain pod.metadata for the full list of options. The new execution modes \u00b6 The new kubernetes backend supports two execution modes: connection or session . The old kuberun backend worked identical to the session mode, where each command execution within an SSH connection would cause a new container to be started. The new connection mode, on the other hand, starts a container with an idle command from the configuration and then uses the exec facility to launch commands. In connection mode the pods are launched with the command specified in kubernetes \u2192 pod \u2192 idleCommand as a command. The purpose of this command is to keep the pod alive and wait for a TERM signal. Any commands (shell, etc.) will be launched similar to how you would use kubectl exec to run an additional command in the pod. When a shell is requested the kubernetes \u2192 pod \u2192 shellCommand parameter is used. Warning The connection execution mode means that the CMD and ENTRYPOINT settings from the container image or the configuration are ignored. If you are switching from the kuberun backend and used the CMD as a security measure it is strongly recommended that you configure the idleCommand and shellCommand options properly. The guest agent \u00b6 ContainerSSH 0.4 also includes support for the new ContainerSSH Guest Agent that enables several features the Kubernetes API does not support support. For example, the guest agent enables waiting for ContainerSSH to attach to the process in session mode before starting the desired program. It is strongly recommended to enable the guest agent for Kubernetes as the API misses several features required for proper operations. The agent must be included into the guest image in order to work. When the agent is included it can be configured as follows: kubernetes : pod : # Path to the new ContainerSSH Guest Agent. agentPath : \"/usr/bin/containerssh-agent\" # Disable the ContainerSSH guest agent. disableAgent : true Warning The agent is enabled by default, you should explicitly disable it if you want to run an image that doesn't have an integrated agent. Removing the disableCommand option \u00b6 The disableCommand option was added to ContainerSSH to prevent connecting users to run a custom application. This filled a similar role to the ForceCommand option in OpenSSH: it prevented connecting users to launch custom commands. However, this command was separately implemented in the kuberun and in the dockerrun backend. This was not maintainable, so it was moved into the security module and can be configured as follows: security : command : mode : disable Removing the insecure option \u00b6 We are also removing the insecure option from the connection configuration and no longer support connecting a Kubernetes cluster without certificate verification. Using the insecure option represents the worst practices in terms of security. If you are using it, please set up a proper CA infrastructure.","title":"Deprecating the KubeRun backend"},{"location":"deprecations/kuberun/#deprecating-the-kuberun-backend-since-04","text":"In version 0.4 ContainerSSH received a generalized Kubernetes backend and we are deprecating the kuberun backend from version 0.3.1 and earlier. We are adding this new backend because we are changing several default values to options which could cause security problems if the old configuration was used. Version 0.4 still includes support for the kuberun backend, but logs a warning when used: You are using the kuberun backend deprecated since ContainerSSH 0.4. This backend will be removed in the future. Please switch to the new docker backend as soon as possible. See https://containerssh.io/deprecations/kuberun for details. This page explains how to switch to the new backend.","title":"Deprecating the KubeRun backend (since 0.4)"},{"location":"deprecations/kuberun/#changing-the-configuration-structure","text":"The new configuration structure is very similar to the old kuberun structure. The most important change is the relocated and more detailed timeouts section: kubernetes : timeouts : # Timeout for a container to start. podStart : 60s # Timeout for a container to stop. podStop : 60s # Timeout for a shell or command to start. commandStart : 60s # Timeout for HTTP calls http : 15s # Timeout for signal requests signal : 60s # Timeout for window change requests window : 60s This replaces the old kubernetes \u2192 connection \u2192 timeout option. The configuration now also moves the kubernetes \u2192 pod \u2192 namespace option to the new metadata section, which is can now be fully customized with Kubernetes pod metadata. The podSpec option was renamed spec to align with Kubernetes: kubernetes : pod : metadata : namespace : default generateName : myPodNamePrefix- labels : foo : bar # Rename podSpec spec : Please run kubectl explain pod.metadata for the full list of options.","title":"Changing the configuration structure"},{"location":"deprecations/kuberun/#the-new-execution-modes","text":"The new kubernetes backend supports two execution modes: connection or session . The old kuberun backend worked identical to the session mode, where each command execution within an SSH connection would cause a new container to be started. The new connection mode, on the other hand, starts a container with an idle command from the configuration and then uses the exec facility to launch commands. In connection mode the pods are launched with the command specified in kubernetes \u2192 pod \u2192 idleCommand as a command. The purpose of this command is to keep the pod alive and wait for a TERM signal. Any commands (shell, etc.) will be launched similar to how you would use kubectl exec to run an additional command in the pod. When a shell is requested the kubernetes \u2192 pod \u2192 shellCommand parameter is used. Warning The connection execution mode means that the CMD and ENTRYPOINT settings from the container image or the configuration are ignored. If you are switching from the kuberun backend and used the CMD as a security measure it is strongly recommended that you configure the idleCommand and shellCommand options properly.","title":"The new execution modes"},{"location":"deprecations/kuberun/#the-guest-agent","text":"ContainerSSH 0.4 also includes support for the new ContainerSSH Guest Agent that enables several features the Kubernetes API does not support support. For example, the guest agent enables waiting for ContainerSSH to attach to the process in session mode before starting the desired program. It is strongly recommended to enable the guest agent for Kubernetes as the API misses several features required for proper operations. The agent must be included into the guest image in order to work. When the agent is included it can be configured as follows: kubernetes : pod : # Path to the new ContainerSSH Guest Agent. agentPath : \"/usr/bin/containerssh-agent\" # Disable the ContainerSSH guest agent. disableAgent : true Warning The agent is enabled by default, you should explicitly disable it if you want to run an image that doesn't have an integrated agent.","title":"The guest agent"},{"location":"deprecations/kuberun/#removing-the-disablecommand-option","text":"The disableCommand option was added to ContainerSSH to prevent connecting users to run a custom application. This filled a similar role to the ForceCommand option in OpenSSH: it prevented connecting users to launch custom commands. However, this command was separately implemented in the kuberun and in the dockerrun backend. This was not maintainable, so it was moved into the security module and can be configured as follows: security : command : mode : disable","title":"Removing the disableCommand option"},{"location":"deprecations/kuberun/#removing-the-insecure-option","text":"We are also removing the insecure option from the connection configuration and no longer support connecting a Kubernetes cluster without certificate verification. Using the insecure option represents the worst practices in terms of security. If you are using it, please set up a proper CA infrastructure.","title":"Removing the insecure option"},{"location":"deprecations/listen/","text":"Moving the ContainerSSH listen option ( since 0.4 ) \u00b6 In ContainerSSH 0.4 we are introducing a framework to run multiple services within one daemon. In the future we want to add more services like a web-based interface. To make this change happen we will stop treating the SSH service as special , so we are moving the listen option from the configuration root to ssh \u2192 listen : # Deprecated version listen : 0.0.0.0:2222 # New version ssh : listen : 0.0.0.0:2222 If you use the old option you will receive the following log warning: You are using the 'listen' option deprecated in ContainerSSH 0.4. Please use the new 'ssh -> listen' option. See https://containerssh.io/deprecations/listen for details. If you provide both options the new option will take precedence and you will receive the following log message: You are using the 'listen' option deprecated in ContainerSSH 0.4 as well as the new 'ssh -> listen' option. The new option takes precedence. Please see https://containerssh.io/deprecations/listen for details.","title":"Moving the Listen option"},{"location":"deprecations/listen/#moving-the-containerssh-listen-option-since-04","text":"In ContainerSSH 0.4 we are introducing a framework to run multiple services within one daemon. In the future we want to add more services like a web-based interface. To make this change happen we will stop treating the SSH service as special , so we are moving the listen option from the configuration root to ssh \u2192 listen : # Deprecated version listen : 0.0.0.0:2222 # New version ssh : listen : 0.0.0.0:2222 If you use the old option you will receive the following log warning: You are using the 'listen' option deprecated in ContainerSSH 0.4. Please use the new 'ssh -> listen' option. See https://containerssh.io/deprecations/listen for details. If you provide both options the new option will take precedence and you will receive the following log message: You are using the 'listen' option deprecated in ContainerSSH 0.4 as well as the new 'ssh -> listen' option. The new option takes precedence. Please see https://containerssh.io/deprecations/listen for details.","title":"Moving the ContainerSSH listen option (since 0.4)"},{"location":"deprecations/publicKeyBase64/","text":"Deprecating the publicKeyBase64 field in the authentication protocol ( since 0.4 ) Before ContainerSSH version 0.4 sent a field called publicKeyBase64 to the authentication server which contained the SSH key in the binary OpenSSH wire format. However, this was not easy to integrate, so ContainerSSH 0.4 adds a field called publicKey containing the public key in the OpenSSH authorized keys format. The publicKeyBase64 field is now removed because it was never useful. Authentication server implementations should switch to using the publicKey field.","title":"publicKeyBase64"},{"location":"deprecations/sessionId/","text":"Deprecating the sessionId field in the authentication and configuration server protocol ( since 0.4 ) Before ContainerSSH 0.4 both the authentication server protocol and the configuration server protocol contained a field called sessionId which would include a self-generated ID for the session. In ContainerSSH 0.4 we are introducing a new ID called connectionId which uniquely identifies the SSH connection across all ContainerSSH-related platforms (auth/config/audit log/etc). The sessionId field is now deprecated and mirrors the contents of connectionId . The field will be removed in the future and auth / config server implementers should no longer rely on it.","title":"sessionId"},{"location":"development/","text":"Developing ContainerSSH Welcome! And a big thank you for wanting to contribute! This page will explain how to get started with contributing to ContainerSSH. This guide will help you through the basics of getting development up and running. Getting started This quick guide will help you hop into ContainerSSH development on the quick. Read more \u00bb Setting up your development environment This guide walks you though the steps needed to set up a development environment, from Git, through Goland and GPG, to the IDE. Read more \u00bb Dashboard Our development dashboard shows you the libraries, issues, and pull requests relevant to ContainerSSH development. Open Dashboard \u00bb Understanding ContainerSSH This document describes the concepts around SSH and how ContainerSSH is built internally. Read more \u00bb","title":"Overview"},{"location":"development/code-style/","text":"Coding style We don't have a strict coding convention that will force you to write code in a very specific way. Instead, we will try to explain how we think about ensuring quality in this document. Please, feel free to bring your own ideas and discuss on the discussions board . Object-Oriented Programming \u00b6 Wait, what? OOP in Go? Go has a concept called receivers that allow you to pass a context structure to a function. This is very similar to how private and public variables are handled in OOP languages. The main benefit of receivers is that they can be used to implement interfaces . Interfaces, in turn, give us the ability to create a standardized API between components without involving a network. The log library , for example, provides the Logger interface and then also includes an implementation for the logger. However, at no point do we have a hard dependency on the actual implementation of the logger. If in the future the implementation turns out to be insufficient replacing it is easy. We use this pattern extensively to separate the ContainerSSH libraries from each other. We are then using these interfaces to write tests for each library without having to run an end-to-end test for every test. Testing \u00b6 This brings us to the topic of testing. ContainerSSH is a security-relevant software so we want to ensure a reasonable level of quality. In the beginning we had a manual testing protocol, but as features became more extensive it became very hard to test each feature for each release. We also rely on GitHub's Dependabot to update our external dependencies. Without tests we would have a very hard time verifying that the updated third party library did not break something. When it comes to test sizes we prefer having unit- or component-level tests and only have a few end-to-end tests. This is because e2e tests require several Kubernetes clusters and a Docker server so they are quite slow and hard to run in a development environment. We want to make sure that contributors can avoid the frustrating cycle of Commit, Push, Wait for CI, Realize it breaks, Repeat, so running tests quickly is very desirable. End-to-end tests also have the drawback that if they break the bug can be hard to track down. In summary, we prefer having granular tests for each library. This is why we have split the codebase into several libraries on GitHub . Each library has their own tests and own CI setup. When a library needs to interact with a different library we usually implement an interface with a well-described contract. This contract can then be used to write tests against. When it comes to actually writing the tests we follow the Detroit/classicist school of testing . Our tests are put in the separate _test package and test our code from the outside. Structuring your code \u00b6 In the early versions of ContainerSSH we had a rather monolithic application. The core SSH server would perform logging, write metrics, deal with SSH specifics, etc. Writing and maintaining the code became very tedious. It took a a large amount of concentration to find the right parts to implement a change on, and finding bugs often took a slog through layers and layers of code. This is frustrating and hinders productivity. We don't want contributors to spend more time finding the right code piece than implementing the actual change. This requires a short-term sacrifice: better code structure and abstractions. Yes, we know, they are not fun to implement. When we refactored ContainerSSH in version 0.4 the size of the codebase grew by over 50%. However, this change was worth it as it paved the way for adding new features without pain in the future. Our aim is that each library or component should deal with one concern. The auth library should deal with authentication, the sshserver library with SSH, and so on. This goes so far that the integration work between two libraries is often relegated to a separate library. Sticking with the example before, the authintegration library creates a layer for the SSH server and calls the authentication library when user authentication is desired. There is no hard and fast rule what (not) to separate. Creating a prototype as a single library is fine. If it turns out that it is too unwieldy to test or use it can be refactored. Thankfully, we have no quarterly deadlines we need to hit, so a feature is released when it is ready. Third party libraries \u00b6 We group third party dependencies in two categories: primary and utility. Primary dependencies are the ones that are required to fulfil the primary function of a library. For example, the Docker libraries would be a primary dependency for the dockerrun library . These libraries are integrated directly. Needless to say, the libraries include component-level tests to verify the integration still works. This stands in contrast to utility libraries. For example, we use Yuki Iwanaga's defaults library to provide default values for structs in multiple ContainerSSH libraries. However, since the library may need to be replaced in the future we opt to create a wrapping layer called structutils . This wrapping layer describes our expectation towards the library and also includes tests to verify that this functionality still holds true. Dealing with networks \u00b6 ContainerSSH integrates several components that can be reached over the network, for example the config server, the auth server, or even Docker and Kubernetes. While in the development environment everything typically works fine, they can be notoriously unreliable in production. What's worse, these issues are extremely hard to debug, so we aim to prevent them. Our two choices of prevention are contexts and retries. Contexts in Go provide a graceful way to observe timeouts. The simplest way to create a timeout context is the following: ctx , cancelFunc := context . WithTimeout ( context . Background , 60 * time . Second , ) defer cancelFunc () Warning It is very important that you include the call to cancelFunc() otherwise you may leak memory. Now that you have a context you can check it inside a loop: loop : for { select { case <- ctx . Done (): break loop default : //Continue whatever you need to do } } Retries also come into play: when performing a call over the network you may encounter random errors you may wish to retry. We frequently couple the context with retries: var lastError error loop : for { lastError = someNetworkCall () if err == nil { break loop } else { logger . Warningf ( \"failed to perform network call, retrying in 10 seconds (%v)\" , lastError , ) } select { case <- ctx . Done (): break loop case <- time . After ( 10 * time . Second ): // Next loop } } if lastError != nil { logger . Errorf ( \"failed to perform network call, giving up (%v)\" , err ) return lastError } Microserviecs \u00b6 The above-mentioned networks also factor in the concept of microservices . ContainerSSH uses two external services for authentication and configuration . These are provided for user convenience making it easier to integrate ContainerSSH. However, we do not plan to add more microservices for development convenience. We want to avoid having more deployment YAML files than actual code. ContainerSSH should be simple to run, even if that means making it harder to structure the code. Conclusion \u00b6 We hope you now have a better idea of the design goals of ContainerSSH. However, it is worth reiterating: there is room for disagreement. If in doubt, feel free to submit a simple pull request and we'll work from there. If your solution is missing bits we'll work with you or even add missing code pieces to come to an agreeable solution.","title":"Coding Style"},{"location":"development/code-style/#object-oriented-programming","text":"Wait, what? OOP in Go? Go has a concept called receivers that allow you to pass a context structure to a function. This is very similar to how private and public variables are handled in OOP languages. The main benefit of receivers is that they can be used to implement interfaces . Interfaces, in turn, give us the ability to create a standardized API between components without involving a network. The log library , for example, provides the Logger interface and then also includes an implementation for the logger. However, at no point do we have a hard dependency on the actual implementation of the logger. If in the future the implementation turns out to be insufficient replacing it is easy. We use this pattern extensively to separate the ContainerSSH libraries from each other. We are then using these interfaces to write tests for each library without having to run an end-to-end test for every test.","title":"Object-Oriented Programming"},{"location":"development/code-style/#testing","text":"This brings us to the topic of testing. ContainerSSH is a security-relevant software so we want to ensure a reasonable level of quality. In the beginning we had a manual testing protocol, but as features became more extensive it became very hard to test each feature for each release. We also rely on GitHub's Dependabot to update our external dependencies. Without tests we would have a very hard time verifying that the updated third party library did not break something. When it comes to test sizes we prefer having unit- or component-level tests and only have a few end-to-end tests. This is because e2e tests require several Kubernetes clusters and a Docker server so they are quite slow and hard to run in a development environment. We want to make sure that contributors can avoid the frustrating cycle of Commit, Push, Wait for CI, Realize it breaks, Repeat, so running tests quickly is very desirable. End-to-end tests also have the drawback that if they break the bug can be hard to track down. In summary, we prefer having granular tests for each library. This is why we have split the codebase into several libraries on GitHub . Each library has their own tests and own CI setup. When a library needs to interact with a different library we usually implement an interface with a well-described contract. This contract can then be used to write tests against. When it comes to actually writing the tests we follow the Detroit/classicist school of testing . Our tests are put in the separate _test package and test our code from the outside.","title":"Testing"},{"location":"development/code-style/#structuring-your-code","text":"In the early versions of ContainerSSH we had a rather monolithic application. The core SSH server would perform logging, write metrics, deal with SSH specifics, etc. Writing and maintaining the code became very tedious. It took a a large amount of concentration to find the right parts to implement a change on, and finding bugs often took a slog through layers and layers of code. This is frustrating and hinders productivity. We don't want contributors to spend more time finding the right code piece than implementing the actual change. This requires a short-term sacrifice: better code structure and abstractions. Yes, we know, they are not fun to implement. When we refactored ContainerSSH in version 0.4 the size of the codebase grew by over 50%. However, this change was worth it as it paved the way for adding new features without pain in the future. Our aim is that each library or component should deal with one concern. The auth library should deal with authentication, the sshserver library with SSH, and so on. This goes so far that the integration work between two libraries is often relegated to a separate library. Sticking with the example before, the authintegration library creates a layer for the SSH server and calls the authentication library when user authentication is desired. There is no hard and fast rule what (not) to separate. Creating a prototype as a single library is fine. If it turns out that it is too unwieldy to test or use it can be refactored. Thankfully, we have no quarterly deadlines we need to hit, so a feature is released when it is ready.","title":"Structuring your code"},{"location":"development/code-style/#third-party-libraries","text":"We group third party dependencies in two categories: primary and utility. Primary dependencies are the ones that are required to fulfil the primary function of a library. For example, the Docker libraries would be a primary dependency for the dockerrun library . These libraries are integrated directly. Needless to say, the libraries include component-level tests to verify the integration still works. This stands in contrast to utility libraries. For example, we use Yuki Iwanaga's defaults library to provide default values for structs in multiple ContainerSSH libraries. However, since the library may need to be replaced in the future we opt to create a wrapping layer called structutils . This wrapping layer describes our expectation towards the library and also includes tests to verify that this functionality still holds true.","title":"Third party libraries"},{"location":"development/code-style/#dealing-with-networks","text":"ContainerSSH integrates several components that can be reached over the network, for example the config server, the auth server, or even Docker and Kubernetes. While in the development environment everything typically works fine, they can be notoriously unreliable in production. What's worse, these issues are extremely hard to debug, so we aim to prevent them. Our two choices of prevention are contexts and retries. Contexts in Go provide a graceful way to observe timeouts. The simplest way to create a timeout context is the following: ctx , cancelFunc := context . WithTimeout ( context . Background , 60 * time . Second , ) defer cancelFunc () Warning It is very important that you include the call to cancelFunc() otherwise you may leak memory. Now that you have a context you can check it inside a loop: loop : for { select { case <- ctx . Done (): break loop default : //Continue whatever you need to do } } Retries also come into play: when performing a call over the network you may encounter random errors you may wish to retry. We frequently couple the context with retries: var lastError error loop : for { lastError = someNetworkCall () if err == nil { break loop } else { logger . Warningf ( \"failed to perform network call, retrying in 10 seconds (%v)\" , lastError , ) } select { case <- ctx . Done (): break loop case <- time . After ( 10 * time . Second ): // Next loop } } if lastError != nil { logger . Errorf ( \"failed to perform network call, giving up (%v)\" , err ) return lastError }","title":"Dealing with networks"},{"location":"development/code-style/#microserviecs","text":"The above-mentioned networks also factor in the concept of microservices . ContainerSSH uses two external services for authentication and configuration . These are provided for user convenience making it easier to integrate ContainerSSH. However, we do not plan to add more microservices for development convenience. We want to avoid having more deployment YAML files than actual code. ContainerSSH should be simple to run, even if that means making it harder to structure the code.","title":"Microserviecs"},{"location":"development/code-style/#conclusion","text":"We hope you now have a better idea of the design goals of ContainerSSH. However, it is worth reiterating: there is room for disagreement. If in doubt, feel free to submit a simple pull request and we'll work from there. If your solution is missing bits we'll work with you or even add missing code pieces to come to an agreeable solution.","title":"Conclusion"},{"location":"development/dashboard/","text":"Development Dashboard Roadmap 1.0.0: Ready for Production \u00b6 Web client SSH port forwarding SSH agent forwarding In-Kubernetes authentication and configuration Signed package repo for DEB, RPM and APK 0.5.0: SSO \u00b6 Keyboard-interactive authentication Support WebAuthn Pass metadata from the auth webhook to the config server ARM / 32 bit support 0.4.2: Health check \u00b6 Health check endpoint Future \u00b6 Stopping ContainerSSH does not remove containers Antivirus Backend for file transfers (scp/sftp) PAM authentication Ideas \u00b6 File manager Support Gitlab CI runners Direct shell backend Kerberos/GSS-API authentication Add support for UDP forwarding Launch VMs Support ECS exec Repositories Repository Description Version ContainerSSH ContainerSSH: Launch containers on demand v0.4.1 MiniContainerSSH A learning-focussed, simplified implementation of ContainerSSH branding ContainerSSH icons, logos containerssh.github.io The ContainerSSH website auditlog Audit logger for ContainerSSH v1.0.0 log Common logging interface for ContainerSSH modules v1.1.6 sshserver The SSH server and decoding library used by ContainerSSH v2.0.0-alpha.1 library-template Template for library repositories github-terraform Terraform repository for managing this GitHub organization auditlogintegration Auditlog integration for the SSH server library v1.0.0 auth ContainerSSH authentication library v1.0.1 http Common HTTP library for ContainerSSH v1.2.0 authintegration SSH server integration of the auth library v1.0.0 configuration Common configuration library for ContainerSSH v2.1.0 geoip The GeoIP lookup library for ContainerSSH v1.0.0 metrics Metrics collection and server library for ContainerSSH v1.0.0 dockerrun The legacy Docker backend for ContainerSSH v0.9.2 backend Container backend abstraction library for ContainerSSH v2.0.2 kuberun The legacy Kubernetes backend for ContainerSSH v0.9.3 service Service layer for ContainerSSH v1.0.0 structutils Utility wrapper for structs for ContainerSSH v1.1.0 unixutils Utilities related to UNIX systems v1.0.0 AuthConfig The Authentication and Configuration Server for ContainerSSH guest-image The source code of the default ContainerSSH guest image kubernetes The Kubernetes backend for ContainerSSH v2.0.1 docker The Docker backend for ContainerSSH v2.0.1 agent The guest agent for ContainerSSH v0.9.4 security The security layer for ContainerSSH v1.0.0 metricsintegration SSH integration for metrics collection for ContainerSSH v1.0.0 packages The ContainerSSH packages page examples ContainerSSH examples images The ContainerSSH container images 20210526 sshproxy SSH proxy backend for ContainerSSH v1.0.0 client-testbed Miniature testbed application for SSH clients health Healthz server for ContainerSSH v1.1.0 Issues Repository Title Milestone Created ContainerSSH Stopping ContainerSSH does not remove containers Future 387 days ago ContainerSSH Web client 1.0.0: Ready for Production 246 days ago ContainerSSH Keyboard-interactive authentication 0.5.0: SSO 246 days ago ContainerSSH SSH port forwarding 1.0.0: Ready for Production 246 days ago ContainerSSH SSH agent forwarding 1.0.0: Ready for Production 246 days ago ContainerSSH File manager Ideas 246 days ago ContainerSSH Support Gitlab CI runners Ideas 245 days ago ContainerSSH PAM authentication Future 222 days ago ContainerSSH Direct shell backend Ideas 221 days ago ContainerSSH Kerberos/GSS-API authentication Ideas 207 days ago ContainerSSH Support WebAuthn 0.5.0: SSO 150 days ago ContainerSSH Antivirus Backend for file transfers (scp/sftp) Future 143 days ago ContainerSSH Pass metadata from the auth webhook to the config server 0.5.0: SSO 129 days ago ContainerSSH Add support for UDP forwarding Ideas 119 days ago ContainerSSH In-Kubernetes authentication and configuration 1.0.0: Ready for Production 115 days ago ContainerSSH Launch VMs Ideas 112 days ago ContainerSSH x509-based authentication 110 days ago ContainerSSH Support ECS exec Ideas 109 days ago ContainerSSH Signed package repo for DEB, RPM and APK 1.0.0: Ready for Production 100 days ago ContainerSSH Health check endpoint 0.4.2: Health check 92 days ago ContainerSSH ARM / 32 bit support 0.5.0: SSO 92 days ago ContainerSSH ContainerSSH will not load the config.yaml file if it is not specified as a parameter. 51 days ago containerssh.github.io Left side menu doesn't increase page size 112 days ago github-terraform Automate label creation 202 days ago guest-image Implement Docker Trust 202 days ago Pull Requests Repository Title Created Mergeable Checks ContainerSSH OAuth2 69 days ago \u2705 \u274c ContainerSSH [WIP] Fix issue #203 51 days ago \u2705 \u274c containerssh.github.io 0.5.0 reference manual 1 day ago \u2705 \u2705 sshserver OAuth authentication 1 day ago \u2705 \u274c auditlogintegration OAuth 69 days ago \u2705 \u2705 auth OAuth2 authentication 70 days ago \u2705 \u274c http Added additional linting 65 days ago \u2705 \u2705 authintegration OAuth authentication 1 day ago \u2705 \u274c configuration Oauth 1 day ago \u2705 \u274c backend OAuth 1 day ago \u2705 \u274c kubernetes OAuth 1 day ago \u2705 \u274c docker OAuth authentication 1 day ago \u2705 \u274c security OAuth 1 day ago \u2705 \u274c Dependency updates Repository Title Created Mergeable Checks ContainerSSH Bump github.com/go-enry/go-license-detector/v4 from 4.1.0 to 4.2.0 71 days ago \u2705 \u274c ContainerSSH Bump github.com/docker/docker from 20.10.6+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u274c ContainerSSH Bump github.com/aws/aws-sdk-go from 1.38.47 to 1.38.71 0 days ago \u2705 \u274c auditlog Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c auditlog Bump github.com/docker/docker from 20.10.5+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u2705 auditlog Bump github.com/aws/aws-sdk-go from 1.38.10 to 1.38.71 1 day ago \u2705 \u2705 log Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 sshserver Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c sshserver Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c auditlogintegration Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c auth Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c auth Bump github.com/containerssh/http from 1.0.0 to 1.2.0 23 days ago \u2705 \u274c http Bump github.com/containerssh/log from 1.1.3 to 1.1.6 43 days ago \u2705 \u2705 http Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 authintegration Bump github.com/containerssh/auth from 1.0.0 to 1.0.1 85 days ago \u2705 \u2705 authintegration Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c authintegration Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c authintegration Bump github.com/containerssh/http from 1.0.0 to 1.2.0 23 days ago \u2705 \u274c configuration Bump github.com/google/go-cmp from 0.5.5 to 0.5.6 39 days ago \u2705 \u2705 configuration Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 configuration Bump github.com/docker/docker from 20.10.6+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u2705 configuration Bump github.com/containerssh/health from 1.0.1 to 1.1.0 23 days ago \u2705 \u2705 configuration Bump github.com/containerssh/http from 1.1.0 to 1.2.0 23 days ago \u2705 \u2705 metrics Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c metrics Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c metrics Bump github.com/containerssh/http from 1.0.0 to 1.2.0 23 days ago \u2705 \u274c backend Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c backend Bump github.com/containerssh/configuration/v2 from 2.0.1 to 2.1.0 25 days ago \u2705 \u274c service Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c unixutils Bump github.com/mattn/go-shellwords from 1.0.11 to 1.0.12 26 days ago \u2705 \u274c guest-image Bump docker/login-action from 1 to 1.9.0 52 days ago \u2705 \u2705 guest-image Bump actions/checkout from 2 to 2.3.4 52 days ago \u2705 \u2705 guest-image Bump docker/build-push-action from 2 to 2.4.0 52 days ago \u2705 \u2705 guest-image Bump docker/setup-qemu-action from 1 to 1.1.0 52 days ago \u2705 \u2705 guest-image Bump docker/setup-buildx-action from 1 to 1.3.0 52 days ago \u2705 \u2705 kubernetes Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 kubernetes Bump k8s.io/client-go from 0.21.1 to 0.21.2 15 days ago \u2705 \u2705 kubernetes Bump k8s.io/api from 0.21.1 to 0.21.2 15 days ago \u2705 \u2705 kubernetes Bump k8s.io/apimachinery from 0.21.1 to 0.21.2 15 days ago \u2705 \u2705 docker Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 docker Bump github.com/google/go-cmp from 0.5.5 to 0.5.6 39 days ago \u2705 \u2705 docker Bump github.com/docker/docker from 20.10.6+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u2705 security Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c sshproxy Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c sshproxy Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c sshproxy Bump github.com/docker/docker from 20.10.5+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u274c","title":"Dashboard"},{"location":"development/dashboard/#100-ready-for-production","text":"Web client SSH port forwarding SSH agent forwarding In-Kubernetes authentication and configuration Signed package repo for DEB, RPM and APK","title":"1.0.0: Ready for Production"},{"location":"development/dashboard/#050-sso","text":"Keyboard-interactive authentication Support WebAuthn Pass metadata from the auth webhook to the config server ARM / 32 bit support","title":"0.5.0: SSO"},{"location":"development/dashboard/#042-health-check","text":"Health check endpoint","title":"0.4.2: Health check"},{"location":"development/dashboard/#future","text":"Stopping ContainerSSH does not remove containers Antivirus Backend for file transfers (scp/sftp) PAM authentication","title":"Future"},{"location":"development/dashboard/#ideas","text":"File manager Support Gitlab CI runners Direct shell backend Kerberos/GSS-API authentication Add support for UDP forwarding Launch VMs Support ECS exec Repositories Repository Description Version ContainerSSH ContainerSSH: Launch containers on demand v0.4.1 MiniContainerSSH A learning-focussed, simplified implementation of ContainerSSH branding ContainerSSH icons, logos containerssh.github.io The ContainerSSH website auditlog Audit logger for ContainerSSH v1.0.0 log Common logging interface for ContainerSSH modules v1.1.6 sshserver The SSH server and decoding library used by ContainerSSH v2.0.0-alpha.1 library-template Template for library repositories github-terraform Terraform repository for managing this GitHub organization auditlogintegration Auditlog integration for the SSH server library v1.0.0 auth ContainerSSH authentication library v1.0.1 http Common HTTP library for ContainerSSH v1.2.0 authintegration SSH server integration of the auth library v1.0.0 configuration Common configuration library for ContainerSSH v2.1.0 geoip The GeoIP lookup library for ContainerSSH v1.0.0 metrics Metrics collection and server library for ContainerSSH v1.0.0 dockerrun The legacy Docker backend for ContainerSSH v0.9.2 backend Container backend abstraction library for ContainerSSH v2.0.2 kuberun The legacy Kubernetes backend for ContainerSSH v0.9.3 service Service layer for ContainerSSH v1.0.0 structutils Utility wrapper for structs for ContainerSSH v1.1.0 unixutils Utilities related to UNIX systems v1.0.0 AuthConfig The Authentication and Configuration Server for ContainerSSH guest-image The source code of the default ContainerSSH guest image kubernetes The Kubernetes backend for ContainerSSH v2.0.1 docker The Docker backend for ContainerSSH v2.0.1 agent The guest agent for ContainerSSH v0.9.4 security The security layer for ContainerSSH v1.0.0 metricsintegration SSH integration for metrics collection for ContainerSSH v1.0.0 packages The ContainerSSH packages page examples ContainerSSH examples images The ContainerSSH container images 20210526 sshproxy SSH proxy backend for ContainerSSH v1.0.0 client-testbed Miniature testbed application for SSH clients health Healthz server for ContainerSSH v1.1.0 Issues Repository Title Milestone Created ContainerSSH Stopping ContainerSSH does not remove containers Future 387 days ago ContainerSSH Web client 1.0.0: Ready for Production 246 days ago ContainerSSH Keyboard-interactive authentication 0.5.0: SSO 246 days ago ContainerSSH SSH port forwarding 1.0.0: Ready for Production 246 days ago ContainerSSH SSH agent forwarding 1.0.0: Ready for Production 246 days ago ContainerSSH File manager Ideas 246 days ago ContainerSSH Support Gitlab CI runners Ideas 245 days ago ContainerSSH PAM authentication Future 222 days ago ContainerSSH Direct shell backend Ideas 221 days ago ContainerSSH Kerberos/GSS-API authentication Ideas 207 days ago ContainerSSH Support WebAuthn 0.5.0: SSO 150 days ago ContainerSSH Antivirus Backend for file transfers (scp/sftp) Future 143 days ago ContainerSSH Pass metadata from the auth webhook to the config server 0.5.0: SSO 129 days ago ContainerSSH Add support for UDP forwarding Ideas 119 days ago ContainerSSH In-Kubernetes authentication and configuration 1.0.0: Ready for Production 115 days ago ContainerSSH Launch VMs Ideas 112 days ago ContainerSSH x509-based authentication 110 days ago ContainerSSH Support ECS exec Ideas 109 days ago ContainerSSH Signed package repo for DEB, RPM and APK 1.0.0: Ready for Production 100 days ago ContainerSSH Health check endpoint 0.4.2: Health check 92 days ago ContainerSSH ARM / 32 bit support 0.5.0: SSO 92 days ago ContainerSSH ContainerSSH will not load the config.yaml file if it is not specified as a parameter. 51 days ago containerssh.github.io Left side menu doesn't increase page size 112 days ago github-terraform Automate label creation 202 days ago guest-image Implement Docker Trust 202 days ago Pull Requests Repository Title Created Mergeable Checks ContainerSSH OAuth2 69 days ago \u2705 \u274c ContainerSSH [WIP] Fix issue #203 51 days ago \u2705 \u274c containerssh.github.io 0.5.0 reference manual 1 day ago \u2705 \u2705 sshserver OAuth authentication 1 day ago \u2705 \u274c auditlogintegration OAuth 69 days ago \u2705 \u2705 auth OAuth2 authentication 70 days ago \u2705 \u274c http Added additional linting 65 days ago \u2705 \u2705 authintegration OAuth authentication 1 day ago \u2705 \u274c configuration Oauth 1 day ago \u2705 \u274c backend OAuth 1 day ago \u2705 \u274c kubernetes OAuth 1 day ago \u2705 \u274c docker OAuth authentication 1 day ago \u2705 \u274c security OAuth 1 day ago \u2705 \u274c Dependency updates Repository Title Created Mergeable Checks ContainerSSH Bump github.com/go-enry/go-license-detector/v4 from 4.1.0 to 4.2.0 71 days ago \u2705 \u274c ContainerSSH Bump github.com/docker/docker from 20.10.6+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u274c ContainerSSH Bump github.com/aws/aws-sdk-go from 1.38.47 to 1.38.71 0 days ago \u2705 \u274c auditlog Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c auditlog Bump github.com/docker/docker from 20.10.5+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u2705 auditlog Bump github.com/aws/aws-sdk-go from 1.38.10 to 1.38.71 1 day ago \u2705 \u2705 log Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 sshserver Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c sshserver Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c auditlogintegration Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c auth Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c auth Bump github.com/containerssh/http from 1.0.0 to 1.2.0 23 days ago \u2705 \u274c http Bump github.com/containerssh/log from 1.1.3 to 1.1.6 43 days ago \u2705 \u2705 http Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 authintegration Bump github.com/containerssh/auth from 1.0.0 to 1.0.1 85 days ago \u2705 \u2705 authintegration Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c authintegration Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c authintegration Bump github.com/containerssh/http from 1.0.0 to 1.2.0 23 days ago \u2705 \u274c configuration Bump github.com/google/go-cmp from 0.5.5 to 0.5.6 39 days ago \u2705 \u2705 configuration Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 configuration Bump github.com/docker/docker from 20.10.6+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u2705 configuration Bump github.com/containerssh/health from 1.0.1 to 1.1.0 23 days ago \u2705 \u2705 configuration Bump github.com/containerssh/http from 1.1.0 to 1.2.0 23 days ago \u2705 \u2705 metrics Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c metrics Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c metrics Bump github.com/containerssh/http from 1.0.0 to 1.2.0 23 days ago \u2705 \u274c backend Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c backend Bump github.com/containerssh/configuration/v2 from 2.0.1 to 2.1.0 25 days ago \u2705 \u274c service Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c unixutils Bump github.com/mattn/go-shellwords from 1.0.11 to 1.0.12 26 days ago \u2705 \u274c guest-image Bump docker/login-action from 1 to 1.9.0 52 days ago \u2705 \u2705 guest-image Bump actions/checkout from 2 to 2.3.4 52 days ago \u2705 \u2705 guest-image Bump docker/build-push-action from 2 to 2.4.0 52 days ago \u2705 \u2705 guest-image Bump docker/setup-qemu-action from 1 to 1.1.0 52 days ago \u2705 \u2705 guest-image Bump docker/setup-buildx-action from 1 to 1.3.0 52 days ago \u2705 \u2705 kubernetes Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 kubernetes Bump k8s.io/client-go from 0.21.1 to 0.21.2 15 days ago \u2705 \u2705 kubernetes Bump k8s.io/api from 0.21.1 to 0.21.2 15 days ago \u2705 \u2705 kubernetes Bump k8s.io/apimachinery from 0.21.1 to 0.21.2 15 days ago \u2705 \u2705 docker Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u2705 docker Bump github.com/google/go-cmp from 0.5.5 to 0.5.6 39 days ago \u2705 \u2705 docker Bump github.com/docker/docker from 20.10.6+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u2705 security Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c sshproxy Bump github.com/containerssh/log from 1.0.0 to 1.1.6 43 days ago \u2705 \u274c sshproxy Bump github.com/containerssh/structutils from 1.0.0 to 1.1.0 39 days ago \u2705 \u274c sshproxy Bump github.com/docker/docker from 20.10.5+incompatible to 20.10.7+incompatible 30 days ago \u2705 \u274c","title":"Ideas"},{"location":"development/getting-started/","text":"Getting started with ContainerSSH development Welcome to developing ContainerSSH! For the purposes of this guide we will assume you have your development environment set up and ready to go . If not, please follow our handy guide to do just that . Ready? Good. Cloning the repository \u00b6 Before we begin you will have to decide what you want to do. If you just want to get ContainerSSH running to get the big picture you will need to clone the ContainerSSH/ContainerSSH repository . This contains the main ContainerSSH executable, as well as the Auth-Config server used for testing: git clone https://github.com/containerssh/containerssh However, ContainerSSH is built in a highly modular fashion so you may need to change a specific library. You can find the list of libraries on our development dashboard . This dashboard contains an overview of all repositories, issues, pull requests, and everything else you will need to find your way around the codebase. Each repository contains a readme explaining how to use that specific component. If you find the readme not helpful please open an issue on that repository asking for more information. If you find yourself needing a new repository because you want to develop something completely new please file a pull request against the github-terraform repository. Tip For the best results we recommend cloning the ContainerSSH repos into /path/to/your/home/go/src/github.com/containerssh/REPONAME . Running ContainerSSH \u00b6 Running ContainerSSH is simple. You will need a clone of the main ContainerSSH repository. Then you have to run two commands. First, the auth-config server needs to be run from the cmd/containerssh-testauthconfigserver directory: go run . When that's running create the cmd/containerssh/config.yaml file with the following content: --- log : level : debug ssh : hostkeys : - ssh_host_rsa_key backend : dockerrun auth : url : \"http://127.0.0.1:8080\" pubkey : false configserver : url : \"http://127.0.0.1:8080/config\" Now copy the ssh_host_rsa_key file from the example folder and then run ContainerSSH from the cmd/containerssh folder: go run . --config config.yaml That's it! Now you have a running ContainerSSH you can connect to on port 2222: ssh foo@localhost -p 2222 Running the tests \u00b6 There are two types of tests for ContainerSSH: end to end tests and component-level tests. Both can be run using the following command from each library's main folder: go test ./... Tip Some tests require a working Docker or Kubernetes backend. Make sure that your Docker socket is running on your platform default and your Kubernetes configuration is available in the .kube/config file in your home directory as the tests will use these to connect to. Submitting a pull requests \u00b6 Once you are done with your development you should fork the repository on GitHub and create a pull request . This pull request will automatically be tested by the CI system. Feel free to keep working on your PR until you are happy with it. Understanding ContainerSSH \u00b6 ContainerSSH is a reasonably complex piece of software. It uses the built-in Go SSH library to create a server and the client libraries for Docker and Kubernetes to forward the data from the SSH channel to the standard input and output of the container. We have dedicated a whole section to understanding how SSH and ContainerSSH in particular work.","title":"Getting Started"},{"location":"development/getting-started/#cloning-the-repository","text":"Before we begin you will have to decide what you want to do. If you just want to get ContainerSSH running to get the big picture you will need to clone the ContainerSSH/ContainerSSH repository . This contains the main ContainerSSH executable, as well as the Auth-Config server used for testing: git clone https://github.com/containerssh/containerssh However, ContainerSSH is built in a highly modular fashion so you may need to change a specific library. You can find the list of libraries on our development dashboard . This dashboard contains an overview of all repositories, issues, pull requests, and everything else you will need to find your way around the codebase. Each repository contains a readme explaining how to use that specific component. If you find the readme not helpful please open an issue on that repository asking for more information. If you find yourself needing a new repository because you want to develop something completely new please file a pull request against the github-terraform repository. Tip For the best results we recommend cloning the ContainerSSH repos into /path/to/your/home/go/src/github.com/containerssh/REPONAME .","title":"Cloning the repository"},{"location":"development/getting-started/#running-containerssh","text":"Running ContainerSSH is simple. You will need a clone of the main ContainerSSH repository. Then you have to run two commands. First, the auth-config server needs to be run from the cmd/containerssh-testauthconfigserver directory: go run . When that's running create the cmd/containerssh/config.yaml file with the following content: --- log : level : debug ssh : hostkeys : - ssh_host_rsa_key backend : dockerrun auth : url : \"http://127.0.0.1:8080\" pubkey : false configserver : url : \"http://127.0.0.1:8080/config\" Now copy the ssh_host_rsa_key file from the example folder and then run ContainerSSH from the cmd/containerssh folder: go run . --config config.yaml That's it! Now you have a running ContainerSSH you can connect to on port 2222: ssh foo@localhost -p 2222","title":"Running ContainerSSH"},{"location":"development/getting-started/#running-the-tests","text":"There are two types of tests for ContainerSSH: end to end tests and component-level tests. Both can be run using the following command from each library's main folder: go test ./... Tip Some tests require a working Docker or Kubernetes backend. Make sure that your Docker socket is running on your platform default and your Kubernetes configuration is available in the .kube/config file in your home directory as the tests will use these to connect to.","title":"Running the tests"},{"location":"development/getting-started/#submitting-a-pull-requests","text":"Once you are done with your development you should fork the repository on GitHub and create a pull request . This pull request will automatically be tested by the CI system. Feel free to keep working on your PR until you are happy with it.","title":"Submitting a pull requests"},{"location":"development/getting-started/#understanding-containerssh","text":"ContainerSSH is a reasonably complex piece of software. It uses the built-in Go SSH library to create a server and the client libraries for Docker and Kubernetes to forward the data from the SSH channel to the standard input and output of the container. We have dedicated a whole section to understanding how SSH and ContainerSSH in particular work.","title":"Understanding ContainerSSH"},{"location":"development/containerssh/","text":"Understanding ContainerSSH ContainerSSH is an SSH server that talks to external APIs such as Docker or Kubernetes. This section will explain how ContainerSSH is built. Understanding SSH \u00b6 We don't really think about SSH all that much. Open PuTTY, or your terminal, SSH into a server, and merrily type commands issued to a server running a distance away. Except if you need to write an SSH server. This section will discuss the concepts you need to work on ContainerSSH. Read more \u00bb Your first SSH server \u00b6 ContainerSSH may be complex, so let's start simple: let's implement a very simple SSH server in Go that talks to the Docker backend. Read more \u00bb Internal Architecture \u00b6 ContainerSSH is a project of several thousand lines of code so overview is critical. Our internal architecture document describes what the moving parts of ContainerSSH are. Read more \u00bb","title":"Overview"},{"location":"development/containerssh/#understanding-ssh","text":"We don't really think about SSH all that much. Open PuTTY, or your terminal, SSH into a server, and merrily type commands issued to a server running a distance away. Except if you need to write an SSH server. This section will discuss the concepts you need to work on ContainerSSH. Read more \u00bb","title":"Understanding SSH"},{"location":"development/containerssh/#your-first-ssh-server","text":"ContainerSSH may be complex, so let's start simple: let's implement a very simple SSH server in Go that talks to the Docker backend. Read more \u00bb","title":"Your first SSH server"},{"location":"development/containerssh/#internal-architecture","text":"ContainerSSH is a project of several thousand lines of code so overview is critical. Our internal architecture document describes what the moving parts of ContainerSSH are. Read more \u00bb","title":"Internal Architecture"},{"location":"development/containerssh/first-ssh-server/","text":"Implementing your first SSH server This section will guide you through implementing your first SSH server in go and combine it with Docker. Tip If you are new to SSH development please read our Understanding SSH guide first. Tip The source code for this mini project is available on GitHub . Step 1: The basic loop \u00b6 Let's start off easy: implementing a TCP server. On *NIX systems listen sockets can be started using the listen() system call and Go follows that pattern nicely: listener , err := net . Listen ( \"tcp\" , \"0.0.0.0:2222\" ) However, net.Listen does not accept connections, it merely opens a listen socket telling the system kernel that it should not reject connections coming to the specified port. Now we need to accept any incoming connections. Let's do that: tcpConn , err := listener . Accept () This call will block until a client connects or the listen socket is closed. Let's ignore the second case and focus on the first. With tcpConn we now have an open plain text TCP connection. We can read from it, we can write to it, but until we call listener.Accept() again we won't get any new connections. So let's put it in a loop: for { tcpConn , err := listener . Accept () } Cool, so now we can accept multiple connections! However, these are still just plain text connections, so let's make them into an SSH connection: sshConn , chans , reqs , err := ssh . NewServerConn ( tcpConn , sshConfig ) We won't go into the details of sshConfig here, let's focus on the returned variables instead. The first returned variable, sshConn is the raw SSH connection. If you use an IDE you can use code completion to figure out some useful methods it contains, for example for closing the connection. More interesting to us are the chans and reqs variables, however. The chans variable contains a Go channel containing SSH channel request. When a client wants to open a new channel we can read from this Go channel and process the request. (Confusing, we know, two things with the same name.) The reqs variable is also a Go channel, but it contains global requests. We won't deal with these now, so let's disregard these completely: go ssh . DiscardRequests ( reqs ) As you can see we used the go keyword. This is running the method called in a goroutine . If you are coming from another programming language you can imagine these as multi-threaded coroutines. Suffice it to say, they won't block our main loop. Back to the chans , let's deal with them too. Let's handle them in a method called handleChannels : go handleChannels ( sshConn , chans ) This method will be rather simple: func handleChannels ( conn * ssh . ServerConn , chans <- chan ssh . NewChannel ) { for newChannel := range chans { go handleChannel ( conn , newChannel ) } } For each new channel we open yet another goroutine. Fear not, goroutines are very cheap in Go. Let's deal with that channel: func handleChannel ( conn * ssh . ServerConn , newChannel ssh . NewChannel ) { if t := newChannel . ChannelType (); t != \"session\" { _ = newChannel . Reject ( ssh . UnknownChannelType , fmt . Sprintf ( \"unknown channel type: %s\" , t )) return } channel , requests , err := newChannel . Accept () //... } So far so good, we reject all non-session channels and otherwise accept. The channel contains the reference to the channel, which is also an io.Reader and an io.Writer for stdin and stdout . The requests variable is a go channel containing SSH channel-specific requests. Now, let's use Docker as our backend. It's simple and it's really well documented . On a *NIX system we can create a Docker client like this: docker , err := client . NewClient ( \"unix:///var/run/docker.sock\" , nil , make ( map [ string ] string ), ) Now we can loop over the requests and handle them, one by one: for req := range requests { reply := func ( success bool , message [] byte ) { if req . WantReply { err := req . Reply ( success , message ) if err != nil { closeConnections () } } } handleRequest ( //... ) } As you can see, the requests may need a reply, so we are constructing a simplified function to send a reply back to the SSH client. For the final piece of our puzzle, let's implement the handleRequest method. For simplicity let's implement a switch-case: switch req . Type { case \"env\" : // Save environment variables for later use case \"pty-req\" : // Set the TTY flag on the Docker client to true later case \"window-change\" : // Use the ContainerResize method on the Docker client later case \"shell\" : // Create a container and run it case \"exec\" : // Create a container and run it } That's it! You can find the details on how to run a container in our highly simplified minicontainerssh example . We have skipped many parts like error handling, but it should give you a good overview of how an SSH server in Go works and how it interacts with the container backend. Now you are ready to dive into the internal architecture of ContainerSSH .","title":"Writing your first SSH server"},{"location":"development/containerssh/first-ssh-server/#step-1-the-basic-loop","text":"Let's start off easy: implementing a TCP server. On *NIX systems listen sockets can be started using the listen() system call and Go follows that pattern nicely: listener , err := net . Listen ( \"tcp\" , \"0.0.0.0:2222\" ) However, net.Listen does not accept connections, it merely opens a listen socket telling the system kernel that it should not reject connections coming to the specified port. Now we need to accept any incoming connections. Let's do that: tcpConn , err := listener . Accept () This call will block until a client connects or the listen socket is closed. Let's ignore the second case and focus on the first. With tcpConn we now have an open plain text TCP connection. We can read from it, we can write to it, but until we call listener.Accept() again we won't get any new connections. So let's put it in a loop: for { tcpConn , err := listener . Accept () } Cool, so now we can accept multiple connections! However, these are still just plain text connections, so let's make them into an SSH connection: sshConn , chans , reqs , err := ssh . NewServerConn ( tcpConn , sshConfig ) We won't go into the details of sshConfig here, let's focus on the returned variables instead. The first returned variable, sshConn is the raw SSH connection. If you use an IDE you can use code completion to figure out some useful methods it contains, for example for closing the connection. More interesting to us are the chans and reqs variables, however. The chans variable contains a Go channel containing SSH channel request. When a client wants to open a new channel we can read from this Go channel and process the request. (Confusing, we know, two things with the same name.) The reqs variable is also a Go channel, but it contains global requests. We won't deal with these now, so let's disregard these completely: go ssh . DiscardRequests ( reqs ) As you can see we used the go keyword. This is running the method called in a goroutine . If you are coming from another programming language you can imagine these as multi-threaded coroutines. Suffice it to say, they won't block our main loop. Back to the chans , let's deal with them too. Let's handle them in a method called handleChannels : go handleChannels ( sshConn , chans ) This method will be rather simple: func handleChannels ( conn * ssh . ServerConn , chans <- chan ssh . NewChannel ) { for newChannel := range chans { go handleChannel ( conn , newChannel ) } } For each new channel we open yet another goroutine. Fear not, goroutines are very cheap in Go. Let's deal with that channel: func handleChannel ( conn * ssh . ServerConn , newChannel ssh . NewChannel ) { if t := newChannel . ChannelType (); t != \"session\" { _ = newChannel . Reject ( ssh . UnknownChannelType , fmt . Sprintf ( \"unknown channel type: %s\" , t )) return } channel , requests , err := newChannel . Accept () //... } So far so good, we reject all non-session channels and otherwise accept. The channel contains the reference to the channel, which is also an io.Reader and an io.Writer for stdin and stdout . The requests variable is a go channel containing SSH channel-specific requests. Now, let's use Docker as our backend. It's simple and it's really well documented . On a *NIX system we can create a Docker client like this: docker , err := client . NewClient ( \"unix:///var/run/docker.sock\" , nil , make ( map [ string ] string ), ) Now we can loop over the requests and handle them, one by one: for req := range requests { reply := func ( success bool , message [] byte ) { if req . WantReply { err := req . Reply ( success , message ) if err != nil { closeConnections () } } } handleRequest ( //... ) } As you can see, the requests may need a reply, so we are constructing a simplified function to send a reply back to the SSH client. For the final piece of our puzzle, let's implement the handleRequest method. For simplicity let's implement a switch-case: switch req . Type { case \"env\" : // Save environment variables for later use case \"pty-req\" : // Set the TTY flag on the Docker client to true later case \"window-change\" : // Use the ContainerResize method on the Docker client later case \"shell\" : // Create a container and run it case \"exec\" : // Create a container and run it } That's it! You can find the details on how to run a container in our highly simplified minicontainerssh example . We have skipped many parts like error handling, but it should give you a good overview of how an SSH server in Go works and how it interacts with the container backend. Now you are ready to dive into the internal architecture of ContainerSSH .","title":"Step 1: The basic loop"},{"location":"development/containerssh/internal-architecture/","text":"Internal Architecture ContainerSSH is build as a collection of libraries , each of which is developed independently to ensure quality, but to the purpose of being integrated into what is ContainerSSH. The core architecture consists of several services , such as SSH or the metrics server . These services are started from the core code as part of a service pool. If any one service fails the service pool shuts down. One of the core services is the SSH service , which creates a standardized, object oriented layer to deal with connecting SSH clients. It also abstracts away the complexities of SSH and the Go SSH library. This library defines a set of interfaces that backends need to implement. The SSH backends are then added in layers . One of the most fundamental layers is auditlogintegration , which captures decoded SSH traffic and forwards it to the audit log library . The other critical layer is authintegration , which forwards authentication requests to the authentication library . The final piece of the puzzle is the backend library which acts as a hub. As a first step it calls the configuration library to obtain dynamic, per-user configuration. It then proceeds to load the security layer and the appropriate backend, e.g. Docker or Kubernetes . These backends form the lowermost layer of the SSH handler stack and forward the connections to the container backend. Module dependency map \u00b6 The following graph shows the internal dependencies of ContainerSSH. This is important to know the order in which modules must be updated:","title":"Internal Architecture"},{"location":"development/containerssh/internal-architecture/#module-dependency-map","text":"The following graph shows the internal dependencies of ContainerSSH. This is important to know the order in which modules must be updated:","title":"Module dependency map"},{"location":"development/containerssh/ssh/","text":"Understanding SSH Let's face it: we don't think about SSH all that much. We SSH into a server and merrily type away our commands. Until we need to write an SSH server. This document describes the high level concepts of SSH: how do you open a connection, what are channels, and how do requests work. This is a very high level overview, but should contain everything you need to get started with ContainerSSH development. Handshake \u00b6 When the user connects an SSH server the SSH keys are verified. We won't discuss this here as for ContainerSSH the Go SSH library takes care of that. The first thing we are concerned with is authentication. Authentication is described by RFC 4252 and it states the following: The server drives the authentication by telling the client which authentication methods can be used to continue the exchange at any given time. The client has the freedom to try the methods listed by the server in any order. In other words, when the user connects the SSH server tells the client which authentication method it supports. The client picks one of them and performs the authentication. The server can then decide to reject, allow, or show the client another list of methods (e.g. to perform two factor authentication). The Go library vastly simplifies this process and only allows a single means of authentication for each connection. Each authentication request contains a username. The username may change between authentication attempts to authenticate against different systems, but this is not customary. Connection \u00b6 Once the authentication is complete the connection is open and both the client and the server may now send two types of messages: global requests and channels . Global requests describe requests in either direction that one party wants from the other. For example, the OpenSSH extensions describe the no-more-sessions@openssh.com to indicate that no more session channels should be opened on this connection. The channels, on the other hand are means of transporting data. For example, the session channel is responsible for executing a program and then transporting the standard input, output, and error data streams to and from the program. They also give both ends the ability to send channel-specific requests (e.g. setting environment variables, resizing the window, etc.). Session channels \u00b6 While there are theoretically other types of channels possible, we currently only support session channels. The client can request channels to be opened at any time. We currently support the following requests on the session channel. These are described in RFC 4254 . env Sets an environment variable for the soon to be executed program. pty Requests an interactive terminal for user input. shell Requests the default shell to be executed. exec Requests a specific program to be executed. subsystem Requests a well-known subsystem (e.g. sftp ) to be executed. window-change Informs the server that an interactive terminal window has changed size. This is only sent once the program has been started with the requests above. signal Requests the server to send a signal to the currently running process. In addition, we also send an exit-status request to the client from the server when the program exits to inform the client of the exit code. Interactive terminals \u00b6 As you can see above, the user can request an interactive terminal using the pty request. This is done automatically by SSH clients if they detect that their input is an interactive terminal. Using interactive terminals changes the operation mode of stdin, stdout, and stderr. While programs normally write their standard output to stdout and their error output to stderr , programs running in interactive mode send their combined output to stdout using a special framing. (TTY multiplexing) Thankfully, we don't need to know too much about TTY multiplexing for writing an SSH server since it is transparently passed through from the container engine to the SSH channel and we don't interact with it. RFCs \u00b6 The SSH protocol is governed by the following RFCs: RFC 913: Simple File Transfer Protocol This document describes the SFTP protocol used over SSH. RFC 4250: The Secure Shell (SSH) Protocol Assigned Numbers This document describes the protocol numbers and standard constants used in SSH. RFC 4251: The Secure Shell (SSH) Protocol Architecture This document describes the design decisions taken to work with SSH. RFC 4252: The Secure Shell (SSH) Authentication Protocol This document describes how user authentication works in SSH. RFC 4253: The Secure Shell (SSH) Transport Layer Protocol This document describes the details of how data is transported over SSH. RFC 4254: The Secure Shell (SSH) Connection Protocol This document contains the parts most interesting to us: how channels, sessions, etc. work. RFC 4255: Using DNS to Securely Publish Secure Shell (SSH) Key Fingerprints This document describes how to publish SSH fingerprints using DNS. It has not seen wide adoption. RFC 4256: Generic Message Exchange Authentication for the Secure Shell Protocol (SSH) This document describes the keyboard-interactive authentication for SSH, which is often used for two factor authentication. RFC 4335: The Secure Shell (SSH) Session Channel Break Extension This document describes the telnet-compatible break request for use in SSH. RFC 4344 , RFC 4345 , RFC 4419 , RFC 4432 These documents describe various encryption-related topics. RFC 4462: Generic Security Service Application Program Interface (GSS-API) Authentication and Key Exchange for the Secure Shell (SSH) Protocol This document describes the GSS-API authentication method that can be used to authenticate with a Kerberos ticket. RFC 4716: The Secure Shell (SSH) Public Key File Format This document describes the PEM-like format to store SSH keys in. RFC 4819: Secure Shell Public Key Subsystem This document describes the SSH public key subsystem usable for adding, removing, and listing public keys. RFC 5647 , RFC 5656 , RFC 6187 , RFC 6239 , RFC 6594 , RFC 6668 These documents describe various cryptography and authentication related topics. RFC 7479: Using Ed25519 in SSHFP Resource Records This document describes publishing ED25519 host keys using DNS. RFC 5592: Secure Shell Transport Model for the Simple Network Management Protocol (SNMP) This protocol describes using SNMP over SSH. RFC 6242: Using the NETCONF Protocol over Secure Shell (SSH) This document describes transporting the RFC 6241 Network Configuration Protocol over SSH. This can be used to manage networking equipment. In addition, OpenSSH defines the following extensions: The OpenSSH Protocol This document describes new cryptographic methods, tunnel forwarding, domain socket forwarding, and many more changes. The CertKeys Document This document describes the OpenSSH CA method. SSH Agent Protocol Describes the protocol used by the SSH agent holding the SSH keys in escrow.","title":"Understanding SSH"},{"location":"development/containerssh/ssh/#handshake","text":"When the user connects an SSH server the SSH keys are verified. We won't discuss this here as for ContainerSSH the Go SSH library takes care of that. The first thing we are concerned with is authentication. Authentication is described by RFC 4252 and it states the following: The server drives the authentication by telling the client which authentication methods can be used to continue the exchange at any given time. The client has the freedom to try the methods listed by the server in any order. In other words, when the user connects the SSH server tells the client which authentication method it supports. The client picks one of them and performs the authentication. The server can then decide to reject, allow, or show the client another list of methods (e.g. to perform two factor authentication). The Go library vastly simplifies this process and only allows a single means of authentication for each connection. Each authentication request contains a username. The username may change between authentication attempts to authenticate against different systems, but this is not customary.","title":"Handshake"},{"location":"development/containerssh/ssh/#connection","text":"Once the authentication is complete the connection is open and both the client and the server may now send two types of messages: global requests and channels . Global requests describe requests in either direction that one party wants from the other. For example, the OpenSSH extensions describe the no-more-sessions@openssh.com to indicate that no more session channels should be opened on this connection. The channels, on the other hand are means of transporting data. For example, the session channel is responsible for executing a program and then transporting the standard input, output, and error data streams to and from the program. They also give both ends the ability to send channel-specific requests (e.g. setting environment variables, resizing the window, etc.).","title":"Connection"},{"location":"development/containerssh/ssh/#session-channels","text":"While there are theoretically other types of channels possible, we currently only support session channels. The client can request channels to be opened at any time. We currently support the following requests on the session channel. These are described in RFC 4254 . env Sets an environment variable for the soon to be executed program. pty Requests an interactive terminal for user input. shell Requests the default shell to be executed. exec Requests a specific program to be executed. subsystem Requests a well-known subsystem (e.g. sftp ) to be executed. window-change Informs the server that an interactive terminal window has changed size. This is only sent once the program has been started with the requests above. signal Requests the server to send a signal to the currently running process. In addition, we also send an exit-status request to the client from the server when the program exits to inform the client of the exit code.","title":"Session channels"},{"location":"development/containerssh/ssh/#interactive-terminals","text":"As you can see above, the user can request an interactive terminal using the pty request. This is done automatically by SSH clients if they detect that their input is an interactive terminal. Using interactive terminals changes the operation mode of stdin, stdout, and stderr. While programs normally write their standard output to stdout and their error output to stderr , programs running in interactive mode send their combined output to stdout using a special framing. (TTY multiplexing) Thankfully, we don't need to know too much about TTY multiplexing for writing an SSH server since it is transparently passed through from the container engine to the SSH channel and we don't interact with it.","title":"Interactive terminals"},{"location":"development/containerssh/ssh/#rfcs","text":"The SSH protocol is governed by the following RFCs: RFC 913: Simple File Transfer Protocol This document describes the SFTP protocol used over SSH. RFC 4250: The Secure Shell (SSH) Protocol Assigned Numbers This document describes the protocol numbers and standard constants used in SSH. RFC 4251: The Secure Shell (SSH) Protocol Architecture This document describes the design decisions taken to work with SSH. RFC 4252: The Secure Shell (SSH) Authentication Protocol This document describes how user authentication works in SSH. RFC 4253: The Secure Shell (SSH) Transport Layer Protocol This document describes the details of how data is transported over SSH. RFC 4254: The Secure Shell (SSH) Connection Protocol This document contains the parts most interesting to us: how channels, sessions, etc. work. RFC 4255: Using DNS to Securely Publish Secure Shell (SSH) Key Fingerprints This document describes how to publish SSH fingerprints using DNS. It has not seen wide adoption. RFC 4256: Generic Message Exchange Authentication for the Secure Shell Protocol (SSH) This document describes the keyboard-interactive authentication for SSH, which is often used for two factor authentication. RFC 4335: The Secure Shell (SSH) Session Channel Break Extension This document describes the telnet-compatible break request for use in SSH. RFC 4344 , RFC 4345 , RFC 4419 , RFC 4432 These documents describe various encryption-related topics. RFC 4462: Generic Security Service Application Program Interface (GSS-API) Authentication and Key Exchange for the Secure Shell (SSH) Protocol This document describes the GSS-API authentication method that can be used to authenticate with a Kerberos ticket. RFC 4716: The Secure Shell (SSH) Public Key File Format This document describes the PEM-like format to store SSH keys in. RFC 4819: Secure Shell Public Key Subsystem This document describes the SSH public key subsystem usable for adding, removing, and listing public keys. RFC 5647 , RFC 5656 , RFC 6187 , RFC 6239 , RFC 6594 , RFC 6668 These documents describe various cryptography and authentication related topics. RFC 7479: Using Ed25519 in SSHFP Resource Records This document describes publishing ED25519 host keys using DNS. RFC 5592: Secure Shell Transport Model for the Simple Network Management Protocol (SNMP) This protocol describes using SNMP over SSH. RFC 6242: Using the NETCONF Protocol over Secure Shell (SSH) This document describes transporting the RFC 6241 Network Configuration Protocol over SSH. This can be used to manage networking equipment. In addition, OpenSSH defines the following extensions: The OpenSSH Protocol This document describes new cryptographic methods, tunnel forwarding, domain socket forwarding, and many more changes. The CertKeys Document This document describes the OpenSSH CA method. SSH Agent Protocol Describes the protocol used by the SSH agent holding the SSH keys in escrow.","title":"RFCs"},{"location":"development/devenv/","text":"Setting up your development environment Welcome! This guide will help you set up your development environment for writing ContainerSSH code. We recommend to following this guide step by step, even when you have already set up some of them yourself. Step 1: Create a GitHub account \u00b6 ContainerSSH development is exclusively handled on GitHub. In order to send code or website contributions you will need to create a GitHub account . Once you have an account we also recommend setting up two-factor authentication . Step 2: Installing Git \u00b6 Unless you plan to develop exclusively on the GitHub web interface you will also need to install Git on your computer. We support development on Windows, Linux, and MacOS, feel free to use any of those operating systems. Please follow the GitHub guide to install Git on your operating system. Step 3: Creating a GPG key \u00b6 Git is a distributed versioning system and you can make commits in the name of others. In order to verify committer identity (for both security and licencing purposes) we require all commits to be signed using GPG. Please follow our GPG for Git guide to enable code signing on your machine.. Step 4: Installing Golang \u00b6 To compile the code you will need Golang. We have a guide to install Golang on various platforms. Step 5: Installing the QA tools \u00b6 To make sure there are no latent errors are creeping in we are using some QA tools you will need . Step 6: Installing Docker \u00b6 The dockerrun backend requires Docker to be installed. Please install Docker to develop against. Step 7: Installing Kubernetes \u00b6 The kuberun backend requires Kubernetes to be installed. Please install a lightweight Kubernetes to develop against. Step 8: Setting up your IDE \u00b6 We have a guide to set up VSCode and Goland as your IDE. Step 9: Website \u00b6 This website requires a Python to run locally . This guide explains the details of setting it up.","title":"Overview"},{"location":"development/devenv/#step-1-create-a-github-account","text":"ContainerSSH development is exclusively handled on GitHub. In order to send code or website contributions you will need to create a GitHub account . Once you have an account we also recommend setting up two-factor authentication .","title":"Step 1: Create a GitHub account"},{"location":"development/devenv/#step-2-installing-git","text":"Unless you plan to develop exclusively on the GitHub web interface you will also need to install Git on your computer. We support development on Windows, Linux, and MacOS, feel free to use any of those operating systems. Please follow the GitHub guide to install Git on your operating system.","title":"Step 2: Installing Git"},{"location":"development/devenv/#step-3-creating-a-gpg-key","text":"Git is a distributed versioning system and you can make commits in the name of others. In order to verify committer identity (for both security and licencing purposes) we require all commits to be signed using GPG. Please follow our GPG for Git guide to enable code signing on your machine..","title":"Step 3: Creating a GPG key"},{"location":"development/devenv/#step-4-installing-golang","text":"To compile the code you will need Golang. We have a guide to install Golang on various platforms.","title":"Step 4: Installing Golang"},{"location":"development/devenv/#step-5-installing-the-qa-tools","text":"To make sure there are no latent errors are creeping in we are using some QA tools you will need .","title":"Step 5: Installing the QA tools"},{"location":"development/devenv/#step-6-installing-docker","text":"The dockerrun backend requires Docker to be installed. Please install Docker to develop against.","title":"Step 6: Installing Docker"},{"location":"development/devenv/#step-7-installing-kubernetes","text":"The kuberun backend requires Kubernetes to be installed. Please install a lightweight Kubernetes to develop against.","title":"Step 7: Installing Kubernetes"},{"location":"development/devenv/#step-8-setting-up-your-ide","text":"We have a guide to set up VSCode and Goland as your IDE.","title":"Step 8: Setting up your IDE"},{"location":"development/devenv/#step-9-website","text":"This website requires a Python to run locally . This guide explains the details of setting it up.","title":"Step 9: Website"},{"location":"development/devenv/docker/","text":"Installing Docker The dockerrun backend requires Docker to create containers. You will need this if you test against this backend. Windows / MacOS As a simple way to get Docker running we recommend installing Docker Desktop . Linux Docker provides convenience scripts to install Docker on Linux: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Alternatively, please follow the manual installation steps to get Docker running.","title":"Installing Docker"},{"location":"development/devenv/golang/","text":"Installing Golang While there is an official doc on installing Golang it is less than helpful for Linux users. We are attempting to collect the best practices in installing Golang for beginners here. Tip If you are using Goland as an IDE you can skip this step. Goland downloads the Go compiler for you. Linux / WSL Ubuntu 20.04 On Ubuntu 20.04 you can install Go directly from the package manager: apt update apt install golang-1.14 RHEL/CentOS yum install golang-bin Fedora dnf install golang-bin Gentoo Linux emerge --ask dev-lang/go Other / non-administrator Download the Linux .tar.gz . Extract the archive into a directory. Add the following section to your ~/.profile , ~/.zshrc , ~/.bashrc , or ~/.bash_profile , depending on your shell, then restart your terminal: export PATH = $PATH :/usr/local/bin/go We also recommend adding ~/go/bin directory to your PATH . Windows Install as administrator Golang offers an MSI-based installer for Windows that makes it easy to install Golang on your Windows machine. Follow the installation wizzard and the go command should start working in the terminal. We also recommend adding %USERPROFILE%\\go\\bin to your PATH environment variable to enable running tools from your home directory. Install as user Download the ZIP archive from the archive page Extract the ZIP file to a folder you have access to. Go to Control Panel \u2192 System and Security \u2192 System \u2192 Advanced system settings . Click on Environment variables... Change the PATH environment variable to point to the bin directory inside your Goland directory. We also recommend adding %USERPROFILE%\\go\\bin to your PATH environment variable to enable running tools from your home directory. MacOS Golang offers a PKG installer for MacOS . The go command will be located in /usr/local/go/bin . If the go command doesn't work try restarting the terminal. If it still doesn't work try running the /usr/local/go/bin/go command. If that command works edit the ~/.profile , ~/.zshrc , or ~/.bash_profile files and add the following lines then restart your terminal: export PATH = $PATH :/usr/local/bin/go We also recommend adding ~/go/bin directory to your PATH .","title":"Installing Golang"},{"location":"development/devenv/gpg/","text":"Setting up GPG for code signing ContainerSSH requires all contributors to sign their commits using GPG. GPG authenticates the committer using their GPG key. This serves two purposes: Security. In Git anyone can make commit in the name of anyone. Using GPG commits makes sure we don't accidentally merge a commit pretending to be one of the core contributors. Licensing. As you may notice, we don't have a Contributor License Agreement to make it as simple as possible for people to contribute. By signing your commits we verify that you have indeed made that commit yourself and you presumably understand that this software is open source under the MIT license. It's not 100% legally bullet proof, but it's a good tradeoff preventing contributors from having to read several pages of legalese. Setting up GPG \u00b6 Linux / WSL On Linux or Windows Subsystem for Linux GPG is already included in the package manager. You can install it using the following commands: Ubuntu sudo apt-get update sudo apt-get install gnupg2 RHEL/CentOS yum install gnupg2 Fedora dnf install gnupg2 Gentoo emerge --ask app-crypt/gnupg Tip You may want to install the Kleopatra GUI for easier access. Windows GPG4Win is a full suite for managing GPG keys on Windows. We recommend installing it with the Kleopatra GUI. MacOS Homebrew brew install gnupg2 MacPorts sudo port install gnupg2 GUI GPGTools offers a graphical version of GPG. Creating your GPG key \u00b6 CLI (GPG 2.1.17+) Run the following command: gpg --full-generate-key Select RSA and RSA as the key format. Select 4096 bits for the bit size. When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config , otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option . Kleopatra Select File \u2192 New Key Pair... Select \"Create a personal OpenPGP key pair\" Set your name and the same e-mail address you have on your GitHub account. If you do not wish to publish your e-mail address GitHub gives you a privacy option . Follow the wizard to create your GPG key. GPGTools (MacOS) Please follow the GPGTools guide to create your key . When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config , otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option . CLI (GPG 2.1.16-) Run the following command: gpg --default-new-key-algo rsa4096 --gen-key Select RSA and RSA as the key format. Select 4096 bits for the bit size. When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config , otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option . Adding your key to GitHub \u00b6 CLI First, list your GPG keys with the key IDs: $ gpg --list-secret-keys --keyid-format LONG ------------------------------------------------ sec rsa4096/YOUR-KEY-ID 2020-06-18 [SC] ... Copy the key ID as you will need it for the next steps, then export your public key: gpg --armor --export YOUR-KEY-ID Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface. Kleopatra Right click the key generated in the previous step. Select \"Export...\". Save the file on your machine. Open the file in a text editor. Copy the key. Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface. GPGTools (MacOS) Select the previously generated key. Click the \"Export\" icon in the toolbar. Click Save. Open the file in a text editor. Copy the key. Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface. Setting up GPG signing in Git \u00b6 Global This method sets up automatic code signing for all git repositories on your computer. Run the following commands under your user account: git config --global user.name \"Your Name\" git config --global user.email \"your-gpg-email@example.com\" git config --global commit.gpgsign true git config --global tag.gpgsign true git config --global user.signingkey YOUR-KEY-ID Per repository Run the following commands in the directory where you cloned the repository: git config user.name \"Your Name\" git config user.email \"your-gpg-email@example.com\" git config commit.gpgsign true git config tag.gpgsign true git config user.signingkey YOUR-KEY-ID Warning This method sets up GPG signing in a single repository. You must configure this every time you clone a new ContainerSSH repository . That's it! You can now continue with setting up the toolchain !","title":"Installing GPG"},{"location":"development/devenv/gpg/#setting-up-gpg","text":"Linux / WSL On Linux or Windows Subsystem for Linux GPG is already included in the package manager. You can install it using the following commands: Ubuntu sudo apt-get update sudo apt-get install gnupg2 RHEL/CentOS yum install gnupg2 Fedora dnf install gnupg2 Gentoo emerge --ask app-crypt/gnupg Tip You may want to install the Kleopatra GUI for easier access. Windows GPG4Win is a full suite for managing GPG keys on Windows. We recommend installing it with the Kleopatra GUI. MacOS Homebrew brew install gnupg2 MacPorts sudo port install gnupg2 GUI GPGTools offers a graphical version of GPG.","title":"Setting up GPG"},{"location":"development/devenv/gpg/#creating-your-gpg-key","text":"CLI (GPG 2.1.17+) Run the following command: gpg --full-generate-key Select RSA and RSA as the key format. Select 4096 bits for the bit size. When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config , otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option . Kleopatra Select File \u2192 New Key Pair... Select \"Create a personal OpenPGP key pair\" Set your name and the same e-mail address you have on your GitHub account. If you do not wish to publish your e-mail address GitHub gives you a privacy option . Follow the wizard to create your GPG key. GPGTools (MacOS) Please follow the GPGTools guide to create your key . When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config , otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option . CLI (GPG 2.1.16-) Run the following command: gpg --default-new-key-algo rsa4096 --gen-key Select RSA and RSA as the key format. Select 4096 bits for the bit size. When prompted for your user information make sure that the e-mail address matches your GitHub e-mail and the one in your Git config , otherwise your push may be rejected. If you do not wish to publish your e-mail address GitHub gives you a privacy option .","title":"Creating your GPG key"},{"location":"development/devenv/gpg/#adding-your-key-to-github","text":"CLI First, list your GPG keys with the key IDs: $ gpg --list-secret-keys --keyid-format LONG ------------------------------------------------ sec rsa4096/YOUR-KEY-ID 2020-06-18 [SC] ... Copy the key ID as you will need it for the next steps, then export your public key: gpg --armor --export YOUR-KEY-ID Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface. Kleopatra Right click the key generated in the previous step. Select \"Export...\". Save the file on your machine. Open the file in a text editor. Copy the key. Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface. GPGTools (MacOS) Select the previously generated key. Click the \"Export\" icon in the toolbar. Click Save. Open the file in a text editor. Copy the key. Go to GitHub \u2192 Settings \u2192 SSH and GPG keys and add a GPG key. Paste the key you just copied into the interface.","title":"Adding your key to GitHub"},{"location":"development/devenv/gpg/#setting-up-gpg-signing-in-git","text":"Global This method sets up automatic code signing for all git repositories on your computer. Run the following commands under your user account: git config --global user.name \"Your Name\" git config --global user.email \"your-gpg-email@example.com\" git config --global commit.gpgsign true git config --global tag.gpgsign true git config --global user.signingkey YOUR-KEY-ID Per repository Run the following commands in the directory where you cloned the repository: git config user.name \"Your Name\" git config user.email \"your-gpg-email@example.com\" git config commit.gpgsign true git config tag.gpgsign true git config user.signingkey YOUR-KEY-ID Warning This method sets up GPG signing in a single repository. You must configure this every time you clone a new ContainerSSH repository . That's it! You can now continue with setting up the toolchain !","title":"Setting up GPG signing in Git"},{"location":"development/devenv/ide/","text":"Setting up an IDE We strongly recommend setting up an IDE to warn you about potential issues and make the development process easier. Visual Studio Code \u00b6 Visual Studio Code is a free IDE for various languages from Microsoft for Windows, MacOS and Linux. It can be installed without admin permissions. Once you have installed it, please click the \"Extensions\" icon on the left and install the \"Go\" extension. You can then click the \"Explorer\" icon and click the \"Clone Repository\" button to clone a ContainerSSH repository . When you clone the repository you will be asked to install other tools, such as the delve debugger . Please install them. Once the repository is set up you can go to the file you want to run (e.g. cmd/containerssh/containerssh.go ), then go to the Run \u2192 Add configuration . You can then customize the parameters of running the program (e.g. where to get the config file from). To run tests open the test file (e.g. godogs_test.go ), click on TestMain and click the little \"run test\" text that comes up. The details of running ContainerSSH are discussed in the Getting started with ContainerSSH Development guide. Goland \u00b6 Goland is a commercial IDE from Jetbrains often used for Go development. It contains a number of analysis tools and quality of life features making it a popular choice. We recommend installing Goland using the Jetbrains Toolbox which will also keep it up to date. Once you launch Goland you will have the option to directly clone a Git repository. However, we recommend first going into the settings, then to Go \u2192 GOROOT and setting up Go. Once you have cloned the repository you can navigate to the file you want to run (e.g. cmd/containerssh/containerssh.go ), then click the little \"run\" icon next to the main function and click the \"Create\" button. This will create a configuration you can edit from the \"Run\" menu. To run the tests you can open the specific test you want to run or create a go test configuration from the Run menu. The details of running ContainerSSH are discussed in the Getting started with ContainerSSH Development guide.","title":"Setting up your IDE"},{"location":"development/devenv/ide/#visual-studio-code","text":"Visual Studio Code is a free IDE for various languages from Microsoft for Windows, MacOS and Linux. It can be installed without admin permissions. Once you have installed it, please click the \"Extensions\" icon on the left and install the \"Go\" extension. You can then click the \"Explorer\" icon and click the \"Clone Repository\" button to clone a ContainerSSH repository . When you clone the repository you will be asked to install other tools, such as the delve debugger . Please install them. Once the repository is set up you can go to the file you want to run (e.g. cmd/containerssh/containerssh.go ), then go to the Run \u2192 Add configuration . You can then customize the parameters of running the program (e.g. where to get the config file from). To run tests open the test file (e.g. godogs_test.go ), click on TestMain and click the little \"run test\" text that comes up. The details of running ContainerSSH are discussed in the Getting started with ContainerSSH Development guide.","title":"Visual Studio Code"},{"location":"development/devenv/ide/#goland","text":"Goland is a commercial IDE from Jetbrains often used for Go development. It contains a number of analysis tools and quality of life features making it a popular choice. We recommend installing Goland using the Jetbrains Toolbox which will also keep it up to date. Once you launch Goland you will have the option to directly clone a Git repository. However, we recommend first going into the settings, then to Go \u2192 GOROOT and setting up Go. Once you have cloned the repository you can navigate to the file you want to run (e.g. cmd/containerssh/containerssh.go ), then click the little \"run\" icon next to the main function and click the \"Create\" button. This will create a configuration you can edit from the \"Run\" menu. To run the tests you can open the specific test you want to run or create a go test configuration from the Run menu. The details of running ContainerSSH are discussed in the Getting started with ContainerSSH Development guide.","title":"Goland"},{"location":"development/devenv/kubernetes/","text":"Installing Kubernetes If you develop against the kuberun backend you will need a working Kubernetes. Windows / MacOS Docker Desktop contains a working Kubernetes. Please enable it to have a working Kubernetes setup. You can test it by running: kubectl get nodes Windows / WSL For WSL we recommend setting up KinD (Kubernetes in Docker) . Please read the KinD guide for getting it running. Please create a cluster with the oldest officially supported Kubernetes to test against: kind create cluster --image=image-url-here You can obtain the image URL from the KinD releases section . Linux We recommend using KinD (Kubernetes in Docker) as a reliable way to get a Kubernetes cluster running. Please create a cluster with the oldest officially supported Kubernetes to test against: kind create cluster --image=image-url-here You can obtain the image URL from the KinD releases section . Tip Some Linux distributions may support tiny Kubernetes distributions like k3s or microk8s , but we have managed to get consistently good results only with KinD.","title":"Installing Kubernetes"},{"location":"development/devenv/qa/","text":"Installing the QA tools Installing golangci-lint \u00b6 We are using golangci-lint as a way to lint the code for problematic practices. We use golangci-li using the following command line: golangci-lint run -E asciicheck -E bodyclose -E dupl -E errorlint -E exportloopref -E funlen Please follow the instructions below: Linux / WSL Go to the GitHub releases of golangci-lint and download the latest Linux .tar.gz . Extract the file to a directory in your path (e.g. ~/bin/go ). Add executable rights to the file (e.g. chmod +x ~/bin/go/golangci-lint ). Windows Go to the GitHub releases of golangci-lint and download the latest Windows ZIP. Extract the golangci-lint.exe to your %USERPROFILE%/go/bin directory. Brew (MacOS) brew install golangci-lint MacPorts (MacOS) sudo port install golangci-lint Manual (All platforms) Go to the GitHub releases of golangci-lint and download the latest archive for your platform. Extract the golangci-lint to the go/bin directory in your home directory.","title":"Installing the QA tools"},{"location":"development/devenv/qa/#installing-golangci-lint","text":"We are using golangci-lint as a way to lint the code for problematic practices. We use golangci-li using the following command line: golangci-lint run -E asciicheck -E bodyclose -E dupl -E errorlint -E exportloopref -E funlen Please follow the instructions below: Linux / WSL Go to the GitHub releases of golangci-lint and download the latest Linux .tar.gz . Extract the file to a directory in your path (e.g. ~/bin/go ). Add executable rights to the file (e.g. chmod +x ~/bin/go/golangci-lint ). Windows Go to the GitHub releases of golangci-lint and download the latest Windows ZIP. Extract the golangci-lint.exe to your %USERPROFILE%/go/bin directory. Brew (MacOS) brew install golangci-lint MacPorts (MacOS) sudo port install golangci-lint Manual (All platforms) Go to the GitHub releases of golangci-lint and download the latest archive for your platform. Extract the golangci-lint to the go/bin directory in your home directory.","title":"Installing golangci-lint"},{"location":"development/devenv/website/","text":"Setting up the website development environment This website is developed using mkdocs using the Material theme . This guide will run you through the steps of setting it up. Installing Python \u00b6 You can download Python from the official website . You will need at least Python 3.8. Cloning the repository \u00b6 In order to develop this website you will need to clone the repository: git clone https://github.com/containerssh/containerssh.github.io Creating a venv \u00b6 Once you have all that done we recommend you create a venv to avoid polluting your computer with packages: python3 -m venv /path/to/containerssh.github.io You can then activate the venv using the following script: venv/Scripts/activate Installing the dependencies \u00b6 Now you need to install the dependencies: pip install -r requirements.txt Optional: Setting the GITHUB_TOKEN \u00b6 Some functions of the website require a working GitHub Token without any special permissions. You can create a token here . You can then set the token using the command line: Linux / MacOS export GITHUB_TOKEN=\"your-token-here\" Windows (PowerShell) $env:GITHUB_TOKEN=\"your-token-here\" Windows (Command prompt) set GITHUB_TOKEN=your-token-here Warning Setting GITHUB_TOKEN dramatically slows down the development server because the GitHub API is queried for every refresh. Only set it when you need it. Running the dev server \u00b6 Run the following command to get a dev server up and running: python -m mkdocs serve This will start the development server on localhost:8000 . Tip We recommend using the free Visual Studio Code or the PyCharm Community Edition as a development environment for the website.","title":"Setting up the website"},{"location":"development/devenv/website/#installing-python","text":"You can download Python from the official website . You will need at least Python 3.8.","title":"Installing Python"},{"location":"development/devenv/website/#cloning-the-repository","text":"In order to develop this website you will need to clone the repository: git clone https://github.com/containerssh/containerssh.github.io","title":"Cloning the repository"},{"location":"development/devenv/website/#creating-a-venv","text":"Once you have all that done we recommend you create a venv to avoid polluting your computer with packages: python3 -m venv /path/to/containerssh.github.io You can then activate the venv using the following script: venv/Scripts/activate","title":"Creating a venv"},{"location":"development/devenv/website/#installing-the-dependencies","text":"Now you need to install the dependencies: pip install -r requirements.txt","title":"Installing the dependencies"},{"location":"development/devenv/website/#optional-setting-the-github_token","text":"Some functions of the website require a working GitHub Token without any special permissions. You can create a token here . You can then set the token using the command line: Linux / MacOS export GITHUB_TOKEN=\"your-token-here\" Windows (PowerShell) $env:GITHUB_TOKEN=\"your-token-here\" Windows (Command prompt) set GITHUB_TOKEN=your-token-here Warning Setting GITHUB_TOKEN dramatically slows down the development server because the GitHub API is queried for every refresh. Only set it when you need it.","title":"Optional: Setting the GITHUB_TOKEN"},{"location":"development/devenv/website/#running-the-dev-server","text":"Run the following command to get a dev server up and running: python -m mkdocs serve This will start the development server on localhost:8000 . Tip We recommend using the free Visual Studio Code or the PyCharm Community Edition as a development environment for the website.","title":"Running the dev server"},{"location":"development/releases/","text":"ContainerSSH release process \u00b6 In ContainerSSH we have two components that are in need of regular releases: libraries and binaries. Libraries Most of our libraries are intended for internal consumption, but some are for external consumption. The release process is the same. Read more \u00bb Binaries Binaries are consumed by our users. Therefore, this release process is more involved to ensure quality. Read more \u00bb","title":"Overview"},{"location":"development/releases/#containerssh-release-process","text":"In ContainerSSH we have two components that are in need of regular releases: libraries and binaries.","title":"ContainerSSH release process"},{"location":"development/releases/binaries/","text":"Creating ContainerSSH binary releases \u00b6 ContainerSSH binaries are what our users consume. This guide will attempt to outline the steps we take to make sure these releases are stable and no steps have been left out. Documentation \u00b6 The first step when preparing a new release the first step is to start preparing the documentation. The documentation for a new release always goes on the containerssh.io/reference/upcoming/ section. This section is copied from the current documentation and is modified for the new release. The new documentation pages must also be added to the mkdocs.yaml to make sure they are in the menu. If there are deprecated features they must be added to the deprecations section of the documentation. Deprecation notices should be written from the perspective of a user, explaining not only what's been deprecated, but also how to upgrade and replace the deprecated feature. You should also pay attention to the quick start section of the website, which may need to be updated for the new version. However, these changes should be added to a branch, only to be merged when the new release goes online. Versioning \u00b6 When creating a new release you must consider what version number to pick. We follow SemVer for ContainerSSH. Backwards-incompatible changes should increase the minor version number before 1.0, and increase the major version number after 1.0. Minor, backwards incompatible features should increase the minor version number, while bugfixes and security updates should increase the patch version number. Tests \u00b6 The main ContainerSSH repository contains a battery of tests that should pass before any release is made. If new features are added tests should be added to match. Releasing binaries \u00b6 Binaries are automatically generated using Goreleaser when a tag is created in the ContainerSSH repository . This will create and upload the built binaries to the GitHub releases section. However, with the binaries being available the job is not done. The build configuration in the images repository must be updated and a tag must be made. This will ensure that the new container images are pushed to the registries.","title":"Binaries"},{"location":"development/releases/binaries/#creating-containerssh-binary-releases","text":"ContainerSSH binaries are what our users consume. This guide will attempt to outline the steps we take to make sure these releases are stable and no steps have been left out.","title":"Creating ContainerSSH binary releases"},{"location":"development/releases/binaries/#documentation","text":"The first step when preparing a new release the first step is to start preparing the documentation. The documentation for a new release always goes on the containerssh.io/reference/upcoming/ section. This section is copied from the current documentation and is modified for the new release. The new documentation pages must also be added to the mkdocs.yaml to make sure they are in the menu. If there are deprecated features they must be added to the deprecations section of the documentation. Deprecation notices should be written from the perspective of a user, explaining not only what's been deprecated, but also how to upgrade and replace the deprecated feature. You should also pay attention to the quick start section of the website, which may need to be updated for the new version. However, these changes should be added to a branch, only to be merged when the new release goes online.","title":"Documentation"},{"location":"development/releases/binaries/#versioning","text":"When creating a new release you must consider what version number to pick. We follow SemVer for ContainerSSH. Backwards-incompatible changes should increase the minor version number before 1.0, and increase the major version number after 1.0. Minor, backwards incompatible features should increase the minor version number, while bugfixes and security updates should increase the patch version number.","title":"Versioning"},{"location":"development/releases/binaries/#tests","text":"The main ContainerSSH repository contains a battery of tests that should pass before any release is made. If new features are added tests should be added to match.","title":"Tests"},{"location":"development/releases/binaries/#releasing-binaries","text":"Binaries are automatically generated using Goreleaser when a tag is created in the ContainerSSH repository . This will create and upload the built binaries to the GitHub releases section. However, with the binaries being available the job is not done. The build configuration in the images repository must be updated and a tag must be made. This will ensure that the new container images are pushed to the registries.","title":"Releasing binaries"},{"location":"development/releases/libraries/","text":"Releasing a library \u00b6 ContainerSSH is made up of over 30 libraries . These libraries are all written in Go and must, therefore, follow the Go modules specification . It's not the most exciting read, but you should familiarize yourself with the process. We will attempt to outline the most important steps here. Versioning \u00b6 When heading up to a release versioning must be kept in mind. We follow SemVer , so only major versions may contain breaking changes. The only exception is versions before 1.0.0 , these may break in minor versions too. This includes methods like factories (e.g. New methods). With Go modules versions beyond version 1 should have the module suffix v2 , v3 , etc. For example, you may have a module called github.com/containerssh/auth/v2 . Release notes \u00b6 Once the main branch is ready for release the last step is writing release notes. We do not follow the concept of simply listing the commits, we aim to write human-readable release notes that explain the changes. The explanation has to be detailed enough so anyone from the target audience can understand what is changing without needing to understand the implementation. The release notes should be added to CHANGELOG.md . Updating the README \u00b6 Before creating a release the README.md file should be updated such that consumers of the library know how to use the new version of the library. Except for a few libraries like auth and configuration our libraries are intended only for consumption in ContainerSSH. This means that you can write the documentation from that perspective. Creating a release \u00b6 The day has finally come: the release notes and codes are in, tests are passing. Now we need to create a release. We do this exclusively from the GitHub interface. We name versions for libraries in the format of v1.2.3 so Go can pull them in, while applications are named 1.2.3 to avoid pulling them in from a Go program. We copy the name and description from the CHANGELOG.md into the release notes on GitHub. For applications the release mechanism will create and upload the binaries.","title":"Libraries"},{"location":"development/releases/libraries/#releasing-a-library","text":"ContainerSSH is made up of over 30 libraries . These libraries are all written in Go and must, therefore, follow the Go modules specification . It's not the most exciting read, but you should familiarize yourself with the process. We will attempt to outline the most important steps here.","title":"Releasing a library"},{"location":"development/releases/libraries/#versioning","text":"When heading up to a release versioning must be kept in mind. We follow SemVer , so only major versions may contain breaking changes. The only exception is versions before 1.0.0 , these may break in minor versions too. This includes methods like factories (e.g. New methods). With Go modules versions beyond version 1 should have the module suffix v2 , v3 , etc. For example, you may have a module called github.com/containerssh/auth/v2 .","title":"Versioning"},{"location":"development/releases/libraries/#release-notes","text":"Once the main branch is ready for release the last step is writing release notes. We do not follow the concept of simply listing the commits, we aim to write human-readable release notes that explain the changes. The explanation has to be detailed enough so anyone from the target audience can understand what is changing without needing to understand the implementation. The release notes should be added to CHANGELOG.md .","title":"Release notes"},{"location":"development/releases/libraries/#updating-the-readme","text":"Before creating a release the README.md file should be updated such that consumers of the library know how to use the new version of the library. Except for a few libraries like auth and configuration our libraries are intended only for consumption in ContainerSSH. This means that you can write the documentation from that perspective.","title":"Updating the README"},{"location":"development/releases/libraries/#creating-a-release","text":"The day has finally come: the release notes and codes are in, tests are passing. Now we need to create a release. We do this exclusively from the GitHub interface. We name versions for libraries in the format of v1.2.3 so Go can pull them in, while applications are named 1.2.3 to avoid pulling them in from a Go program. We copy the name and description from the CHANGELOG.md into the release notes on GitHub. For applications the release mechanism will create and upload the binaries.","title":"Creating a release"},{"location":"development/tools/","text":"External tools we use \u00b6 This guide runs you through the external tools we use and how we use them. This will give you a better idea on how ContainerSSH is managed. GitHub GitHub provides a large chunk of our infrastructure, ranging from Git hosting, CI system to hosting this very website. Read more \u00bb Terraform Since we have a large number of repositories we use the Terraform Cloud to create and configure most of the settings in our GitHub organization. This also enables non-privileged users to request new repositories or changes to existing ones. Read more \u00bb Snyk Snyk is our tool of choice to keep an eye on security updates we need to apply to Go dependencies, as well as container images. Read more \u00bb Docker Hub Docker Hub is the primary source for our container images. Docker has graciously weaved rate limits for our organization. Read more \u00bb","title":"Overview"},{"location":"development/tools/#external-tools-we-use","text":"This guide runs you through the external tools we use and how we use them. This will give you a better idea on how ContainerSSH is managed.","title":"External tools we use"},{"location":"development/tools/docker/","text":"How we use the Docker Hub \u00b6 Docker Hub is our main distribution point for ContainerSSH images. We also maintain mirrors on Quay.io . Docker has graciously included us in their open source program so our images are not subject to rate limits . This is especially important because ContainerSSH pulls the default guest image (an Ubuntu with the installed guest agent and SFTP) from the Docker Hub for every connection. At the time of writing this image has already been pulled more than 100.000 times. In order to make sure that our pushes are independent from any one person we have created a machine user called containersshbuilder , which only has permissions to the ContainerSSH organization. The credentials of this user are added to GitHub actions . The container images are build using the images repository . This repository contains a Go build program that uses docker-compose to build and test ContainerSSH images before pushing them. (We learned this the hard way .) The default guest image on the other hand is rebuilt daily to incorporate the latest updates.","title":"Docker"},{"location":"development/tools/docker/#how-we-use-the-docker-hub","text":"Docker Hub is our main distribution point for ContainerSSH images. We also maintain mirrors on Quay.io . Docker has graciously included us in their open source program so our images are not subject to rate limits . This is especially important because ContainerSSH pulls the default guest image (an Ubuntu with the installed guest agent and SFTP) from the Docker Hub for every connection. At the time of writing this image has already been pulled more than 100.000 times. In order to make sure that our pushes are independent from any one person we have created a machine user called containersshbuilder , which only has permissions to the ContainerSSH organization. The credentials of this user are added to GitHub actions . The container images are build using the images repository . This repository contains a Go build program that uses docker-compose to build and test ContainerSSH images before pushing them. (We learned this the hard way .) The default guest image on the other hand is rebuilt daily to incorporate the latest updates.","title":"How we use the Docker Hub"},{"location":"development/tools/github/","text":"How we use GitHub \u00b6 Our GitHub account contains over 30 repositories. These repositories are managed by Terraform and most of them are created from the library-template template repository. This is done in an effort to make sure each component has its own tests and documentation. Tests in each repository are executed by GitHub actions. Most repositories use CodeQL for security analysis, golangci-lint for code quality, and run go test for executing tests. Some repositories have integrations with Docker and Kubernetes. Docker is included in GitHub Actions and Kubernetes support is added by using the Kubernetes in Docker action . When running against Kubernetes we run against all currently supported versions . The tests use the kubeconfig of the current user to fetch the configuration on how to connect Kubernetes. An example of this can be found in the Kubernetes repository . When we release ContainerSSH we use Goreleaser to create binaries for multiple platforms. Goreleaser is also responsible for uploading the generated binaries to GitHub as a release. Currently, Goreleaser also generates .deb and .rpm packages. Later on, this will be replaced by providing a package repository that people can add to their operating systems. This is also hosted in GitHub Pages .","title":"GitHub"},{"location":"development/tools/github/#how-we-use-github","text":"Our GitHub account contains over 30 repositories. These repositories are managed by Terraform and most of them are created from the library-template template repository. This is done in an effort to make sure each component has its own tests and documentation. Tests in each repository are executed by GitHub actions. Most repositories use CodeQL for security analysis, golangci-lint for code quality, and run go test for executing tests. Some repositories have integrations with Docker and Kubernetes. Docker is included in GitHub Actions and Kubernetes support is added by using the Kubernetes in Docker action . When running against Kubernetes we run against all currently supported versions . The tests use the kubeconfig of the current user to fetch the configuration on how to connect Kubernetes. An example of this can be found in the Kubernetes repository . When we release ContainerSSH we use Goreleaser to create binaries for multiple platforms. Goreleaser is also responsible for uploading the generated binaries to GitHub as a release. Currently, Goreleaser also generates .deb and .rpm packages. Later on, this will be replaced by providing a package repository that people can add to their operating systems. This is also hosted in GitHub Pages .","title":"How we use GitHub"},{"location":"development/tools/snyk/","text":"How we use Snyk \u00b6 Snyk is a wonderful service and program to monitor dependencies that have security issues. While not all dependencies are high profile enough to make this a replacement for vetting dependencies, it helps tremendously with keeping up with container image updates. Snyk monitors both our container images on Docker Hub as well as our Go dependencies from the go.mod files. Tip The Go monitoring in Snyk is a bit finicky, so when a vulnerable library is discovered we add a replace section to the go.mod file as follows: // Fixes CVE-2020-9283 replace ( golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 golang.org/x/crypto v0.0.0-20200220183623-bac4c82f6975 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 ) This is done even if the vulnerable library is not ultimately used. The reason for this is part added safety, and in other part to satisfy Snyk. When vulnerable container images are discovered we update our appropriate repositories, while in case of Go library vulnerabilities we release a new version.","title":"Snyk"},{"location":"development/tools/snyk/#how-we-use-snyk","text":"Snyk is a wonderful service and program to monitor dependencies that have security issues. While not all dependencies are high profile enough to make this a replacement for vetting dependencies, it helps tremendously with keeping up with container image updates. Snyk monitors both our container images on Docker Hub as well as our Go dependencies from the go.mod files. Tip The Go monitoring in Snyk is a bit finicky, so when a vulnerable library is discovered we add a replace section to the go.mod file as follows: // Fixes CVE-2020-9283 replace ( golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 golang.org/x/crypto v0.0.0-20200220183623-bac4c82f6975 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9 => golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83 ) This is done even if the vulnerable library is not ultimately used. The reason for this is part added safety, and in other part to satisfy Snyk. When vulnerable container images are discovered we update our appropriate repositories, while in case of Go library vulnerabilities we release a new version.","title":"How we use Snyk"},{"location":"development/tools/terraform/","text":"How we use Terraform \u00b6 Our GitHub organization contains more than 30 repositories . In order to manage the permissions and settings properly we maintain a repository with Terraform code that is consumed by Terraform Cloud . To pull this off we make use of the GitHub provider for Terraform . It is worth noting that the documentation may not always be up to date. It also doesn't support managing all properties of GitHub repositories, most prominently the OpenGraph preview images. These are put in place by hand. Once the code is pushed into the main repository in our GitHub repository, the Terraform Cloud picks up the change and applies it. This can be used to generate new repositories or change the settings on a repository.","title":"Terraform"},{"location":"development/tools/terraform/#how-we-use-terraform","text":"Our GitHub organization contains more than 30 repositories . In order to manage the permissions and settings properly we maintain a repository with Terraform code that is consumed by Terraform Cloud . To pull this off we make use of the GitHub provider for Terraform . It is worth noting that the documentation may not always be up to date. It also doesn't support managing all properties of GitHub repositories, most prominently the OpenGraph preview images. These are put in place by hand. Once the code is pushed into the main repository in our GitHub repository, the Terraform Cloud picks up the change and applies it. This can be used to generate new repositories or change the settings on a repository.","title":"How we use Terraform"},{"location":"downloads/","text":"Downloads Latest release (0.4.1, released May 26, 2021) \u00b6 Read the reference manual \u00bb Container ContainerSSH can be installed in a containerized system (Kubernetes, Docker, Podman) by referencing the following image names: containerssh/containerssh:0.4 containerssh/containerssh:0.4.1 quay.io/containerssh/containerssh:0.4 quay.io/containerssh/containerssh:0.4.1 Our container images are built on Alpine Linux (x86, 64 bit) . Note about container image versioning We provide the images with multiple version tags. latest will always reference the latest image build of the latest stable version. 0.4 will always reference the latest image build of the latest 0.4 version, and 0.4.0 will always reference the latest image build of 0.4.1. Each of these tags will see updates as we update the base Alpine Linux image to apply security fixes. If you need to roll back to an exact previous image you can reference the image by build date, e.g. 0.4.1-20210526 . The list of images can be found on the Docker Hub . Linux x86 (.tar.gz) x86 (.deb) x86 (.rpm) MacOS Intel (.tar.gz) Windows .zip FreeBSD .tar.gz Older releases \u00b6 Version Container Linux Windows MacOS FreeBSD Manual 0.4.0 Apr 1, 2021 containerssh/containerssh:0.4.0 .tar.gz .deb .rpm .apk .zip .tar.gz .tar.gz Read \u00bb 0.3.1 Oct 23, 2020 containerssh/containerssh:0.3.1 .tar.gz .deb .rpm .zip .tar.gz Read \u00bb 0.3.0 Oct 21, 2020 containerssh/containerssh:0.3.0 .tar.gz .deb .rpm .zip .tar.gz Read \u00bb 0.2.2 Aug 2, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.2.1 Jul 30, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.2.0 Jul 9, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.1.1 Jul 9, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.1.0 Jun 18, 2020 .tar.gz .deb .rpm .zip .tar.gz Note Container images that no longer have pulls have been removed to conserve resources.","title":"Downloads"},{"location":"downloads/#latest-release-041-released-may-26-2021","text":"Read the reference manual \u00bb Container ContainerSSH can be installed in a containerized system (Kubernetes, Docker, Podman) by referencing the following image names: containerssh/containerssh:0.4 containerssh/containerssh:0.4.1 quay.io/containerssh/containerssh:0.4 quay.io/containerssh/containerssh:0.4.1 Our container images are built on Alpine Linux (x86, 64 bit) . Note about container image versioning We provide the images with multiple version tags. latest will always reference the latest image build of the latest stable version. 0.4 will always reference the latest image build of the latest 0.4 version, and 0.4.0 will always reference the latest image build of 0.4.1. Each of these tags will see updates as we update the base Alpine Linux image to apply security fixes. If you need to roll back to an exact previous image you can reference the image by build date, e.g. 0.4.1-20210526 . The list of images can be found on the Docker Hub . Linux x86 (.tar.gz) x86 (.deb) x86 (.rpm) MacOS Intel (.tar.gz) Windows .zip FreeBSD .tar.gz","title":"Latest release (0.4.1, released May 26, 2021)"},{"location":"downloads/#older-releases","text":"Version Container Linux Windows MacOS FreeBSD Manual 0.4.0 Apr 1, 2021 containerssh/containerssh:0.4.0 .tar.gz .deb .rpm .apk .zip .tar.gz .tar.gz Read \u00bb 0.3.1 Oct 23, 2020 containerssh/containerssh:0.3.1 .tar.gz .deb .rpm .zip .tar.gz Read \u00bb 0.3.0 Oct 21, 2020 containerssh/containerssh:0.3.0 .tar.gz .deb .rpm .zip .tar.gz Read \u00bb 0.2.2 Aug 2, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.2.1 Jul 30, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.2.0 Jul 9, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.1.1 Jul 9, 2020 .tar.gz .deb .rpm .zip .tar.gz 0.1.0 Jun 18, 2020 .tar.gz .deb .rpm .zip .tar.gz Note Container images that no longer have pulls have been removed to conserve resources.","title":"Older releases"},{"location":"getting-started/","text":"Quick start This is a quick start guide to get a test server up and running in less than 5 minutes with docker-compose or Kubernetes. \u25b6\ufe0f Watch as Video (Docker) \u25b6\ufe0f Watch as Video (Kubernetes) Warning This setup will let any password authenticate. Only use it for testing. Step 1: Set up a Dockerized environment \u00b6 Docker To run this quick start please make sure you have a working Docker environment and a working docker-compose . Kubernetes To run this quick start please make sure you have a working Kubernetes environment. We recommend setting up Docker Desktop , k3s , or Kubernetes in Docker . Step 2: Download the sample files \u00b6 Please download the contents of the example directory from the source code repository. Step 3: Launch ContainerSSH \u00b6 Docker In the downloaded directory run docker-compose up -d . Kubernetes In the downloaded directory run kubectl apply -f kubernetes.yaml . Step 4: Logging in \u00b6 Run ssh foo@localhost -p 2222 on the same machine via a new terminal window. This is your test client. You should be able to log in with any password. Alternatively you can also try the user busybox to land in a Busybox container. Step 5: Cleaning up \u00b6 Docker Once you're done, you can shut down the server using the docker-compose down , then remove the images using docker-compose rm . Finally, you can also remove the guest image: docker image rm containerssh/containerssh-guest-image Kubernetes Once you're done, you can shut down the server by running kubectl delete -f kubernetes.yaml . Step 6: Making it productive \u00b6 The authentication and configuration server included in the example is a dummy server and lets any password in. To actually use ContainerSSH, you will have to write your own authentication server . We recommend reading the architecture overview before proceeding. Tip You can pass the CONTAINERSSH_ALLOW_ALL environment variable to the demo auth-config server to build a honeypot.","title":"Quick Start"},{"location":"getting-started/#step-1-set-up-a-dockerized-environment","text":"Docker To run this quick start please make sure you have a working Docker environment and a working docker-compose . Kubernetes To run this quick start please make sure you have a working Kubernetes environment. We recommend setting up Docker Desktop , k3s , or Kubernetes in Docker .","title":"Step 1: Set up a Dockerized environment"},{"location":"getting-started/#step-2-download-the-sample-files","text":"Please download the contents of the example directory from the source code repository.","title":"Step 2: Download the sample files"},{"location":"getting-started/#step-3-launch-containerssh","text":"Docker In the downloaded directory run docker-compose up -d . Kubernetes In the downloaded directory run kubectl apply -f kubernetes.yaml .","title":"Step 3: Launch ContainerSSH"},{"location":"getting-started/#step-4-logging-in","text":"Run ssh foo@localhost -p 2222 on the same machine via a new terminal window. This is your test client. You should be able to log in with any password. Alternatively you can also try the user busybox to land in a Busybox container.","title":"Step 4: Logging in"},{"location":"getting-started/#step-5-cleaning-up","text":"Docker Once you're done, you can shut down the server using the docker-compose down , then remove the images using docker-compose rm . Finally, you can also remove the guest image: docker image rm containerssh/containerssh-guest-image Kubernetes Once you're done, you can shut down the server by running kubectl delete -f kubernetes.yaml .","title":"Step 5: Cleaning up"},{"location":"getting-started/#step-6-making-it-productive","text":"The authentication and configuration server included in the example is a dummy server and lets any password in. To actually use ContainerSSH, you will have to write your own authentication server . We recommend reading the architecture overview before proceeding. Tip You can pass the CONTAINERSSH_ALLOW_ALL environment variable to the demo auth-config server to build a honeypot.","title":"Step 6: Making it productive"},{"location":"getting-started/architecture/","text":"Architecture \u25b6\ufe0f Watch as Video ContainerSSH is a modular software that consists of the following main components: The user connects ContainerSSH using an SSH client (e.g. PuTTY) ContainerSSH performs the handshake and offers the user the authentication methods supported. ContainerSSH will submit the users SSH key or password to the authentication server using HTTP (TLS encryption and authentication possible.) For more details see the page about the Auth Server . If the authentication is successful ContainerSSH will optionally contact the Config Server to fetch the container backend configuration. The config server can pass anything from container backend credentials to image configuration to ContainerSSH. For more details see the page about the Config Server . When the users SSH client requests a shell or program ContainerSSH contacts the backend configured (Docker or Kubernetes) and launches the desired Pod / Container. Currently, each new shell or program request launches a new container. For more details see the backends page . The authentication and configuration servers are not part of ContainerSSH and you will need to provide them.","title":"Architecture"},{"location":"getting-started/authserver/","text":"Implementing an authentication server ContainerSSH does not know your users and their passwords. Therefore, it calls out to a microservice that you have to provide. Your service can verify the users, passwords, and SSH keys. You will have to provide the microservice URL in the configuration. auth : url : \"http://your-server-name/\" Tip We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. For password authentication ContainerSSH will call out to the /password path on your authentication server. The request body will be the following: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" } The public key auth ContainerSSH will call out to /pubkey in the following format: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"publicKey\" : \"ssh-rsa ...\" } The public key is provided in the SSH wire format in base64 encoding. Your server will need to respond with the following JSON: { \"success\" : true } Tip We provide a Go library to implement a authentication server . You can read more in the reference manual","title":"Authentication Server"},{"location":"getting-started/backends/","text":"Backend selection ContainerSSH is built to support multiple backends. The backend can be changed in the configuration file: # change to `kubernetes` to talk to Kubernetes backend : docker ContainerSSH currently supports the following backends: Backend Description docker Runs Docker containers. kubernetes Runs Kubernetes containers. sshproxy Forwards SSH connections to a backend server. Read more in the SSH proxy reference manual \u00bb","title":"Selecting a Backend"},{"location":"getting-started/configuration/","text":"Configuring ContainerSSH Before you can run ContainerSSH, you will need to create a configuration file. The minimal configuration file looks like this: ssh : hostkeys : # Generate a host key with openssl genrsa - /path/to/your/host/key auth : # See auth server below url : http://your-auth-server/ password : true # Perform password authentication pubkey : false # Perform public key authentication The config file must end in .yml , .yaml , or .json . You can dump the entire configuration file using ./containerssh --dump-config Note Parts of the configuration can be provided dynamically based on the username using a configserver . Note In order to actually use ContainerSSH you will also need to provide a backend configuration either via this file or via the configserver .","title":"Configuration"},{"location":"getting-started/docker/","text":"The Docker backend The docker backend launches a container using the Docker API. The default configuration connects the Docker socket on its default path. Changing the container image \u00b6 The container image depends on the backend you are using. For docker you can change the image in the config file: docker : execution : container : image : your/image You can read more in the reference manual","title":"The Docker Backend"},{"location":"getting-started/docker/#changing-the-container-image","text":"The container image depends on the backend you are using. For docker you can change the image in the config file: docker : execution : container : image : your/image You can read more in the reference manual","title":"Changing the container image"},{"location":"getting-started/faq/","text":"FAQ Is ContainerSSH secure? \u00b6 ContainerSSH depends on a number of libraries to achieve what it does. A security hole in any of the critical ones could mean a compromise of your container environment, especially if you are using the Docker backend. (Docker has no access control so a compromise means your whole host is compromised.) Please read the hardening guide if you intend to use ContainerSSH in production. Is ContainerSSH production-ready? \u00b6 ContainerSSH is in use by several companies in production and has caused no issues or crashes. That being said, it is very early in its development and the API and configuration file format may still change. If you intend to use ContainerSSH in production please read the hardening guide and feel free to reach out . Does ContainerSSH delete containers after it is done? \u00b6 ContainerSSH does its best to delete containers it creates. However, at this time there is no cleanup mechanism in case it crashes. Do I need to run ContainerSSH as root? \u00b6 No! In fact, you shouldn't! ContainerSSH is perfectly fine running as non-root as long as it has access to Kubernetes or Docker. (Granted, access to the Docker socket means it could easily launch a root process on the host.) Does ContainerSSH support SFTP? \u00b6 Yes, but your container image must contain an SFTP server binary and your config.yaml or config server must contain the correct path for the server. Does ContainerSSH support SCP? \u00b6 No, ContainerSSH does not support SCP and since OpenSSH is dropping SCP support too it probably never will. Does ContainerSSH support TCP port forwarding? \u00b6 No, but it is planned . Does ContainerSSH support SSH agent forwarding? \u00b6 No, but it is planned . Does ContainerSSH support X11 forwarding? \u00b6 No, and X11 is a rarely used feature, so we are not planning on supporting it in the near future. Does ContainerSSH support forwarding signals? \u00b6 Yes, as of version 0.4 all backends support signal forwarding using the ContainerSSH agent. Does ContainerSSH support window resizing? \u00b6 Yes. Does ContainerSSH support environment variable passing? \u00b6 Yes. Does ContainerSSH support returning the exit status? \u00b6 Yes. Can ContainerSSH run exec into existing containers? \u00b6 No, all containers are started for a connection or session and are removed at the end. This will be a future feature. Can ContainerSSH deploy additional services, such as sidecar containers, etc? \u00b6 ContainerSSH supports the entire Kubernetes pod specification so you can launch as many containers as you want in a single pod. The Docker backend, however, does not support sidecar containers. Can I use my normal kubeconfig files? \u00b6 Unfortunately, no. Kubeconfig files are parsed by kubectl and the code is quite elaborate. At this time, adding it to ContainerSSH is not planned.","title":"FAQ"},{"location":"getting-started/faq/#is-containerssh-secure","text":"ContainerSSH depends on a number of libraries to achieve what it does. A security hole in any of the critical ones could mean a compromise of your container environment, especially if you are using the Docker backend. (Docker has no access control so a compromise means your whole host is compromised.) Please read the hardening guide if you intend to use ContainerSSH in production.","title":"Is ContainerSSH secure?"},{"location":"getting-started/faq/#is-containerssh-production-ready","text":"ContainerSSH is in use by several companies in production and has caused no issues or crashes. That being said, it is very early in its development and the API and configuration file format may still change. If you intend to use ContainerSSH in production please read the hardening guide and feel free to reach out .","title":"Is ContainerSSH production-ready?"},{"location":"getting-started/faq/#does-containerssh-delete-containers-after-it-is-done","text":"ContainerSSH does its best to delete containers it creates. However, at this time there is no cleanup mechanism in case it crashes.","title":"Does ContainerSSH delete containers after it is done?"},{"location":"getting-started/faq/#do-i-need-to-run-containerssh-as-root","text":"No! In fact, you shouldn't! ContainerSSH is perfectly fine running as non-root as long as it has access to Kubernetes or Docker. (Granted, access to the Docker socket means it could easily launch a root process on the host.)","title":"Do I need to run ContainerSSH as root?"},{"location":"getting-started/faq/#does-containerssh-support-sftp","text":"Yes, but your container image must contain an SFTP server binary and your config.yaml or config server must contain the correct path for the server.","title":"Does ContainerSSH support SFTP?"},{"location":"getting-started/faq/#does-containerssh-support-scp","text":"No, ContainerSSH does not support SCP and since OpenSSH is dropping SCP support too it probably never will.","title":"Does ContainerSSH support SCP?"},{"location":"getting-started/faq/#does-containerssh-support-tcp-port-forwarding","text":"No, but it is planned .","title":"Does ContainerSSH support TCP port forwarding?"},{"location":"getting-started/faq/#does-containerssh-support-ssh-agent-forwarding","text":"No, but it is planned .","title":"Does ContainerSSH support SSH agent forwarding?"},{"location":"getting-started/faq/#does-containerssh-support-x11-forwarding","text":"No, and X11 is a rarely used feature, so we are not planning on supporting it in the near future.","title":"Does ContainerSSH support X11 forwarding?"},{"location":"getting-started/faq/#does-containerssh-support-forwarding-signals","text":"Yes, as of version 0.4 all backends support signal forwarding using the ContainerSSH agent.","title":"Does ContainerSSH support forwarding signals?"},{"location":"getting-started/faq/#does-containerssh-support-window-resizing","text":"Yes.","title":"Does ContainerSSH support window resizing?"},{"location":"getting-started/faq/#does-containerssh-support-environment-variable-passing","text":"Yes.","title":"Does ContainerSSH support environment variable passing?"},{"location":"getting-started/faq/#does-containerssh-support-returning-the-exit-status","text":"Yes.","title":"Does ContainerSSH support returning the exit status?"},{"location":"getting-started/faq/#can-containerssh-run-exec-into-existing-containers","text":"No, all containers are started for a connection or session and are removed at the end. This will be a future feature.","title":"Can ContainerSSH run exec into existing containers?"},{"location":"getting-started/faq/#can-containerssh-deploy-additional-services-such-as-sidecar-containers-etc","text":"ContainerSSH supports the entire Kubernetes pod specification so you can launch as many containers as you want in a single pod. The Docker backend, however, does not support sidecar containers.","title":"Can ContainerSSH deploy additional services, such as sidecar containers, etc?"},{"location":"getting-started/faq/#can-i-use-my-normal-kubeconfig-files","text":"Unfortunately, no. Kubeconfig files are parsed by kubectl and the code is quite elaborate. At this time, adding it to ContainerSSH is not planned.","title":"Can I use my normal kubeconfig files?"},{"location":"getting-started/getting-help/","text":"Getting help This is merely the quick start guide. We also provide a reference manual as well as guides for specific use cases . However, sometimes the best documentation can leave you stuck with an issue. If you need help, want to discuss a topic around ContainerSSH, or have feedback you can use our GitHub Discussions Board to post a question.","title":"Getting Help"},{"location":"getting-started/installation/","text":"Installing ContainerSSH ContainerSSH is provided on the Downloads page . You can install it in a containerized environment or as a standalone software on Windows, Linux, and MacOS. Standalone ContainerSSH can be deployed outside of a container. On our downloads page we provide binaries for Linux, Windows, and MacOS. We also provide DEB and RPM packages. Before running ContainerSSH you will need to create a config.yaml file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /path/to/your/host.key auth : url : http://your-auth-server/ ContainerSSH can then be started by running ./containerssh --config /path/to/your/config.yaml Docker When deploying in Docker you must first prepare a configuration file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /var/run/secrets/host.key auth : url : http://your-auth-server/ You can then run ContainerSSH with the following command line: docker run -d \\ -v /srv/containerssh/config.yaml:/etc/containerssh/config.yaml \\ -v /srv/containerssh/host.key:/var/run/secrets/host.key \\ -p 2222 :2222 \\ containerssh/containerssh:0.4 Kubernetes When running ContainerSSH inside a Kubernetes cluster you must furst create a Secret that contains the host key. openssl genrsa | kubectl create secret generic containerssh-hostkey --from-file = host.key = /dev/stdin Next, you can create a ConfigMap to hold the ContainerSSH configuration: ( cat << EOF ssh: hostkeys: - /etc/containerssh/host.key auth: url: http://your-auth-server/ EOF ) | kubectl create configmap containerssh-config --from-file = config.yaml = /dev/stdin Then you can create a deployment to run ContainerSSH: ( cat << EOF apiVersion: apps/v1 kind: Deployment metadata: name: containerssh labels: app: containerssh spec: replicas: 1 selector: matchLabels: app: containerssh template: metadata: labels: app: containerssh spec: containers: - name: containerssh image: containerssh/containerssh:0.4 ports: - containerPort: 2222 volumeMounts: - name: hostkey mountPath: /etc/containerssh/host.key subPath: host.key readOnly: true - name: config mountPath: /etc/containerssh/config.yaml subPath: config.yaml readOnly: true volumes: - name: hostkey secret: secretName: containerssh-hostkey - name: config configMap: name: containerssh-config EOF ) | kubectl apply -f - Finally, you can create a service to expose the SSH port. You can customize this to create a loadbalancer or nodeport to make SSH publicly available. See kubectl expose --help for details. kubectl expose deployment containerssh \\ --port = 2222 --target-port = 2222 \\ --name = containerssh","title":"Installation"},{"location":"getting-started/kubernetes/","text":"The Kubernetes backend runs a pod in a Kubernetes cluster and attaches to a container there. Running outside of Kubernetes \u00b6 If you are running ContainerSSH outside of Kubernetes you will need the following configuration: kubernetes : connection : host : your-kubernetes-api-server:6443 cert : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- Alternatively you can use cacertFile , keyFile and certFile to point to files on the filesystem. Running inside a Kubernetes cluster \u00b6 When you run inside of a Kubernetes cluster you can use the service account token: kubernetes : connection : certFile : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile : /var/run/secrets/kubernetes.io/serviceaccount/token Changing the container image \u00b6 For the kubernetes backend the container image can be changed by modifying the pod spec: kubernetes : pod : consoleContainerNumber : 0 metadata : namespace : default spec : containers : - name : shell image : containerssh/containerssh-guest-image Note: if you are running multiple containers you should specify the consoleContainerNumber parameter to indicate which container you wish to attach to when an SSH session is opened. You can read more in the reference manual","title":"The Kubernetes Backend"},{"location":"getting-started/kubernetes/#running-outside-of-kubernetes","text":"If you are running ContainerSSH outside of Kubernetes you will need the following configuration: kubernetes : connection : host : your-kubernetes-api-server:6443 cert : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- Alternatively you can use cacertFile , keyFile and certFile to point to files on the filesystem.","title":"Running outside of Kubernetes"},{"location":"getting-started/kubernetes/#running-inside-a-kubernetes-cluster","text":"When you run inside of a Kubernetes cluster you can use the service account token: kubernetes : connection : certFile : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile : /var/run/secrets/kubernetes.io/serviceaccount/token","title":"Running inside a Kubernetes cluster"},{"location":"getting-started/kubernetes/#changing-the-container-image","text":"For the kubernetes backend the container image can be changed by modifying the pod spec: kubernetes : pod : consoleContainerNumber : 0 metadata : namespace : default spec : containers : - name : shell image : containerssh/containerssh-guest-image Note: if you are running multiple containers you should specify the consoleContainerNumber parameter to indicate which container you wish to attach to when an SSH session is opened. You can read more in the reference manual","title":"Changing the container image"},{"location":"getting-started/sshproxy/","text":"The SSH proxy backend \u00b6 The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend. The base configuration structure \u00b6 The minimum configuration is the following: backend : sshproxy sshproxy : # Add the backend server here server : 127.0.0.1 # Set the following option to true to reuse the connecting user's username. usernamePassThrough : true # Or specify a username manually username : root # Specify the password password : changeme # Or the private key. This can reference a file or be added directly. privateKey : | -----BEGIN OPENSSH PRIVATE KEY----- ... # Provide all fingerprints of the backing SSH server's host keys: allowedHostKeyFingerprints : - SHA256:... Read more in the SSH proxy reference manual \u00bb","title":"The SSH Proxy Backend"},{"location":"getting-started/sshproxy/#the-ssh-proxy-backend","text":"The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend.","title":"The SSH proxy backend"},{"location":"getting-started/sshproxy/#the-base-configuration-structure","text":"The minimum configuration is the following: backend : sshproxy sshproxy : # Add the backend server here server : 127.0.0.1 # Set the following option to true to reuse the connecting user's username. usernamePassThrough : true # Or specify a username manually username : root # Specify the password password : changeme # Or the private key. This can reference a file or be added directly. privateKey : | -----BEGIN OPENSSH PRIVATE KEY----- ... # Provide all fingerprints of the backing SSH server's host keys: allowedHostKeyFingerprints : - SHA256:... Read more in the SSH proxy reference manual \u00bb","title":"The base configuration structure"},{"location":"guides/","text":"Guides for ContainerSSH Building a honeypot with ContainerSSH This guide will lead you through the steps required to create an SSH honeypot with ContainerSSH. Read more \u00bb Logging to the ELK stack with Docker and Fluentd This guide will lead you through the steps required to transport ContainerSSH logs from Docker to the ELK stack using Fluentd. Read more \u00bb","title":"Overview"},{"location":"guides/honeypot/","text":"Creating a honeypot with ContainerSSH \u00b6 This guide will lead you through the steps of creating an SSH honeypot with ContainerSSH. Danger Creating SSH honeypots with a real Linux backend is inherently dangerous. Any local privilege escalation could lead to the attacker taking over your host system. While this tutorial represents the best practices in building a honeypot, the responsibility of securing your installation ultimately rests upon you. Please do not attempt this unless you are intimately familiar with securing container environments. Docker has really good documentation on this topic. Step 1: Infrastructure \u00b6 In order to set up a honeypot securely you will need at least two hosts: one to run ContainerSSH and the second to run the container infrastructure the attacker is dropped into. We'll call the first host the gateway VM and the second one sacrificial VM. Ideally, the sacrificial VM should run on its own dedicated physical hardware to prevent leakage of secrets due to CPU bugs. Both VMs need sufficient disk space to hold audit logs and containers. Furthermore, you will need an S3-compatible object storage to upload audit logs and we will need a Prometheus installation for monitoring. We strongly recommend automating the setup with a tool like Terraform to rapidly apply security updates. Step 2: Firewalling the gateway \u00b6 You should set up the gateway host in such a way that it is visible from the Internet. You will need the following firewall rules: Port 22 should be open to the Internet. Ports 9100 and 9101 should be open from your Prometheus instance. These will be used by the Prometheus node exporter and the ContainerSSH metrics server respectively. Outbound rules to your S3-compatible object storage. Step 3: Firewalling the sacrificial host \u00b6 The sacrificial host should not have any public Internet connectivity, instead it should only be connected to the gateway host. In order to keep this host up to date a prebuilt VM image with Docker installed should be used. The update process of this VM image can be automated using tools like Packer . On the firewall side, the sacrificial host should not allow any outbound connections and only allow inbound connections on TCP port 2376 from the gateway host. Step 4: Creating certificates for authentication on the sacrificial host \u00b6 The next step involves creating a CA infrastructure so ContainerSSH can authenticate against the Docker daemon on the sacrificial host. This is described in the Docker manual . Once your Docker socket is exposed you should test if it can be accessed without certificates. Running the following two commands from the gateway host without configuring the certificates should fail: docker run -H tcp://your-sacrificial-host:2375 -ti ubuntu docker run -H tcp://your-sacrificial-host:2376 -ti ubuntu If this command does not fail the certificates have not been set up correctly. Step 5: Installing the node exporter \u00b6 On the gateway host you will need to install the Prometheus node exporter to make metrics such as disk space usage available to your monitoring system. Please read their readme on how to do this . Step 6: Building the guest image \u00b6 Since our sacrificial host will have no internet access you will need to upload the guest image files . You can do this by exporting the image using docker export , then uploading the tar file to the host and using docker import to import it into the Docker daemon. Optionally, you can build your custom image and create an ubuntu user in the image to give the attacker a more realistic system. Step 7: Creating the ContainerSSH configuration file \u00b6 Finally, we can create the ContainerSSH configuration file on the gateway host. Let's create a few directories: mkdir -p /srv/containerssh/config/ mkdir -p /srv/containerssh/audit/ Then we generate the host key. This should be written in /srv/containerssh/ssh_host_rsa_key . openssl genrsa Then we can create the config file in /srv/containerssh/config.yaml log : level : warning ssh : banner : | ******************************************************************** Warning! ******************************************************************** This is a honeypot. All information, including IP address, username, password, any commands you type, or files you upload will be visible to the honeypot. If you do not agree disconnect now. ******************************************************************** hostkeys : - /etc/containerssh/ssh_host_rsa_key backend : docker docker : connection : host : tcp://SACRIFICIAL-HOST-IP:2376 cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- execution : imagePullPolicy : Never container : image : containerssh/test-guest hostname : bitcoin # Disable network in the container networkdisabled : true # Force running as user 1000 user : 1000 # Optionally set working directory workingdir : /home/ubuntu host : # Don't let the attacker write to the root FS. readonlyrootfs : true resources : # 10% of CPU cpuperiod : 10000 cpuquota : 1000 # 50 MB of memory with swap memoryswap : 52428800 memoryswappiness : 50 # 25 MB of memory memory : 26214400 # Reserve 20 MB of memory memoryreservation : 20000000 # Max 1000 processes to prevent fork bombs pidslimit : 1000 tmpfs : # Create writable directories in memory /tmp : rw,noexec,nosuid,size=65536k,uid=1000,gid=1000 /run : rw,noexec,nosuid,size=65536k,uid=1000,gid=1000 /home/ubuntu : rw,noexec,nosuid,size=65536k,uid=1000,gid=1000 metrics : enable : true listen : \"0.0.0.0:9101\" path : \"/metrics\" audit : enable : true format : binary storage : s3 intercept : stdin : true stdout : true stderr : true passwords : true s3 : # Local directory to store the audit log temporarily. local : /var/log/containerssh/audit/ accessKey : YOUR-S3-ACCESS-KEY-HERE secretKey : YOUR-S3-SECRET-KEY-HERE region : YOUR-S3-REGION bucket : YOUR-S3-BUCKET-NAME # Optional: set your S3 endpoint endpoint : https://YOUR-S3-ENDPOINT metadata : # Which metadata fields to set in the object storage. username : true ip : false auth : url : \"http://127.0.0.1:8080\" configserver : url : \"http://127.0.0.1:8080/config\" Step 7: Starting ContainerSSH \u00b6 Now you are ready to start ContainerSSH: docker run -d \\ --restart=always \\ -v /srv/containerssh/config/:/etc/containerssh/ \\ -v /srv/containerssh/audit/:/var/log/containerssh/audit/ \\ --net=host \\ containerssh/containerssh:0.4.1 Step 8: Starting the auth-config server \u00b6 Next, we'll need the auth-config server to let the users in: docker run -d \\ --restart=always \\ -p 127.0.0.1:8080:8080 \\ -e CONTAINERSSH_ALLOW_ALL=1 \\ containerssh/containerssh-test-authconfig:0.4.1 Step 9: Redirecting port 22 \u00b6 As a final step we will need to redirect port 22 to port 2222: iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222 You will need to use the firewall facilities of your OS to make this rule persistent. Step 10: Setting up monitoring \u00b6 Please set up monitoring for both the host metrics (such as disk space usage) and ContainerSSH itself in your Prometheus instance. Further hardening \u00b6 This creates a honeypot that lets attackers access a container. However, in a real world scenario you may want to integrate micro virtual machines instead of containers for better security, such as Firecracker . Alternatively, you may want to investigate tools like gVisor which implement a separate security layer for your container. This is beyond the scope of this guide.","title":"Honeypot"},{"location":"guides/honeypot/#creating-a-honeypot-with-containerssh","text":"This guide will lead you through the steps of creating an SSH honeypot with ContainerSSH. Danger Creating SSH honeypots with a real Linux backend is inherently dangerous. Any local privilege escalation could lead to the attacker taking over your host system. While this tutorial represents the best practices in building a honeypot, the responsibility of securing your installation ultimately rests upon you. Please do not attempt this unless you are intimately familiar with securing container environments. Docker has really good documentation on this topic.","title":"Creating a honeypot with ContainerSSH"},{"location":"guides/honeypot/#step-1-infrastructure","text":"In order to set up a honeypot securely you will need at least two hosts: one to run ContainerSSH and the second to run the container infrastructure the attacker is dropped into. We'll call the first host the gateway VM and the second one sacrificial VM. Ideally, the sacrificial VM should run on its own dedicated physical hardware to prevent leakage of secrets due to CPU bugs. Both VMs need sufficient disk space to hold audit logs and containers. Furthermore, you will need an S3-compatible object storage to upload audit logs and we will need a Prometheus installation for monitoring. We strongly recommend automating the setup with a tool like Terraform to rapidly apply security updates.","title":"Step 1: Infrastructure"},{"location":"guides/honeypot/#step-2-firewalling-the-gateway","text":"You should set up the gateway host in such a way that it is visible from the Internet. You will need the following firewall rules: Port 22 should be open to the Internet. Ports 9100 and 9101 should be open from your Prometheus instance. These will be used by the Prometheus node exporter and the ContainerSSH metrics server respectively. Outbound rules to your S3-compatible object storage.","title":"Step 2: Firewalling the gateway"},{"location":"guides/honeypot/#step-3-firewalling-the-sacrificial-host","text":"The sacrificial host should not have any public Internet connectivity, instead it should only be connected to the gateway host. In order to keep this host up to date a prebuilt VM image with Docker installed should be used. The update process of this VM image can be automated using tools like Packer . On the firewall side, the sacrificial host should not allow any outbound connections and only allow inbound connections on TCP port 2376 from the gateway host.","title":"Step 3: Firewalling the sacrificial host"},{"location":"guides/honeypot/#step-4-creating-certificates-for-authentication-on-the-sacrificial-host","text":"The next step involves creating a CA infrastructure so ContainerSSH can authenticate against the Docker daemon on the sacrificial host. This is described in the Docker manual . Once your Docker socket is exposed you should test if it can be accessed without certificates. Running the following two commands from the gateway host without configuring the certificates should fail: docker run -H tcp://your-sacrificial-host:2375 -ti ubuntu docker run -H tcp://your-sacrificial-host:2376 -ti ubuntu If this command does not fail the certificates have not been set up correctly.","title":"Step 4: Creating certificates for authentication on the sacrificial host"},{"location":"guides/honeypot/#step-5-installing-the-node-exporter","text":"On the gateway host you will need to install the Prometheus node exporter to make metrics such as disk space usage available to your monitoring system. Please read their readme on how to do this .","title":"Step 5: Installing the node exporter"},{"location":"guides/honeypot/#step-6-building-the-guest-image","text":"Since our sacrificial host will have no internet access you will need to upload the guest image files . You can do this by exporting the image using docker export , then uploading the tar file to the host and using docker import to import it into the Docker daemon. Optionally, you can build your custom image and create an ubuntu user in the image to give the attacker a more realistic system.","title":"Step 6: Building the guest image"},{"location":"guides/honeypot/#step-7-creating-the-containerssh-configuration-file","text":"Finally, we can create the ContainerSSH configuration file on the gateway host. Let's create a few directories: mkdir -p /srv/containerssh/config/ mkdir -p /srv/containerssh/audit/ Then we generate the host key. This should be written in /srv/containerssh/ssh_host_rsa_key . openssl genrsa Then we can create the config file in /srv/containerssh/config.yaml log : level : warning ssh : banner : | ******************************************************************** Warning! ******************************************************************** This is a honeypot. All information, including IP address, username, password, any commands you type, or files you upload will be visible to the honeypot. If you do not agree disconnect now. ******************************************************************** hostkeys : - /etc/containerssh/ssh_host_rsa_key backend : docker docker : connection : host : tcp://SACRIFICIAL-HOST-IP:2376 cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- execution : imagePullPolicy : Never container : image : containerssh/test-guest hostname : bitcoin # Disable network in the container networkdisabled : true # Force running as user 1000 user : 1000 # Optionally set working directory workingdir : /home/ubuntu host : # Don't let the attacker write to the root FS. readonlyrootfs : true resources : # 10% of CPU cpuperiod : 10000 cpuquota : 1000 # 50 MB of memory with swap memoryswap : 52428800 memoryswappiness : 50 # 25 MB of memory memory : 26214400 # Reserve 20 MB of memory memoryreservation : 20000000 # Max 1000 processes to prevent fork bombs pidslimit : 1000 tmpfs : # Create writable directories in memory /tmp : rw,noexec,nosuid,size=65536k,uid=1000,gid=1000 /run : rw,noexec,nosuid,size=65536k,uid=1000,gid=1000 /home/ubuntu : rw,noexec,nosuid,size=65536k,uid=1000,gid=1000 metrics : enable : true listen : \"0.0.0.0:9101\" path : \"/metrics\" audit : enable : true format : binary storage : s3 intercept : stdin : true stdout : true stderr : true passwords : true s3 : # Local directory to store the audit log temporarily. local : /var/log/containerssh/audit/ accessKey : YOUR-S3-ACCESS-KEY-HERE secretKey : YOUR-S3-SECRET-KEY-HERE region : YOUR-S3-REGION bucket : YOUR-S3-BUCKET-NAME # Optional: set your S3 endpoint endpoint : https://YOUR-S3-ENDPOINT metadata : # Which metadata fields to set in the object storage. username : true ip : false auth : url : \"http://127.0.0.1:8080\" configserver : url : \"http://127.0.0.1:8080/config\"","title":"Step 7: Creating the ContainerSSH configuration file"},{"location":"guides/honeypot/#step-7-starting-containerssh","text":"Now you are ready to start ContainerSSH: docker run -d \\ --restart=always \\ -v /srv/containerssh/config/:/etc/containerssh/ \\ -v /srv/containerssh/audit/:/var/log/containerssh/audit/ \\ --net=host \\ containerssh/containerssh:0.4.1","title":"Step 7: Starting ContainerSSH"},{"location":"guides/honeypot/#step-8-starting-the-auth-config-server","text":"Next, we'll need the auth-config server to let the users in: docker run -d \\ --restart=always \\ -p 127.0.0.1:8080:8080 \\ -e CONTAINERSSH_ALLOW_ALL=1 \\ containerssh/containerssh-test-authconfig:0.4.1","title":"Step 8: Starting the auth-config server"},{"location":"guides/honeypot/#step-9-redirecting-port-22","text":"As a final step we will need to redirect port 22 to port 2222: iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222 You will need to use the firewall facilities of your OS to make this rule persistent.","title":"Step 9: Redirecting port 22"},{"location":"guides/honeypot/#step-10-setting-up-monitoring","text":"Please set up monitoring for both the host metrics (such as disk space usage) and ContainerSSH itself in your Prometheus instance.","title":"Step 10: Setting up monitoring"},{"location":"guides/honeypot/#further-hardening","text":"This creates a honeypot that lets attackers access a container. However, in a real world scenario you may want to integrate micro virtual machines instead of containers for better security, such as Firecracker . Alternatively, you may want to investigate tools like gVisor which implement a separate security layer for your container. This is beyond the scope of this guide.","title":"Further hardening"},{"location":"guides/docker-elk/","text":"Logging to the ELK stack with Docker and Fluentd \u00b6 This guide will show you how you can set up logging from ContainerSSH to your ELK stack when running in Docker. To facilitate the log transport we will be using Fluentd . The source code of this guide is provided in our examples repository . We will be using docker-compose to deploy the elements of the setup inside our Docker engine. In order to follow this guide you will need a local Docker setup similar to that in the quick start guide . Step 1: Starting ContainerSSH \u00b6 As with the quick start guide, we will be starting ContainerSSH using docker-compose. To do this we will first create our ContainerSSH configuration called config.yaml : --- ssh : hostkeys : - /var/secrets/ssh_host_rsa_key auth : url : \"http://authconfig:8080\" configserver : url : \"http://authconfig:8080/config\" dockerrun : host : unix:///var/run/docker.sock log : level : debug Next we will create a Dockerfile for ContainerSSH. We need to do this because ContainerSSH runs as non-root by default, but we won't have access to the Docker socket like this. Hence, our Dockerfile is rather simple: FROM containerssh/containerssh:0.4.1 USER 0 Warning Do not use this for production. See the Docker reference manual for details how to harden your setup. Now we can create the SSH host keys. You can do this using OpenSSL by running openssl genrsa . For testing purposes you can use the dummy key from the example repo . The key should be saved as ssh_host_rsa_key . The final piece of the puzzle is creating the docker-compose.yaml file: --- version : '3.2' services : containerssh : build : . ports : - 127.0.0.1:2222:2222 volumes : - type : bind source : ./config.yaml target : /etc/containerssh/config.yaml - type : bind source : ./ssh_host_rsa_key target : /var/secrets/ssh_host_rsa_key - type : bind source : /var/run/docker.sock target : /var/run/docker.sock authconfig : image : containerssh/containerssh-test-authconfig:0.4.1 That's it, now we can start ContainerSSH using docker-compose up and log in using ssh foo@localhost -p 2222 with the password bar . Step 2: Adding Fluentd \u00b6 As a next step we will add Fluentd to our docker-compose.yaml and configure the ContainerSSH container to log to Fluentd. First, let's create a file called fluentd/conf/fluent.conf . We add the listening config: <source> @type forward port 24224 bind 0.0.0.0 </source> This will cause Fluentd to listen on port 24224 . We will configure Docker to send the logs here later. Next we'll add a filter to unpack the JSON log messages ContainerSSH sends: <filter containerssh.**> @type parser format json key_name log reserve_data true </filter> As a final piece we'll add forwarding to ElasticSearch: <match containerssh.**> @type elasticsearch host elasticsearch port 9200 logstash_format true </match> Note The container named elasticsearch will be started later. Now that we have the config ready we can create the Fluentd Dockerfile in the fluentd folder: FROM fluent/fluentd:v1.12.0-debian-1.0 USER root RUN [ \"gem\" , \"install\" , \"fluent-plugin-elasticsearch\" , \"--no-document\" , \"--version\" , \"5.0.1\" ] USER fluent Everything is in place, let's add Fluentd to our docker-compose.yaml : services : # ... fluentd : build : ./fluentd volumes : - ./fluentd/conf:/fluentd/etc ports : # We need to expose these ports to the host so the Docker engine can log to it. - \"127.0.0.1:24224:24224\" - \"127.0.0.1:24224:24224/udp\" And finally, let's change the ContainerSSH part of the same file to send logs to Fluentd: services : containerssh : #... logging : driver : fluentd options : # This address is from the perspective of the Docker daemon \"fluentd-address\" : \"127.0.0.1:24224\" # This is the tag we match in the Fluentd config. \"tag\" : \"containerssh.{{.ID}}\" depends_on : - fluentd Now everything is done and we can start the modified setup by running docker-compose build and then docker-compose up . Step 3: Starting the ELK stack \u00b6 For our test setup we'll start a single-node Elasticsearch and Kibana by adding them to our docker-compose.yaml : services : # ... elasticsearch : image : docker.elastic.co/elasticsearch/elasticsearch:7.10.2 container_name : elasticsearch environment : # We are running ElasticSearch in single-node mode. # Do we need to say this is not production ready? - \"discovery.type=single-node\" kibana : image : kibana:7.10.1 ports : - \"127.0.0.1:5601:5601\" depends_on : - elasticsearch Now we're all done so we can start the stack with docker-compose up . Step 4: Configuring Kibana \u00b6 When you first start Kibana you will need to configure Kibana. To do that you need to head to http://localhost:5601/app/management/kibana/indexPatterns/create to create a new index pattern. Please enter the following: logstash-* At this point you may get an error that this index isn't present. This is usually because there are no logs in Elasticsearch yet. This can easily be fixed by manually stopping and restarting the ContainerSSH container to generate some logs: docker stop <id of ContainerSSH container> && docker start <id of ContainerSSH container> On the next step Kibana will ask you for the timestamp field. You can use both @timestamp and timestamp , they will contain the same values. When the index is created you can head to http://localhost:5601/app/discover#/ and you should now see ContainerSSH logs in your Kibana. Congratulations!","title":"Logging to the ELK stack with Docker and Fluentd"},{"location":"guides/docker-elk/#logging-to-the-elk-stack-with-docker-and-fluentd","text":"This guide will show you how you can set up logging from ContainerSSH to your ELK stack when running in Docker. To facilitate the log transport we will be using Fluentd . The source code of this guide is provided in our examples repository . We will be using docker-compose to deploy the elements of the setup inside our Docker engine. In order to follow this guide you will need a local Docker setup similar to that in the quick start guide .","title":"Logging to the ELK stack with Docker and Fluentd"},{"location":"guides/docker-elk/#step-1-starting-containerssh","text":"As with the quick start guide, we will be starting ContainerSSH using docker-compose. To do this we will first create our ContainerSSH configuration called config.yaml : --- ssh : hostkeys : - /var/secrets/ssh_host_rsa_key auth : url : \"http://authconfig:8080\" configserver : url : \"http://authconfig:8080/config\" dockerrun : host : unix:///var/run/docker.sock log : level : debug Next we will create a Dockerfile for ContainerSSH. We need to do this because ContainerSSH runs as non-root by default, but we won't have access to the Docker socket like this. Hence, our Dockerfile is rather simple: FROM containerssh/containerssh:0.4.1 USER 0 Warning Do not use this for production. See the Docker reference manual for details how to harden your setup. Now we can create the SSH host keys. You can do this using OpenSSL by running openssl genrsa . For testing purposes you can use the dummy key from the example repo . The key should be saved as ssh_host_rsa_key . The final piece of the puzzle is creating the docker-compose.yaml file: --- version : '3.2' services : containerssh : build : . ports : - 127.0.0.1:2222:2222 volumes : - type : bind source : ./config.yaml target : /etc/containerssh/config.yaml - type : bind source : ./ssh_host_rsa_key target : /var/secrets/ssh_host_rsa_key - type : bind source : /var/run/docker.sock target : /var/run/docker.sock authconfig : image : containerssh/containerssh-test-authconfig:0.4.1 That's it, now we can start ContainerSSH using docker-compose up and log in using ssh foo@localhost -p 2222 with the password bar .","title":"Step 1: Starting ContainerSSH"},{"location":"guides/docker-elk/#step-2-adding-fluentd","text":"As a next step we will add Fluentd to our docker-compose.yaml and configure the ContainerSSH container to log to Fluentd. First, let's create a file called fluentd/conf/fluent.conf . We add the listening config: <source> @type forward port 24224 bind 0.0.0.0 </source> This will cause Fluentd to listen on port 24224 . We will configure Docker to send the logs here later. Next we'll add a filter to unpack the JSON log messages ContainerSSH sends: <filter containerssh.**> @type parser format json key_name log reserve_data true </filter> As a final piece we'll add forwarding to ElasticSearch: <match containerssh.**> @type elasticsearch host elasticsearch port 9200 logstash_format true </match> Note The container named elasticsearch will be started later. Now that we have the config ready we can create the Fluentd Dockerfile in the fluentd folder: FROM fluent/fluentd:v1.12.0-debian-1.0 USER root RUN [ \"gem\" , \"install\" , \"fluent-plugin-elasticsearch\" , \"--no-document\" , \"--version\" , \"5.0.1\" ] USER fluent Everything is in place, let's add Fluentd to our docker-compose.yaml : services : # ... fluentd : build : ./fluentd volumes : - ./fluentd/conf:/fluentd/etc ports : # We need to expose these ports to the host so the Docker engine can log to it. - \"127.0.0.1:24224:24224\" - \"127.0.0.1:24224:24224/udp\" And finally, let's change the ContainerSSH part of the same file to send logs to Fluentd: services : containerssh : #... logging : driver : fluentd options : # This address is from the perspective of the Docker daemon \"fluentd-address\" : \"127.0.0.1:24224\" # This is the tag we match in the Fluentd config. \"tag\" : \"containerssh.{{.ID}}\" depends_on : - fluentd Now everything is done and we can start the modified setup by running docker-compose build and then docker-compose up .","title":"Step 2: Adding Fluentd"},{"location":"guides/docker-elk/#step-3-starting-the-elk-stack","text":"For our test setup we'll start a single-node Elasticsearch and Kibana by adding them to our docker-compose.yaml : services : # ... elasticsearch : image : docker.elastic.co/elasticsearch/elasticsearch:7.10.2 container_name : elasticsearch environment : # We are running ElasticSearch in single-node mode. # Do we need to say this is not production ready? - \"discovery.type=single-node\" kibana : image : kibana:7.10.1 ports : - \"127.0.0.1:5601:5601\" depends_on : - elasticsearch Now we're all done so we can start the stack with docker-compose up .","title":"Step 3: Starting the ELK stack"},{"location":"guides/docker-elk/#step-4-configuring-kibana","text":"When you first start Kibana you will need to configure Kibana. To do that you need to head to http://localhost:5601/app/management/kibana/indexPatterns/create to create a new index pattern. Please enter the following: logstash-* At this point you may get an error that this index isn't present. This is usually because there are no logs in Elasticsearch yet. This can easily be fixed by manually stopping and restarting the ContainerSSH container to generate some logs: docker stop <id of ContainerSSH container> && docker start <id of ContainerSSH container> On the next step Kibana will ask you for the timestamp field. You can use both @timestamp and timestamp , they will contain the same values. When the index is created you can head to http://localhost:5601/app/discover#/ and you should now see ContainerSSH logs in your Kibana. Congratulations!","title":"Step 4: Configuring Kibana"},{"location":"reference/","text":"ContainerSSH Reference Manual The Reference Manual provides reference material for ContainerSSH 0.4 and is oriented towards system operators wishing to use ContainerSSH on their system. Introduction \u00b6 This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation. If you need a one minute primer on how ContainerSSH works please watch this video . Changes since ContainerSSH 0.4.0 \u00b6 ContainerSSH 0.4.1 is a bugfix release resolving several issues with the previous release. The reference manual for ContainerSSH 0.4.0 is available here . Incorrect handling of container configuration with Docker backend \u00b6 The 0.4.0 release introduced a bug discovered by a user which prevented setting the ContainerSSH image from the configuration server. The reason behind this failure was the incorrect JSON serialization. This release fixes the serialization and restores the correct way of operations. Incorrect handling of Kubernetes configuration \u00b6 Kubernetes uses a different YAML serialization library which lead to it being impossible to set volume parameters and potentially other options in the configuration file as discovered by a user . This release fixes this issue by using the Kubernetes YAML serialization for the Kubernetes configuration only. Incorrect handling of the password/pubkey options \u00b6 The previous version ignored the password and pubkey options in the authentication section and sent all requests to the authentication server regardless of the setting. This release fixes that and restores the function of these options to the way they worked in version 0.3.","title":"Overview"},{"location":"reference/#introduction","text":"This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation. If you need a one minute primer on how ContainerSSH works please watch this video .","title":"Introduction"},{"location":"reference/#changes-since-containerssh-040","text":"ContainerSSH 0.4.1 is a bugfix release resolving several issues with the previous release. The reference manual for ContainerSSH 0.4.0 is available here .","title":"Changes since ContainerSSH 0.4.0"},{"location":"reference/#incorrect-handling-of-container-configuration-with-docker-backend","text":"The 0.4.0 release introduced a bug discovered by a user which prevented setting the ContainerSSH image from the configuration server. The reason behind this failure was the incorrect JSON serialization. This release fixes the serialization and restores the correct way of operations.","title":"Incorrect handling of container configuration with Docker backend"},{"location":"reference/#incorrect-handling-of-kubernetes-configuration","text":"Kubernetes uses a different YAML serialization library which lead to it being impossible to set volume parameters and potentially other options in the configuration file as discovered by a user . This release fixes this issue by using the Kubernetes YAML serialization for the Kubernetes configuration only.","title":"Incorrect handling of Kubernetes configuration"},{"location":"reference/#incorrect-handling-of-the-passwordpubkey-options","text":"The previous version ignored the password and pubkey options in the authentication section and sent all requests to the authentication server regardless of the setting. This release fixes that and restores the function of these options to the way they worked in version 0.3.","title":"Incorrect handling of the password/pubkey options"},{"location":"reference/audit/","text":"Audit logging ContainerSSH contains an audit logging facility that can log every interaction happening over SSH. This functionality is disabled by default as it has serious security and privacy implications, as well as severe resource requirements. Audit logging can be enabled in the configuration using the following structure: audit : enable : true format : none|binary|asciinema # Which format to log in. Defaults to none. storage : none|s3|file # Where to write audit log. Defaults to none. intercept : stdin : true|false # Intercept keystrokes from user stdout : true|false # Intercept standard output stderr : true|false # Intercept standard error passwords : true|false # Intercept passwords during authentication Audit logging is a powerful tool. It can capture the following events. Connections Authentication attempts, optionally with credentials Global and channel-specific SSH requests Programs launched from SSH Input from the user (optional) Output and errors to the user (optional) The events recorded depend on the chosen format. With the audit format all information is recorded with nanosecond timing, so events can be accurately reconstructed after the fact. About interceptions \u00b6 The intercept options give you a wide range of options when it comes to detailed logging of actions by users. You may want to, for example, enable stdout logging while keeping stdin disabled to avoid accidentally capturing passwords typed into the console. However, this approach may fail if SFTP is enabled as you will fail to capture binaries uploaded to the server. Audit logging should therefore be enjoyed with great care and the logs should always be stored on an encrypted storage device. Log formats \u00b6 The binary format (recommended) \u00b6 The binary format is intended for an accurate reconstruction of everything happening during an SSH session. It allows for accurate reconstruction of what happened during the session. Audit logs are stored in a compressed binary format and can be decoded to a series of JSON messages using the containerssh-auditlog-decoder supplied as part of the ContainerSSH release. Alternatively, you can implement your own decoder . We are providing a Go library for decoding audit log messages . This format can be decoded using the containerssh-auditlog-decoder application supplied with ContainerSSH. The asciinema format \u00b6 The asciinema format stores logs in a format suitable for replay in the Asciinema player . Note Make sure you enable the stdout and stderr interceptions otherwise the asciinema encoder won't capture anything. Warning Asciinema is intended for entertainment purposes only and doesn't store all relevant information required for an accurate audit log. Storage backends \u00b6 The s3 storage (recommended) \u00b6 The S3 storage sends the logs to an S3-compatible object storage for long term storage. This is the recommended way of storing audit logs because it is a server-independent storage device that supports permissions. The S3 storage stores the logs in a local directory and uploads them once an upload part is full (default: 5MB) or the connection closes. If the upload fails, ContainerSSH will retry the upload as soon as possible. If ContainerSSH is stopped and restarted it will attempt to upload the audit logs still in the local directory, but no guarantee is made that these logs will not be corrupt after a crash. Warning The local directory should be stored on a persistent storage and must not be shared between ContainerSSH instances. It must be large enough to host all sessions in their entirety that are currently running. When IO interception is enabled and your users are downloading or uploading large amounts of data this can run you up to several GB of storage needed locally. We recommend turning off IO interception for cases where large amounts of data are being transferred. The S3 storage can be configured as follows: audit : ... storage : s3 s3 : local : /local/storage/directory accessKey : \"your-access-key-here\" secretKey : \"your-secret-key-here\" bucket : \"your-existing-bucket-name-here\" region : \"your-region-name-here\" endpoint : \"https://your-custom-s3-url\" # Optional uploadPartSize : 5242880 # In bytes, min: 5MB, max: 5GB acl : \"public-read\" # Optional, in case you want to set an ACL metadata : username : true # Expose username via S3 metadata. Defaults to false. ip : true # Expose IP address via S3 metadata. Defaults to false. cacert : | # Optional Your trusted CA certificate in PEM format here for your S3 server. Tip You can restrict the access key permissions to PutObject , CreateMultipartUpload , UploadPart , CompleteMultipartUpload , ListMultipartUploads , and AbortMultipartUpload . Other permissions are not required. Tip You may also want to investigate if your S3 provider supports WORM / object locking, object lifecycles, or server side encryption for compliance. The file storage \u00b6 The file storage writes audit logs to files on the disk. The storage location can be configured using the following option: audit : type : file file : directory : /var/log/audit","title":"Audit logging"},{"location":"reference/audit/#about-interceptions","text":"The intercept options give you a wide range of options when it comes to detailed logging of actions by users. You may want to, for example, enable stdout logging while keeping stdin disabled to avoid accidentally capturing passwords typed into the console. However, this approach may fail if SFTP is enabled as you will fail to capture binaries uploaded to the server. Audit logging should therefore be enjoyed with great care and the logs should always be stored on an encrypted storage device.","title":"About interceptions"},{"location":"reference/audit/#log-formats","text":"","title":"Log formats"},{"location":"reference/audit/#the-binary-format-recommended","text":"The binary format is intended for an accurate reconstruction of everything happening during an SSH session. It allows for accurate reconstruction of what happened during the session. Audit logs are stored in a compressed binary format and can be decoded to a series of JSON messages using the containerssh-auditlog-decoder supplied as part of the ContainerSSH release. Alternatively, you can implement your own decoder . We are providing a Go library for decoding audit log messages . This format can be decoded using the containerssh-auditlog-decoder application supplied with ContainerSSH.","title":"The binary format (recommended)"},{"location":"reference/audit/#the-asciinema-format","text":"The asciinema format stores logs in a format suitable for replay in the Asciinema player . Note Make sure you enable the stdout and stderr interceptions otherwise the asciinema encoder won't capture anything. Warning Asciinema is intended for entertainment purposes only and doesn't store all relevant information required for an accurate audit log.","title":"The asciinema format"},{"location":"reference/audit/#storage-backends","text":"","title":"Storage backends"},{"location":"reference/audit/#the-s3-storage-recommended","text":"The S3 storage sends the logs to an S3-compatible object storage for long term storage. This is the recommended way of storing audit logs because it is a server-independent storage device that supports permissions. The S3 storage stores the logs in a local directory and uploads them once an upload part is full (default: 5MB) or the connection closes. If the upload fails, ContainerSSH will retry the upload as soon as possible. If ContainerSSH is stopped and restarted it will attempt to upload the audit logs still in the local directory, but no guarantee is made that these logs will not be corrupt after a crash. Warning The local directory should be stored on a persistent storage and must not be shared between ContainerSSH instances. It must be large enough to host all sessions in their entirety that are currently running. When IO interception is enabled and your users are downloading or uploading large amounts of data this can run you up to several GB of storage needed locally. We recommend turning off IO interception for cases where large amounts of data are being transferred. The S3 storage can be configured as follows: audit : ... storage : s3 s3 : local : /local/storage/directory accessKey : \"your-access-key-here\" secretKey : \"your-secret-key-here\" bucket : \"your-existing-bucket-name-here\" region : \"your-region-name-here\" endpoint : \"https://your-custom-s3-url\" # Optional uploadPartSize : 5242880 # In bytes, min: 5MB, max: 5GB acl : \"public-read\" # Optional, in case you want to set an ACL metadata : username : true # Expose username via S3 metadata. Defaults to false. ip : true # Expose IP address via S3 metadata. Defaults to false. cacert : | # Optional Your trusted CA certificate in PEM format here for your S3 server. Tip You can restrict the access key permissions to PutObject , CreateMultipartUpload , UploadPart , CompleteMultipartUpload , ListMultipartUploads , and AbortMultipartUpload . Other permissions are not required. Tip You may also want to investigate if your S3 provider supports WORM / object locking, object lifecycles, or server side encryption for compliance.","title":"The s3 storage (recommended)"},{"location":"reference/audit/#the-file-storage","text":"The file storage writes audit logs to files on the disk. The storage location can be configured using the following option: audit : type : file file : directory : /var/log/audit","title":"The file storage"},{"location":"reference/auth/","text":"Authentication ContainerSSH does not know your users and their passwords. Therefore, it calls out to a microservice that you have to provide. Your service can verify the users, passwords, and SSH keys. You will have to provide the microservice URL in the configuration. Configuration \u00b6 The authentication webhook can be configured in the main configuration using the following structure: auth : <options> The following options are supported: Name Type Description password bool Enable password authentication. pubkey bool Enable public key authentication. url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. authTimeout string Timeout for the authentication process. HTTP calls that result in a non-200 response call will be retried until this timeout is reached. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion []string Minimum TLS version to support. See the TLS version section below. curve []string Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. cipher []string,string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false. Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Server certificates must use Subject Alternative Names (SAN's) for proper server verification. Elliptic curve algorithms \u00b6 The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1 Cipher suites \u00b6 The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Tip Cipher suites can be provided as a list or as a colon ( : ) separated string. Client authentication \u00b6 In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem The authentication webhook \u00b6 The authentication webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create an authentication server. Warning A warning about rate limiting: if the authentication server desires to do rate limiting for connecting users it should take into account that a user is allowed to try multiple authentication attempts (currently hard-coded to 6 per connection) before they are disconnected. Some of the authentication attempts (e.g. public keys) happen automatically on the client side without the user having any influence on them. Furthermore, ContainerSSH retries failed HTTP calls. To be effective the authentication server should count the unique connection identifiers seen in the connectionId field and implement a lock-out based on these. Password authentication \u00b6 On password authentication the authentication server will receive the following request to the /password endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" } Public key authentication \u00b6 On public key authentication the authentication server will receive the following request to the /pubkey endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"publicKey\" : \"ssh-rsa ...\" } The public key will be sent in the authorized key format. Response \u00b6 Both endpoints need to respond with an application/json response of the following content: { \"success\" : true } Tip We provide a Go library to implement a authentication server .","title":"Authentication"},{"location":"reference/auth/#configuration","text":"The authentication webhook can be configured in the main configuration using the following structure: auth : <options> The following options are supported: Name Type Description password bool Enable password authentication. pubkey bool Enable public key authentication. url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. authTimeout string Timeout for the authentication process. HTTP calls that result in a non-200 response call will be retried until this timeout is reached. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion []string Minimum TLS version to support. See the TLS version section below. curve []string Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. cipher []string,string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false.","title":"Configuration"},{"location":"reference/auth/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/auth/#tls-version","text":"The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Server certificates must use Subject Alternative Names (SAN's) for proper server verification.","title":"TLS version"},{"location":"reference/auth/#elliptic-curve-algorithms","text":"The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1","title":"Elliptic curve algorithms"},{"location":"reference/auth/#cipher-suites","text":"The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Tip Cipher suites can be provided as a list or as a colon ( : ) separated string.","title":"Cipher suites"},{"location":"reference/auth/#client-authentication","text":"In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem","title":"Client authentication"},{"location":"reference/auth/#the-authentication-webhook","text":"The authentication webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create an authentication server. Warning A warning about rate limiting: if the authentication server desires to do rate limiting for connecting users it should take into account that a user is allowed to try multiple authentication attempts (currently hard-coded to 6 per connection) before they are disconnected. Some of the authentication attempts (e.g. public keys) happen automatically on the client side without the user having any influence on them. Furthermore, ContainerSSH retries failed HTTP calls. To be effective the authentication server should count the unique connection identifiers seen in the connectionId field and implement a lock-out based on these.","title":"The authentication webhook"},{"location":"reference/auth/#password-authentication","text":"On password authentication the authentication server will receive the following request to the /password endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" }","title":"Password authentication"},{"location":"reference/auth/#public-key-authentication","text":"On public key authentication the authentication server will receive the following request to the /pubkey endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"publicKey\" : \"ssh-rsa ...\" } The public key will be sent in the authorized key format.","title":"Public key authentication"},{"location":"reference/auth/#response","text":"Both endpoints need to respond with an application/json response of the following content: { \"success\" : true } Tip We provide a Go library to implement a authentication server .","title":"Response"},{"location":"reference/backends/","text":"Backend selection ContainerSSH is built to support multiple backends. The backend can be changed in the configuration file: backend : <backend type> ContainerSSH currently supports the following backends: Backend Description docker Runs Docker containers. kubernetes Runs Kubernetes containers. sshproxy Forwards SSH connections to a backend server. dockerrun Deprecated backend that runs Docker containers. kuberrun Deprecated backend that runs Kubernetes pods.","title":"Backend selection"},{"location":"reference/codes/","text":"Message codes \u00b6 This page contains all message codes logged by ContainerSSH. Some of these are errors, while others only give you status information. Many of these messages are only logged when the log level is set to debug . Core \u00b6 Code Explanation CORE_CONFIG_CANNOT_WRITE_FILE ContainerSSH cannot update the configuration file with the new host keys and will only use the host key for the current run. CORE_CONFIG_ERROR ContainerSSH encountered an error in the configuration. CORE_CONFIG_FILE ContainerSSH is reading the configuration file CORE_HOST_KEY_GENERATION_FAILED ContainerSSH could not generate host keys and is aborting the run. CORE_NO_HOST_KEYS The configuration does not contain host keys. ContainerSSH will attempt to generate host keys and update the configuration file. Auditlog \u00b6 Code Explanation AUDIT_S3_CANNOT_CLOSE_METADATA_FILE_HANDLE ContainerSSH could not close the metadata file in the local folder. This typically happens when the local folder is on an NFS share. (This is NOT supported.) AUDIT_S3_CLOSE_FAILED ContainerSSH failed to close an audit log file in the local directory. This usually happens when the local directory is on an NFS share. (This is NOT supported.) AUDIT_S3_FAILED_CREATING_METADATA_FILE ContainerSSH failed to create the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is writable and has enough disk space. AUDIT_S3_FAILED_METADATA_JSON_ENCODING ContainerSSH failed to encode the metadata file. This is a bug, please report it. AUDIT_S3_FAILED_READING_METADATA_FILE ContainerSSH failed to read the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is readable and the files have not been corrupted. AUDIT_S3_FAILED_STAT_QUEUE_ENTRY ContainerSSH failed to stat the queue file. This usually happens when the local directory is being manually manipulated. AUDIT_S3_FAILED_WRITING_METADATA_FILE ContainerSSH failed to write the local metadata file. Please check if your disk has enough disk space. AUDIT_S3_MULTIPART_ABORTING ContainerSSH is aborting a multipart upload. Check the log message for details. AUDIT_S3_MULTIPART_FAILED_ABORT ContainerSSH failed aborting a multipart upload from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_FAILED_LIST ContainerSSH failed to list multipart uploads on the object storage bucket. This is needed to abort uploads from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_PART_UPLOADING ContainerSSH is uploading a part of an audit log to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_COMPLETE ContainerSSH completed the upload of an audit log part to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_FAILED ContainerSSH failed to upload a part to the S3-compatible object storage. Check the message for details. AUDIT_S3_MULTIPART_UPLOAD ContainerSSH is starting a new S3 multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZATION_FAILED ContainerSSH has uploaded all audit log parts, but could not finalize the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZED ContainerSSH has uploaded all audit log parts and has successfully finalized the upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZING ContainerSSH has uploaded all audit log parts and is now finalizing the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_INITIALIZATION_FAILED ContainerSSH failed to initialize a new multipart upload to the S3-compatible object storage. Check if the S3 configuration is correct and the provided S3 access key and secrets have permissions to start a multipart upload. AUDIT_S3_NO_SUCH_QUEUE_ENTRY ContainerSSH was trying to upload an audit log from the metadata file, but the audit log does not exist. AUDIT_S3_RECOVERING ContainerSSH found a previously aborted multipart upload locally and is now attempting to recover the upload. AUDIT_S3_REMOVE_FAILED ContainerSSH failed to remove an uploaded audit log from the local directory. This usually happens on Windows when a different process has the audit log open. (This is not a supported setup.) AUDIT_S3_SINGLE_UPLOAD ContainerSSH is uploading the full audit log in a single upload to the S3-compatible object storage. This happens when the audit log size is below the minimum size for a multi-part upload. AUDIT_S3_SINGLE_UPLOAD_COMPLETE ContainerSSH successfully uploaded the audit log as a single upload. AUDIT_S3_SINGLE_UPLOAD_FAILED ContainerSSH failed to upload the audit log as a single upload. Authentication \u00b6 Code Explanation AUTH ContainerSSH is trying to contact the authentication backend to verify the user credentials. AUTH_BACKEND_ERROR The ContainerSSH authentication server responded with a non-200 status code. ContainerSSH will retry the authentication for a few times before giving up. This is most likely a bug in your authentication server, please check your logs. AUTH_FAILED The user has provided invalid credentials and the authentication is rejected. AUTH_INVALID_STATUS This message indicates that the authentication server returned an invalid HTTP status code. AUTH_NOT_SUPPORTED The authentication method the client requested is not supported by ContainerSSH. AUTH_SUCCESSFUL The user has provided the correct credentials and the authentication is accepted. Backend \u00b6 Code Explanation BACKEND_CONFIG_ERROR The backend retreived from the configuration server is invalid. See the error message for details. Configuration \u00b6 Code Explanation CONFIG_BACKEND_ERROR ContainerSSH has received an invalid response from the configuration server or the network connection broke. ContainerSSH will retry fetching the user-specific configuration until the timeout. If this error persists check the connectivity to the configuration server, or the logs of the configuration server itself to find out of there may be a specific error. CONFIG_INVALID_STATUS_CODE ContainerSSH has received a non-200 response code when calling a per-user backend configuration from the configuration server. CONFIG_REQUEST ContainerSSH is sending a quest to the configuration server to obtain a per-user backend configuration. CONFIG_RESPONSE ContainerSSH has received a per-user backend configuration from the configuration server. CONFIG_SERVER_AVAILABLE The ContainerSSH configuration server is now available at the specified address. Docker \u00b6 Code Explanation DOCKER_AGENT_READ_FAILED The ContainerSSH Docker module failed to read from the ContainerSSH agent. This is most likely because the ContainerSSH guest agent is not present in the guest image, but agent support is enabled. DOCKER_CLOSE_INPUT_FAILED The ContainerSSH Docker module attempted to close the input (stdin) for reading but failed to do so. DOCKER_CLOSE_OUTPUT_FAILED The ContainerSSH Docker module attempted to close the output (stdout and stderr) for writing but failed to do so. DOCKER_CONFIG_ERROR The ContainerSSH Docker module detected a configuration error. Please check your configuration. DOCKER_CONTAINER_ATTACH The ContainerSSH Docker module is attaching to a container in session mode. DOCKER_CONTAINER_ATTACH_FAILED The ContainerSSH Docker module has failed to attach to a container in session mode. DOCKER_CONTAINER_CREATE The ContainerSSH Docker module is creating a container. DOCKER_CONTAINER_CREATE_FAILED The ContainerSSH Docker module failed to create a container. This may be a temporary and retried or a permanent error message. Check the log message for details. DOCKER_CONTAINER_REMOVE The ContainerSSH Docker module os removing the container. DOCKER_CONTAINER_REMOVE_FAILED The ContainerSSH Docker module could not remove the container. This message may be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_REMOVE_SUCCESSFUL The ContainerSSH Docker module has successfully removed the container. DOCKER_CONTAINER_SHUTTING_DOWN The ContainerSSH Docker module is shutting down a container. DOCKER_CONTAINER_SIGNAL The ContainerSSH Docker module is sending a signal to the container. DOCKER_CONTAINER_SIGNAL_FAILED The ContainerSSH Docker module has failed to send a signal to the container. DOCKER_CONTAINER_START The ContainerSSH Docker module is starting the previously-created container. DOCKER_CONTAINER_START_FAILED The ContainerSSH docker module failed to start the container. This message can either be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_STOP The ContainerSSH Docker module is stopping the container. DOCKER_CONTAINER_STOP_FAILED The ContainerSSH Docker module failed to stop the container. This message can be either temporary and retried or permanent. Check the log message for details. DOCKER_EXEC The ContainerSSH Docker module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the container. DOCKER_EXEC_ATTACH The ContainerSSH Docker module is attaching to the previously-created execution. DOCKER_EXEC_ATTACH_FAILED The ContainerSSH Docker module could not attach to the previously-created execution. DOCKER_EXEC_CREATE The ContainerSSH Docker module is creating an execution. DOCKER_EXEC_CREATE_FAILED The ContainerSSH Docker module has failed to create an execution. This can be temporary and retried or permanent. See the error message for details. DOCKER_EXEC_PID_READ_FAILED The ContainerSSH Docker module has failed to read the process ID from the ContainerSSH Guest Agent . This is most likely because the guest image does not contain the guest agent, but guest agent support has been enabled. DOCKER_EXEC_RESIZE The ContainerSSH Docker module is resizing the console. DOCKER_EXEC_RESIZE_FAILED The ContainerSSH Docker module failed to resize the console. DOCKER_EXEC_SIGNAL The ContainerSSH Docker module is delivering a signal in container mode. DOCKER_EXEC_SIGNAL_FAILED The ContainerSSH Docker module failed to deliver a signal. DOCKER_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Docker module failed to deliver a signal because ContainerSSH Guest Agent support is disabled. DOCKER_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Docker module successfully delivered the requested signal. DOCKER_EXIT_CODE The ContainerSSH Docker module is fetching the exit code from the program. DOCKER_EXIT_CODE_CONTAINER_RESTARTING The ContainerSSH Docker module could not fetch the exit code from the program because the container is restarting. This is typically a misconfiguration as ContainerSSH containers should not automatically restart. DOCKER_EXIT_CODE_FAILED The ContainerSSH Docker module has failed to fetch the exit code of the program. DOCKER_EXIT_CODE_NEGATIVE The ContainerSSH Docker module has received a negative exit code from Docker. This should never happen and is most likely a bug. DOCKER_EXIT_CODE_STILL_RUNNING The ContainerSSH Docker module could not fetch the program exit code because the program is still running. This error may be temporary and retried or permanent. DOCKER_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the container image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. DOCKER_IMAGE_LISTING The ContainerSSH Docker module is listing the locally present container images to determine if the specified container image needs to be pulled. DOCKER_IMAGE_LISTING_FAILED The ContainerSSH Docker module failed to list the images present in the local Docker daemon. This is used to determine if the image needs to be pulled. This can be because the Docker daemon is not reachable, the certificate is invalid, or there is something else interfering with listing the images. DOCKER_IMAGE_PULL The ContainerSSH Docker module is pulling the container image. DOCKER_IMAGE_PULL_FAILED The ContainerSSH Docker module failed to pull the specified container image. This can be because of connection issues to the Docker daemon, or because the Docker daemon itself can't pull the image. If you don't intend to have the image pulled you should set the ImagePullPolicy to Never . See the Docker documentation for details. DOCKER_IMAGE_PULL_NEEDED_CHECKING The ContainerSSH Docker module is checking if an image pull is needed. DOCKER_PROGRAM_ALREADY_RUNNING The ContainerSSH Docker module can't execute the request because the program is already running. This is a client error. DOCKER_SIGNAL_FAILED_NO_PID The ContainerSSH Docker module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. DOCKER_STREAM_INPUT_FAILED The ContainerSSH Docker module failed to stream stdin to the Docker engine. DOCKER_STREAM_OUTPUT_FAILED The ContainerSSH Docker module failed to stream stdout and stderr from the Docker engine. DOCKER_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Docker module is not configured to run the requested subsystem. HTTP \u00b6 Code Explanation HTTP_CLIENT_CONNECTION_FAILED This message indicates a connection failure on the network level. HTTP_CLIENT_DECODE_FAILED This message indicates that decoding the JSON response has failed. The status code is set for this code. HTTP_CLIENT_ENCODE_FAILED This message indicates that JSON encoding the request failed. This is usually a bug. HTTP_CLIENT_REDIRECT This message indicates that the server responded with a HTTP redirect. HTTP_CLIENT_REDIRECTS_DISABLED This message indicates that ContainerSSH is not following a HTTP redirect sent by the server. Use the allowRedirects option to allow following HTTP redirects. HTTP_CLIENT_REQUEST This message indicates that a HTTP request is being sent from ContainerSSH HTTP_CLIENT_RESPONSE This message indicates that ContainerSSH received a HTTP response from a server. HTTP_SERVER_ENCODE_FAILED The HTTP server failed to encode the response object. This is a bug, please report it. HTTP_SERVER_RESPONSE_WRITE_FAILED The HTTP server failed to write the response. Kubernetes \u00b6 Code Explanation KUBERNETES_CLOSE_OUTPUT_FAILED The ContainerSSH Kubernetes module attempted to close the output (stdout and stderr) for writing but failed to do so. KUBERNETES_CONFIG_ERROR The ContainerSSH Kubernetes module detected a configuration error. Please check your configuration. KUBERNETES_EXEC The ContainerSSH Kubernetes module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the pod. KUBERNETES_EXEC_RESIZE The ContainerSSH Kubernetes module is resizing the terminal window. KUBERNETES_EXEC_RESIZE_FAILED The ContainerSSH Kubernetes module failed to resize the console. KUBERNETES_EXEC_SIGNAL The ContainerSSH Kubernetes module is delivering a signal. KUBERNETES_EXEC_SIGNAL_FAILED The ContainerSSH Kubernetes module failed to deliver a signal. KUBERNETES_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Kubernetes module failed to deliver a signal because guest agent support is disabled. KUBERNETES_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Kubernetes module successfully delivered the requested signal. KUBERNETES_EXIT_CODE_FAILED The ContainerSSH Kubernetes module has failed to fetch the exit code of the program. KUBERNETES_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the pod image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. KUBERNETES_PID_RECEIVED The ContainerSSH Kubernetes module has received a PID from the Kubernetes guest agent. KUBERNETES_POD_ATTACH The ContainerSSH Kubernetes module is attaching to a pod in session mode. KUBERNETES_POD_CREATE The ContainerSSH Kubernetes module is creating a pod. KUBERNETES_POD_CREATE_FAILED The ContainerSSH Kubernetes module failed to create a pod. This may be a temporary and retried or a permanent error message. Check the log message for details. KUBERNETES_POD_REMOVE The ContainerSSH Kubernetes module is removing a pod. KUBERNETES_POD_REMOVE_FAILED The ContainerSSH Kubernetes module could not remove the pod. This message may be temporary and retried or permanent. Check the log message for details. KUBERNETES_POD_REMOVE_SUCCESSFUL The ContainerSSH Kubernetes module has successfully removed the pod. KUBERNETES_POD_SHUTTING_DOWN The ContainerSSH Kubernetes module is shutting down a pod. KUBERNETES_POD_WAIT The ContainerSSH Kubernetes module is waiting for the pod to come up. KUBERNETES_POD_WAIT_FAILED The ContainerSSH Kubernetes module failed to wait for the pod to come up. Check the error message for details. KUBERNETES_PROGRAM_ALREADY_RUNNING The ContainerSSH Kubernetes module can't execute the request because the program is already running. This is a client error. KUBERNETES_PROGRAM_NOT_RUNNING This message indicates that the user requested an action that can only be performed when a program is running, but there is currently no program running. KUBERNETES_SIGNAL_FAILED_EXITED The ContainerSSH Kubernetes module can't deliver a signal because the program already exited. KUBERNETES_SIGNAL_FAILED_NO_PID The ContainerSSH Kubernetes module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. KUBERNETES_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Kubernetes module is not configured to run the requested subsystem. KUBERUN_DEPRECATED This message indicates that you are still using the deprecated KubeRun backend. This backend doesn't support all safety and functionality improvements and will be removed in the future. Please read the deprecation notice for a migration guide KUBERUN_EXEC_DISABLED This message indicates that the user tried to execute a program, but program execution is disabled in the legacy KubeRun configuration. KUBERUN_INSECURE This message indicates that you are using Kubernetes in the \"insecure\" mode where certificate verification is disabled. This is a major security flaw, has been deprecated and is removed in the new Kubernetes backend. Please change your configuration to properly validates the server certificates. Log \u00b6 Code Explanation LOG_FILE_OPEN_FAILED ContainerSSH failed to open the specified log file. LOG_ROTATE_FAILED ContainerSSH cannot rotate the logs as requested because of an underlying error. LOG_WRITE_FAILED ContainerSSH cannot write to the specified log file. This usually happens because the underlying filesystem is full or the log is located on a non-local storage (e.g. NFS), which is not supported. TEST This is message that should only be seen in unit and component tests, never in production. UNKNOWN_ERROR This is an untyped error. If you see this in a log that is a bug and should be reported. Metrics \u00b6 Code Explanation METRICS_AVAILABLE The metrics service is now online and ready for service. Security \u00b6 Code Explanation SECURITY_ENV_REJECTED ContainerSSH rejected setting the environment variable because it does not pass the security settings. SECURITY_EXEC_FAILED_SETENV Program execution failed in conjunction with the forceCommand option because ContainerSSH could not set the SSH_ORIGINAL_COMMAND environment variable on the backend. SECURITY_EXEC_FORCING_COMMAND ContainerSSH is replacing the command passed from the client (if any) to the specified command and is setting the SSH_ORIGINAL_COMMAND environment variable. SECURITY_EXEC_REJECTED A program execution request has been rejected because it doesn't conform to the security settings. SECURITY_SHELL_REJECTED ContainerSSH rejected launching a shell due to the security settings. SECURITY_SIGNAL_REJECTED ContainerSSH rejected delivering a signal because it does not pass the security settings. SECURITY_SUBSYSTEM_REJECTED ContainerSSH rejected the subsystem because it does pass the security settings. SECURITY_TTY_REJECTED ContainerSSH rejected the pseudoterminal request because of the security settings. Service \u00b6 Code Explanation SERVICE_CRASHED A ContainerSSH has stopped improperly. SERVICE_POOL_RUNNING All ContainerSSH services are now running. SERVICE_POOL_STARTING All ContainerSSH services are starting. SERVICE_POOL_STOPPED ContainerSSH has stopped all services. SERVICE_POOL_STOPPING ContainerSSH is stopping all services. SERVICE_RUNNING A ContainerSSH service is now running SERVICE_STARTING ContainerSSH is starting a component service SERVICE_STOPPED A ContainerSSH service has stopped. SERVICE_STOPPING A ContainerSSH service is now stopping. SSH \u00b6 Code Explanation SSH_ALREADY_RUNNING The SSH server is already running and has been started again. This is a bug, please report it. SSH_AUTH_FAILED The user has provided invalid credentials. SSH_AUTH_SUCCESSFUL The user has provided valid credentials and is now authenticated. SSH_AUTH_UNAVAILABLE The user has requested an authentication method that is currently unavailable. SSH_AVAILABLE The SSH service is now online and ready for service. SSH_BACKEND_REJECTED_HANDSHAKE The backend has rejected the connecting user after successful authentication. SSH_CHANNEL_REQUEST The user has send a new channel-specific request. SSH_CHANNEL_REQUEST_FAILED ContainerSSH couldn't fulfil the channel-specific request. SSH_CHANNEL_REQUEST_SUCCESSFUL ContainerSSH has successfully processed the channel-specific request. SSH_CONNECTED A user has connected over SSH. SSH_DECODE_FAILED ContainerSSH failed to decode something from the user. This is either a bug in ContainerSSH or in the connecting client. SSH_DISCONNECTED An SSH connection has been severed. SSH_EXIT ContainerSSH is sending the exit code of the program to the user. SSH_EXIT_CODE_FAILED ContainerSSH failed to obtain and send the exit code of the program to the user. SSH_EXIT_SIGNAL ContainerSSH is sending the exit signal from an abnormally exited program to the user. SSH_HANDSHAKE_FAILED The connecting party failed to establish a secure SSH connection. This is most likely due to invalid credentials or a backend error. SSH_HANDSHAKE_SUCCESSFUL The user has provided valid credentials and has now established an SSH connection. SSH_LISTEN_CLOSE_FAILED ContainerSSH failed to close the listen socket. SSH_NEW_CHANNEL A user has established a new SSH channel. (Not connection!) SSH_NEW_CHANNEL_REJECTED The user has requested a new channel to be opened, but was rejected. SSH_REPLY_SEND_FAILED ContainerSSH couldn't send the reply to a request to the user. This is usually the case if the user suddenly disconnects. SSH_START_FAILED ContainerSSH failed to start the SSH service. This is usually because of invalid configuration. SSH_UNSUPPORTED_CHANNEL_TYPE The user requested a channel type that ContainerSSH doesn't support (e.g. TCP/IP forwarding). SSH_UNSUPPORTED_GLOBAL_REQUEST The users client has send a global request ContainerSSH does not support. This is nothing to worry about.","title":"Message codes"},{"location":"reference/codes/#message-codes","text":"This page contains all message codes logged by ContainerSSH. Some of these are errors, while others only give you status information. Many of these messages are only logged when the log level is set to debug .","title":"Message codes"},{"location":"reference/codes/#core","text":"Code Explanation CORE_CONFIG_CANNOT_WRITE_FILE ContainerSSH cannot update the configuration file with the new host keys and will only use the host key for the current run. CORE_CONFIG_ERROR ContainerSSH encountered an error in the configuration. CORE_CONFIG_FILE ContainerSSH is reading the configuration file CORE_HOST_KEY_GENERATION_FAILED ContainerSSH could not generate host keys and is aborting the run. CORE_NO_HOST_KEYS The configuration does not contain host keys. ContainerSSH will attempt to generate host keys and update the configuration file.","title":"Core"},{"location":"reference/codes/#auditlog","text":"Code Explanation AUDIT_S3_CANNOT_CLOSE_METADATA_FILE_HANDLE ContainerSSH could not close the metadata file in the local folder. This typically happens when the local folder is on an NFS share. (This is NOT supported.) AUDIT_S3_CLOSE_FAILED ContainerSSH failed to close an audit log file in the local directory. This usually happens when the local directory is on an NFS share. (This is NOT supported.) AUDIT_S3_FAILED_CREATING_METADATA_FILE ContainerSSH failed to create the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is writable and has enough disk space. AUDIT_S3_FAILED_METADATA_JSON_ENCODING ContainerSSH failed to encode the metadata file. This is a bug, please report it. AUDIT_S3_FAILED_READING_METADATA_FILE ContainerSSH failed to read the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is readable and the files have not been corrupted. AUDIT_S3_FAILED_STAT_QUEUE_ENTRY ContainerSSH failed to stat the queue file. This usually happens when the local directory is being manually manipulated. AUDIT_S3_FAILED_WRITING_METADATA_FILE ContainerSSH failed to write the local metadata file. Please check if your disk has enough disk space. AUDIT_S3_MULTIPART_ABORTING ContainerSSH is aborting a multipart upload. Check the log message for details. AUDIT_S3_MULTIPART_FAILED_ABORT ContainerSSH failed aborting a multipart upload from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_FAILED_LIST ContainerSSH failed to list multipart uploads on the object storage bucket. This is needed to abort uploads from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_PART_UPLOADING ContainerSSH is uploading a part of an audit log to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_COMPLETE ContainerSSH completed the upload of an audit log part to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_FAILED ContainerSSH failed to upload a part to the S3-compatible object storage. Check the message for details. AUDIT_S3_MULTIPART_UPLOAD ContainerSSH is starting a new S3 multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZATION_FAILED ContainerSSH has uploaded all audit log parts, but could not finalize the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZED ContainerSSH has uploaded all audit log parts and has successfully finalized the upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZING ContainerSSH has uploaded all audit log parts and is now finalizing the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_INITIALIZATION_FAILED ContainerSSH failed to initialize a new multipart upload to the S3-compatible object storage. Check if the S3 configuration is correct and the provided S3 access key and secrets have permissions to start a multipart upload. AUDIT_S3_NO_SUCH_QUEUE_ENTRY ContainerSSH was trying to upload an audit log from the metadata file, but the audit log does not exist. AUDIT_S3_RECOVERING ContainerSSH found a previously aborted multipart upload locally and is now attempting to recover the upload. AUDIT_S3_REMOVE_FAILED ContainerSSH failed to remove an uploaded audit log from the local directory. This usually happens on Windows when a different process has the audit log open. (This is not a supported setup.) AUDIT_S3_SINGLE_UPLOAD ContainerSSH is uploading the full audit log in a single upload to the S3-compatible object storage. This happens when the audit log size is below the minimum size for a multi-part upload. AUDIT_S3_SINGLE_UPLOAD_COMPLETE ContainerSSH successfully uploaded the audit log as a single upload. AUDIT_S3_SINGLE_UPLOAD_FAILED ContainerSSH failed to upload the audit log as a single upload.","title":"Auditlog"},{"location":"reference/codes/#authentication","text":"Code Explanation AUTH ContainerSSH is trying to contact the authentication backend to verify the user credentials. AUTH_BACKEND_ERROR The ContainerSSH authentication server responded with a non-200 status code. ContainerSSH will retry the authentication for a few times before giving up. This is most likely a bug in your authentication server, please check your logs. AUTH_FAILED The user has provided invalid credentials and the authentication is rejected. AUTH_INVALID_STATUS This message indicates that the authentication server returned an invalid HTTP status code. AUTH_NOT_SUPPORTED The authentication method the client requested is not supported by ContainerSSH. AUTH_SUCCESSFUL The user has provided the correct credentials and the authentication is accepted.","title":"Authentication"},{"location":"reference/codes/#backend","text":"Code Explanation BACKEND_CONFIG_ERROR The backend retreived from the configuration server is invalid. See the error message for details.","title":"Backend"},{"location":"reference/codes/#configuration","text":"Code Explanation CONFIG_BACKEND_ERROR ContainerSSH has received an invalid response from the configuration server or the network connection broke. ContainerSSH will retry fetching the user-specific configuration until the timeout. If this error persists check the connectivity to the configuration server, or the logs of the configuration server itself to find out of there may be a specific error. CONFIG_INVALID_STATUS_CODE ContainerSSH has received a non-200 response code when calling a per-user backend configuration from the configuration server. CONFIG_REQUEST ContainerSSH is sending a quest to the configuration server to obtain a per-user backend configuration. CONFIG_RESPONSE ContainerSSH has received a per-user backend configuration from the configuration server. CONFIG_SERVER_AVAILABLE The ContainerSSH configuration server is now available at the specified address.","title":"Configuration"},{"location":"reference/codes/#docker","text":"Code Explanation DOCKER_AGENT_READ_FAILED The ContainerSSH Docker module failed to read from the ContainerSSH agent. This is most likely because the ContainerSSH guest agent is not present in the guest image, but agent support is enabled. DOCKER_CLOSE_INPUT_FAILED The ContainerSSH Docker module attempted to close the input (stdin) for reading but failed to do so. DOCKER_CLOSE_OUTPUT_FAILED The ContainerSSH Docker module attempted to close the output (stdout and stderr) for writing but failed to do so. DOCKER_CONFIG_ERROR The ContainerSSH Docker module detected a configuration error. Please check your configuration. DOCKER_CONTAINER_ATTACH The ContainerSSH Docker module is attaching to a container in session mode. DOCKER_CONTAINER_ATTACH_FAILED The ContainerSSH Docker module has failed to attach to a container in session mode. DOCKER_CONTAINER_CREATE The ContainerSSH Docker module is creating a container. DOCKER_CONTAINER_CREATE_FAILED The ContainerSSH Docker module failed to create a container. This may be a temporary and retried or a permanent error message. Check the log message for details. DOCKER_CONTAINER_REMOVE The ContainerSSH Docker module os removing the container. DOCKER_CONTAINER_REMOVE_FAILED The ContainerSSH Docker module could not remove the container. This message may be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_REMOVE_SUCCESSFUL The ContainerSSH Docker module has successfully removed the container. DOCKER_CONTAINER_SHUTTING_DOWN The ContainerSSH Docker module is shutting down a container. DOCKER_CONTAINER_SIGNAL The ContainerSSH Docker module is sending a signal to the container. DOCKER_CONTAINER_SIGNAL_FAILED The ContainerSSH Docker module has failed to send a signal to the container. DOCKER_CONTAINER_START The ContainerSSH Docker module is starting the previously-created container. DOCKER_CONTAINER_START_FAILED The ContainerSSH docker module failed to start the container. This message can either be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_STOP The ContainerSSH Docker module is stopping the container. DOCKER_CONTAINER_STOP_FAILED The ContainerSSH Docker module failed to stop the container. This message can be either temporary and retried or permanent. Check the log message for details. DOCKER_EXEC The ContainerSSH Docker module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the container. DOCKER_EXEC_ATTACH The ContainerSSH Docker module is attaching to the previously-created execution. DOCKER_EXEC_ATTACH_FAILED The ContainerSSH Docker module could not attach to the previously-created execution. DOCKER_EXEC_CREATE The ContainerSSH Docker module is creating an execution. DOCKER_EXEC_CREATE_FAILED The ContainerSSH Docker module has failed to create an execution. This can be temporary and retried or permanent. See the error message for details. DOCKER_EXEC_PID_READ_FAILED The ContainerSSH Docker module has failed to read the process ID from the ContainerSSH Guest Agent . This is most likely because the guest image does not contain the guest agent, but guest agent support has been enabled. DOCKER_EXEC_RESIZE The ContainerSSH Docker module is resizing the console. DOCKER_EXEC_RESIZE_FAILED The ContainerSSH Docker module failed to resize the console. DOCKER_EXEC_SIGNAL The ContainerSSH Docker module is delivering a signal in container mode. DOCKER_EXEC_SIGNAL_FAILED The ContainerSSH Docker module failed to deliver a signal. DOCKER_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Docker module failed to deliver a signal because ContainerSSH Guest Agent support is disabled. DOCKER_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Docker module successfully delivered the requested signal. DOCKER_EXIT_CODE The ContainerSSH Docker module is fetching the exit code from the program. DOCKER_EXIT_CODE_CONTAINER_RESTARTING The ContainerSSH Docker module could not fetch the exit code from the program because the container is restarting. This is typically a misconfiguration as ContainerSSH containers should not automatically restart. DOCKER_EXIT_CODE_FAILED The ContainerSSH Docker module has failed to fetch the exit code of the program. DOCKER_EXIT_CODE_NEGATIVE The ContainerSSH Docker module has received a negative exit code from Docker. This should never happen and is most likely a bug. DOCKER_EXIT_CODE_STILL_RUNNING The ContainerSSH Docker module could not fetch the program exit code because the program is still running. This error may be temporary and retried or permanent. DOCKER_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the container image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. DOCKER_IMAGE_LISTING The ContainerSSH Docker module is listing the locally present container images to determine if the specified container image needs to be pulled. DOCKER_IMAGE_LISTING_FAILED The ContainerSSH Docker module failed to list the images present in the local Docker daemon. This is used to determine if the image needs to be pulled. This can be because the Docker daemon is not reachable, the certificate is invalid, or there is something else interfering with listing the images. DOCKER_IMAGE_PULL The ContainerSSH Docker module is pulling the container image. DOCKER_IMAGE_PULL_FAILED The ContainerSSH Docker module failed to pull the specified container image. This can be because of connection issues to the Docker daemon, or because the Docker daemon itself can't pull the image. If you don't intend to have the image pulled you should set the ImagePullPolicy to Never . See the Docker documentation for details. DOCKER_IMAGE_PULL_NEEDED_CHECKING The ContainerSSH Docker module is checking if an image pull is needed. DOCKER_PROGRAM_ALREADY_RUNNING The ContainerSSH Docker module can't execute the request because the program is already running. This is a client error. DOCKER_SIGNAL_FAILED_NO_PID The ContainerSSH Docker module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. DOCKER_STREAM_INPUT_FAILED The ContainerSSH Docker module failed to stream stdin to the Docker engine. DOCKER_STREAM_OUTPUT_FAILED The ContainerSSH Docker module failed to stream stdout and stderr from the Docker engine. DOCKER_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Docker module is not configured to run the requested subsystem.","title":"Docker"},{"location":"reference/codes/#http","text":"Code Explanation HTTP_CLIENT_CONNECTION_FAILED This message indicates a connection failure on the network level. HTTP_CLIENT_DECODE_FAILED This message indicates that decoding the JSON response has failed. The status code is set for this code. HTTP_CLIENT_ENCODE_FAILED This message indicates that JSON encoding the request failed. This is usually a bug. HTTP_CLIENT_REDIRECT This message indicates that the server responded with a HTTP redirect. HTTP_CLIENT_REDIRECTS_DISABLED This message indicates that ContainerSSH is not following a HTTP redirect sent by the server. Use the allowRedirects option to allow following HTTP redirects. HTTP_CLIENT_REQUEST This message indicates that a HTTP request is being sent from ContainerSSH HTTP_CLIENT_RESPONSE This message indicates that ContainerSSH received a HTTP response from a server. HTTP_SERVER_ENCODE_FAILED The HTTP server failed to encode the response object. This is a bug, please report it. HTTP_SERVER_RESPONSE_WRITE_FAILED The HTTP server failed to write the response.","title":"HTTP"},{"location":"reference/codes/#kubernetes","text":"Code Explanation KUBERNETES_CLOSE_OUTPUT_FAILED The ContainerSSH Kubernetes module attempted to close the output (stdout and stderr) for writing but failed to do so. KUBERNETES_CONFIG_ERROR The ContainerSSH Kubernetes module detected a configuration error. Please check your configuration. KUBERNETES_EXEC The ContainerSSH Kubernetes module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the pod. KUBERNETES_EXEC_RESIZE The ContainerSSH Kubernetes module is resizing the terminal window. KUBERNETES_EXEC_RESIZE_FAILED The ContainerSSH Kubernetes module failed to resize the console. KUBERNETES_EXEC_SIGNAL The ContainerSSH Kubernetes module is delivering a signal. KUBERNETES_EXEC_SIGNAL_FAILED The ContainerSSH Kubernetes module failed to deliver a signal. KUBERNETES_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Kubernetes module failed to deliver a signal because guest agent support is disabled. KUBERNETES_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Kubernetes module successfully delivered the requested signal. KUBERNETES_EXIT_CODE_FAILED The ContainerSSH Kubernetes module has failed to fetch the exit code of the program. KUBERNETES_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the pod image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. KUBERNETES_PID_RECEIVED The ContainerSSH Kubernetes module has received a PID from the Kubernetes guest agent. KUBERNETES_POD_ATTACH The ContainerSSH Kubernetes module is attaching to a pod in session mode. KUBERNETES_POD_CREATE The ContainerSSH Kubernetes module is creating a pod. KUBERNETES_POD_CREATE_FAILED The ContainerSSH Kubernetes module failed to create a pod. This may be a temporary and retried or a permanent error message. Check the log message for details. KUBERNETES_POD_REMOVE The ContainerSSH Kubernetes module is removing a pod. KUBERNETES_POD_REMOVE_FAILED The ContainerSSH Kubernetes module could not remove the pod. This message may be temporary and retried or permanent. Check the log message for details. KUBERNETES_POD_REMOVE_SUCCESSFUL The ContainerSSH Kubernetes module has successfully removed the pod. KUBERNETES_POD_SHUTTING_DOWN The ContainerSSH Kubernetes module is shutting down a pod. KUBERNETES_POD_WAIT The ContainerSSH Kubernetes module is waiting for the pod to come up. KUBERNETES_POD_WAIT_FAILED The ContainerSSH Kubernetes module failed to wait for the pod to come up. Check the error message for details. KUBERNETES_PROGRAM_ALREADY_RUNNING The ContainerSSH Kubernetes module can't execute the request because the program is already running. This is a client error. KUBERNETES_PROGRAM_NOT_RUNNING This message indicates that the user requested an action that can only be performed when a program is running, but there is currently no program running. KUBERNETES_SIGNAL_FAILED_EXITED The ContainerSSH Kubernetes module can't deliver a signal because the program already exited. KUBERNETES_SIGNAL_FAILED_NO_PID The ContainerSSH Kubernetes module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. KUBERNETES_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Kubernetes module is not configured to run the requested subsystem. KUBERUN_DEPRECATED This message indicates that you are still using the deprecated KubeRun backend. This backend doesn't support all safety and functionality improvements and will be removed in the future. Please read the deprecation notice for a migration guide KUBERUN_EXEC_DISABLED This message indicates that the user tried to execute a program, but program execution is disabled in the legacy KubeRun configuration. KUBERUN_INSECURE This message indicates that you are using Kubernetes in the \"insecure\" mode where certificate verification is disabled. This is a major security flaw, has been deprecated and is removed in the new Kubernetes backend. Please change your configuration to properly validates the server certificates.","title":"Kubernetes"},{"location":"reference/codes/#log","text":"Code Explanation LOG_FILE_OPEN_FAILED ContainerSSH failed to open the specified log file. LOG_ROTATE_FAILED ContainerSSH cannot rotate the logs as requested because of an underlying error. LOG_WRITE_FAILED ContainerSSH cannot write to the specified log file. This usually happens because the underlying filesystem is full or the log is located on a non-local storage (e.g. NFS), which is not supported. TEST This is message that should only be seen in unit and component tests, never in production. UNKNOWN_ERROR This is an untyped error. If you see this in a log that is a bug and should be reported.","title":"Log"},{"location":"reference/codes/#metrics","text":"Code Explanation METRICS_AVAILABLE The metrics service is now online and ready for service.","title":"Metrics"},{"location":"reference/codes/#security","text":"Code Explanation SECURITY_ENV_REJECTED ContainerSSH rejected setting the environment variable because it does not pass the security settings. SECURITY_EXEC_FAILED_SETENV Program execution failed in conjunction with the forceCommand option because ContainerSSH could not set the SSH_ORIGINAL_COMMAND environment variable on the backend. SECURITY_EXEC_FORCING_COMMAND ContainerSSH is replacing the command passed from the client (if any) to the specified command and is setting the SSH_ORIGINAL_COMMAND environment variable. SECURITY_EXEC_REJECTED A program execution request has been rejected because it doesn't conform to the security settings. SECURITY_SHELL_REJECTED ContainerSSH rejected launching a shell due to the security settings. SECURITY_SIGNAL_REJECTED ContainerSSH rejected delivering a signal because it does not pass the security settings. SECURITY_SUBSYSTEM_REJECTED ContainerSSH rejected the subsystem because it does pass the security settings. SECURITY_TTY_REJECTED ContainerSSH rejected the pseudoterminal request because of the security settings.","title":"Security"},{"location":"reference/codes/#service","text":"Code Explanation SERVICE_CRASHED A ContainerSSH has stopped improperly. SERVICE_POOL_RUNNING All ContainerSSH services are now running. SERVICE_POOL_STARTING All ContainerSSH services are starting. SERVICE_POOL_STOPPED ContainerSSH has stopped all services. SERVICE_POOL_STOPPING ContainerSSH is stopping all services. SERVICE_RUNNING A ContainerSSH service is now running SERVICE_STARTING ContainerSSH is starting a component service SERVICE_STOPPED A ContainerSSH service has stopped. SERVICE_STOPPING A ContainerSSH service is now stopping.","title":"Service"},{"location":"reference/codes/#ssh","text":"Code Explanation SSH_ALREADY_RUNNING The SSH server is already running and has been started again. This is a bug, please report it. SSH_AUTH_FAILED The user has provided invalid credentials. SSH_AUTH_SUCCESSFUL The user has provided valid credentials and is now authenticated. SSH_AUTH_UNAVAILABLE The user has requested an authentication method that is currently unavailable. SSH_AVAILABLE The SSH service is now online and ready for service. SSH_BACKEND_REJECTED_HANDSHAKE The backend has rejected the connecting user after successful authentication. SSH_CHANNEL_REQUEST The user has send a new channel-specific request. SSH_CHANNEL_REQUEST_FAILED ContainerSSH couldn't fulfil the channel-specific request. SSH_CHANNEL_REQUEST_SUCCESSFUL ContainerSSH has successfully processed the channel-specific request. SSH_CONNECTED A user has connected over SSH. SSH_DECODE_FAILED ContainerSSH failed to decode something from the user. This is either a bug in ContainerSSH or in the connecting client. SSH_DISCONNECTED An SSH connection has been severed. SSH_EXIT ContainerSSH is sending the exit code of the program to the user. SSH_EXIT_CODE_FAILED ContainerSSH failed to obtain and send the exit code of the program to the user. SSH_EXIT_SIGNAL ContainerSSH is sending the exit signal from an abnormally exited program to the user. SSH_HANDSHAKE_FAILED The connecting party failed to establish a secure SSH connection. This is most likely due to invalid credentials or a backend error. SSH_HANDSHAKE_SUCCESSFUL The user has provided valid credentials and has now established an SSH connection. SSH_LISTEN_CLOSE_FAILED ContainerSSH failed to close the listen socket. SSH_NEW_CHANNEL A user has established a new SSH channel. (Not connection!) SSH_NEW_CHANNEL_REJECTED The user has requested a new channel to be opened, but was rejected. SSH_REPLY_SEND_FAILED ContainerSSH couldn't send the reply to a request to the user. This is usually the case if the user suddenly disconnects. SSH_START_FAILED ContainerSSH failed to start the SSH service. This is usually because of invalid configuration. SSH_UNSUPPORTED_CHANNEL_TYPE The user requested a channel type that ContainerSSH doesn't support (e.g. TCP/IP forwarding). SSH_UNSUPPORTED_GLOBAL_REQUEST The users client has send a global request ContainerSSH does not support. This is nothing to worry about.","title":"SSH"},{"location":"reference/configserver/","text":"Configuration Server ContainerSSH has the ability to configure the backend and the launched container dynamically based on the username and/or IP address. To do this ContainerSSH calls out to a configuration server if configured. Configuration \u00b6 The configserver webhook can be configured in the main configuration using the following structure: configuration : <options> The following options are supported: Name Type Description url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion string Minimum TLS version to support. See the TLS version section below. curve string Elliptic curve algorithms to support. See the [Elliptic curve algorithms][] section below. cipher []string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false. Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Middle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Elliptic curve algorithms \u00b6 The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1 Cipher suites \u00b6 The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Client authentication \u00b6 In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem The configuration webhook \u00b6 The configuration webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create a configuration server. The config server will receive a request in following format: { \"username\" : \"ssh username\" , \"connectionId\" : \"ssh session ID\" } The configuration server will have to respond with the following response accompanied with the content type of application/json . { \"config\" : { // Provide a par t ial co nf igura t io n here } } The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running ./containerssh --dump-config . The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file. Currently only the following options can be set from the configuration server: Backend Docker Kubernetes DockerRun KubeRun Security Tip We provide a Go library to implement a config server .","title":"Configuration server"},{"location":"reference/configserver/#configuration","text":"The configserver webhook can be configured in the main configuration using the following structure: configuration : <options> The following options are supported: Name Type Description url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion string Minimum TLS version to support. See the TLS version section below. curve string Elliptic curve algorithms to support. See the [Elliptic curve algorithms][] section below. cipher []string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false.","title":"Configuration"},{"location":"reference/configserver/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Middle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/configserver/#tls-version","text":"The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported.","title":"TLS version"},{"location":"reference/configserver/#elliptic-curve-algorithms","text":"The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1","title":"Elliptic curve algorithms"},{"location":"reference/configserver/#cipher-suites","text":"The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305","title":"Cipher suites"},{"location":"reference/configserver/#client-authentication","text":"In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem","title":"Client authentication"},{"location":"reference/configserver/#the-configuration-webhook","text":"The configuration webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create a configuration server. The config server will receive a request in following format: { \"username\" : \"ssh username\" , \"connectionId\" : \"ssh session ID\" } The configuration server will have to respond with the following response accompanied with the content type of application/json . { \"config\" : { // Provide a par t ial co nf igura t io n here } } The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running ./containerssh --dump-config . The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file. Currently only the following options can be set from the configuration server: Backend Docker Kubernetes DockerRun KubeRun Security Tip We provide a Go library to implement a config server .","title":"The configuration webhook"},{"location":"reference/docker/","text":"The Docker backend The Docker backend should work with any Docker Engine version starting with 1.6 thanks to the version negotiation present. We fix issues starting with Docker version 18.02. Tip This is the documentation for the Docker backend . For deploying ContainerSSH inside Docker please see the installation guide . The base configuration structure \u00b6 In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : docker docker : connection : <connection configuration here> execution : <execution configuration here> timeouts : <timeouts configuration here> Configuring connection parameters \u00b6 The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : connection : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Configuring container execution \u00b6 Container execution options can be specified as follows: docker : execution : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases. Basic container configuration \u00b6 The basic configuration options are as follows: docker : execution : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu Mounting volumes \u00b6 Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs Bind mounts \u00b6 Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : execution : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume. Mounts \u00b6 The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : execution : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755 tmpfs \u00b6 The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : execution : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page . Other options \u00b6 Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description mode string Specify connection to launch one container per SSH connection or session to run one container per SSH session (multiple containers per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how docker exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. imagePullPolicy Never,IfNotPresent,Always Specifies when to pull the container image. Defaults to IfNotPresent , which pulls the image when it is not locally present or if the image has no tag/has the latest tag. It is recommended that you provide a custom, versioned image name to prevent pulling the image at every connection. Configuring timeouts \u00b6 The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description containerStart The time to wait for the container to start. containerStop The time to wait for the container to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete. Securing Docker \u00b6 Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server. Securing the Docker socket \u00b6 Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates! Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : execution : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : execution : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty. Preventing storage exhaustion \u00b6 A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : execution : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : execution : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : execution : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: docker : execution : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the totoal memory + swap usage: docker : execution : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : execution : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : execution : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000 Preventing process exhaustion \u00b6 You can also limit the number of processes that can be launched inside the container: docker : execution : host : resources : pidslimit : 1000 Limiting network access \u00b6 In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : execution : container : networkdisabled : true Limiting disk I/O \u00b6 Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : execution : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Docker"},{"location":"reference/docker/#the-base-configuration-structure","text":"In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : docker docker : connection : <connection configuration here> execution : <execution configuration here> timeouts : <timeouts configuration here>","title":"The base configuration structure"},{"location":"reference/docker/#configuring-connection-parameters","text":"The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : connection : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE-----","title":"Configuring connection parameters"},{"location":"reference/docker/#configuring-container-execution","text":"Container execution options can be specified as follows: docker : execution : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases.","title":"Configuring container execution"},{"location":"reference/docker/#basic-container-configuration","text":"The basic configuration options are as follows: docker : execution : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu","title":"Basic container configuration"},{"location":"reference/docker/#mounting-volumes","text":"Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs","title":"Mounting volumes"},{"location":"reference/docker/#bind-mounts","text":"Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : execution : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume.","title":"Bind mounts"},{"location":"reference/docker/#mounts","text":"The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : execution : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755","title":"Mounts"},{"location":"reference/docker/#tmpfs","text":"The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : execution : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page .","title":"tmpfs"},{"location":"reference/docker/#other-options","text":"Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description mode string Specify connection to launch one container per SSH connection or session to run one container per SSH session (multiple containers per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how docker exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. imagePullPolicy Never,IfNotPresent,Always Specifies when to pull the container image. Defaults to IfNotPresent , which pulls the image when it is not locally present or if the image has no tag/has the latest tag. It is recommended that you provide a custom, versioned image name to prevent pulling the image at every connection.","title":"Other options"},{"location":"reference/docker/#configuring-timeouts","text":"The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description containerStart The time to wait for the container to start. containerStop The time to wait for the container to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete.","title":"Configuring timeouts"},{"location":"reference/docker/#securing-docker","text":"Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server.","title":"Securing Docker"},{"location":"reference/docker/#securing-the-docker-socket","text":"Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates!","title":"Securing the Docker socket"},{"location":"reference/docker/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : execution : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : execution : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty.","title":"Preventing root escalation"},{"location":"reference/docker/#preventing-storage-exhaustion","text":"A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : execution : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : execution : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : execution : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted.","title":"Preventing storage exhaustion"},{"location":"reference/docker/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: docker : execution : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the totoal memory + swap usage: docker : execution : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : execution : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true","title":"Preventing memory exhaustion"},{"location":"reference/docker/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : execution : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000","title":"Preventing CPU exhaustion"},{"location":"reference/docker/#preventing-process-exhaustion","text":"You can also limit the number of processes that can be launched inside the container: docker : execution : host : resources : pidslimit : 1000","title":"Preventing process exhaustion"},{"location":"reference/docker/#limiting-network-access","text":"In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : execution : container : networkdisabled : true","title":"Limiting network access"},{"location":"reference/docker/#limiting-disk-io","text":"Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : execution : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Limiting disk I/O"},{"location":"reference/dockerrun/","text":"The DockerRun backend Warning The DockerRun backend is deprecated and the Docker backend should be used instead. Read the migration guide here \u00bb The DockerRun backend should work with any Docker Engine version starting with 1.6 thanks to the version negotiation present. We fix issues starting with Docker version 18.02. Tip This is the documentation for the DockerRun backend . For deploying ContainerSSH inside Docker please see the installation guide . The base configuration structure \u00b6 In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : dockerrun dockerrun : <connection configuration here> config : <execution configuration here> Configuring connection parameters \u00b6 The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Configuring container execution \u00b6 Container execution options can be specified as follows: docker : config : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If a container already exists with the same name the container creation will fail, so this should be left empty for most use cases. Basic container configuration \u00b6 The basic configuration options are as follows: docker : config : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu Mounting volumes \u00b6 Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs Bind mounts \u00b6 Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : config : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume. Mounts \u00b6 The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : config : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755 tmpfs \u00b6 The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : config : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page . Other options \u00b6 Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. timeout string Timeout for container operations in nanoseconds. Time units can be set. Securing Docker \u00b6 Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server. Securing the Docker socket \u00b6 Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates! Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : config : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : config : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty. Preventing storage exhaustion \u00b6 A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : config : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : config : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : config : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to crash the server. This can be prevented using the following configuration: docker : config : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the total memory + swap usage: docker : config : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : config : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : config : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000 Preventing process exhaustion \u00b6 You can also limit the number of processes that can be launched inside the container: docker : config : host : resources : pidslimit : 1000 Limiting network access \u00b6 In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : config : container : networkdisabled : true Limiting disk I/O \u00b6 Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : config : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"DockerRun (deprecated)"},{"location":"reference/dockerrun/#the-base-configuration-structure","text":"In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : dockerrun dockerrun : <connection configuration here> config : <execution configuration here>","title":"The base configuration structure"},{"location":"reference/dockerrun/#configuring-connection-parameters","text":"The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE-----","title":"Configuring connection parameters"},{"location":"reference/dockerrun/#configuring-container-execution","text":"Container execution options can be specified as follows: docker : config : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If a container already exists with the same name the container creation will fail, so this should be left empty for most use cases.","title":"Configuring container execution"},{"location":"reference/dockerrun/#basic-container-configuration","text":"The basic configuration options are as follows: docker : config : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu","title":"Basic container configuration"},{"location":"reference/dockerrun/#mounting-volumes","text":"Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs","title":"Mounting volumes"},{"location":"reference/dockerrun/#bind-mounts","text":"Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : config : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume.","title":"Bind mounts"},{"location":"reference/dockerrun/#mounts","text":"The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : config : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755","title":"Mounts"},{"location":"reference/dockerrun/#tmpfs","text":"The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : config : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page .","title":"tmpfs"},{"location":"reference/dockerrun/#other-options","text":"Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. timeout string Timeout for container operations in nanoseconds. Time units can be set.","title":"Other options"},{"location":"reference/dockerrun/#securing-docker","text":"Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server.","title":"Securing Docker"},{"location":"reference/dockerrun/#securing-the-docker-socket","text":"Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates!","title":"Securing the Docker socket"},{"location":"reference/dockerrun/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : config : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : config : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty.","title":"Preventing root escalation"},{"location":"reference/dockerrun/#preventing-storage-exhaustion","text":"A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : config : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : config : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : config : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted.","title":"Preventing storage exhaustion"},{"location":"reference/dockerrun/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to crash the server. This can be prevented using the following configuration: docker : config : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the total memory + swap usage: docker : config : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : config : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true","title":"Preventing memory exhaustion"},{"location":"reference/dockerrun/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : config : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000","title":"Preventing CPU exhaustion"},{"location":"reference/dockerrun/#preventing-process-exhaustion","text":"You can also limit the number of processes that can be launched inside the container: docker : config : host : resources : pidslimit : 1000","title":"Preventing process exhaustion"},{"location":"reference/dockerrun/#limiting-network-access","text":"In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : config : container : networkdisabled : true","title":"Limiting network access"},{"location":"reference/dockerrun/#limiting-disk-io","text":"Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : config : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Limiting disk I/O"},{"location":"reference/features/","text":"Supported SSH features \u00b6 This table contains the list of currently supported SSH features in ContainerSSH. Feature Support RFC Shell execution RFC 4254 section 6.5 Command execution RFC 4254 section 6.5 Subsystem execution RFC 4254 section 6.5 Requesting a Pseudo-Terminal RFC 4254 section 6.2 Setting environment variables RFC 4254 section 6.4 Forwarding signals RFC 4254 section 6.9 Window dimension change RFC 4254 section 6.7 Return exit statuses RFC 4254 section 6.10 Return exit signals RFC 4254 section 6.10 TCP/IP port forwarding RFC 4254 section 7 X11 forwarding RFC 4254 section 6.2 SSH agent forwarding (OpenSSH extension: auth-agent-req@openssh.com ) draft-ietf-secsh-agent-02 Keepalive (OpenSSH extension: keepalive@openssh.com ) No more sessions (OpenSSH extension: no-more-sessions@openssh.com )","title":"Supported features"},{"location":"reference/features/#supported-ssh-features","text":"This table contains the list of currently supported SSH features in ContainerSSH. Feature Support RFC Shell execution RFC 4254 section 6.5 Command execution RFC 4254 section 6.5 Subsystem execution RFC 4254 section 6.5 Requesting a Pseudo-Terminal RFC 4254 section 6.2 Setting environment variables RFC 4254 section 6.4 Forwarding signals RFC 4254 section 6.9 Window dimension change RFC 4254 section 6.7 Return exit statuses RFC 4254 section 6.10 Return exit signals RFC 4254 section 6.10 TCP/IP port forwarding RFC 4254 section 7 X11 forwarding RFC 4254 section 6.2 SSH agent forwarding (OpenSSH extension: auth-agent-req@openssh.com ) draft-ietf-secsh-agent-02 Keepalive (OpenSSH extension: keepalive@openssh.com ) No more sessions (OpenSSH extension: no-more-sessions@openssh.com )","title":"Supported SSH features"},{"location":"reference/hardening/","text":"Hardening ContainerSSH ContainerSSH is built to secure its inner workings as much as possible. You can take several steps to secure it further. Running ContainerSSH \u00b6 ContainerSSH should be run as an unprivileged user (e.g. root) as it does not need access to any privileged resources. When running it from the default container image containerssh/containerssh this is the default. When running it outside a container you should keep the default bind port of 2222. On Linux you can then use iptables to redirect port 22 to the unprivileged port: iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222 When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman . Tip Don't forget to add this rule to your persistent firewall configuration. Securing authentication \u00b6 Authentication server connection \u00b6 ContainerSSH talks to an authentication server over HTTP. There are two potential attacks here: If an attacker can intercept the connection between ContainerSSH and the authentication server the attacker can read the passwords for password authentication. If an attacker can send requests to the authentication server they can brute force SSH passwords. Therefore, the connection between ContainerSSH and the authentication server should be secured in the following 3 ways: Implement firewalls such that only ContainerSSH can access the authentication server. Only use HTTPS with certificate verification to access the authentication server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the authentication server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the authentication chapter . Rate limiting \u00b6 ContainerSSH contains no rate limiting for authentication across connections, this is the job of the authentication server. The number of authentication attempts within a connection is limited to 6. The authentication server must take care to do rate limiting right: within a single connection multiple authentication attempts may be made and if the authentication server returns a non-200 response code ContainerSSH will retry connections. It is recommended that the authentication server use the connectionId field to distinguish between SSH connections as this field is guaranteed to be unique for a connection. Client credential security \u00b6 Passwords are vulnerable to being stolen and cannot be transferred to hardware keys. For the most security it is recommended to disable password authentication and only use SSH keys. When storing SSH keys on the client computer they should be protected by a passphrase and limited permissions on the key file. If possible, however, SSH keys should be transferred to a hardware token like the Yubikey . The Yubikey should be configured to require a physical touch on every authentication and should be unlocked with a passcode to prevent unauthorized applications on the client accessing the key for connections. Securing the configuration server \u00b6 ContainerSSH can optionally reach out to a configuration server to fetch dynamic backend configuration based on the username. The backend configuration may contain secrets, such as certificates for accessing Docker and Kubernetes, or application-specific secrets. Therefore, if an attacker can access the configuration server they can extract secrets from the returned configuration. This can be mitigated similar to the authentication server: Implement firewalls such that only ContainerSSH can access the configuration server. Only use HTTPS with certificate verification to access the configuration server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the configuration server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the configuration server chapter . Limiting SSH requests \u00b6 The security module provides the ability to limit which requests are allowed from a client. As ContainerSSH is upgraded the default is to allow new features that will come in with future releases (e.g. TCP port forwarding). In order to secure ContainerSSH for future releases it is recommended to set the defaultMode to disable and enable the modes that you need. For subsystems specifically we recommend filtering and allowing only the sftp subsystem as future ContainerSSH versions may support more subsystems. security : defaultMode : disable env : mode : enable command : mode : enable shell : mode : enable subsystem : mode : filter allow : - sftp tty : mode : enable signal : mode : enable Securing Docker \u00b6 Docker-specific settings for security are described in the Docker documentation . Securing Kubernetes \u00b6 Kubernetes-specific settings for security are described in the Kubernetes documentation .","title":"Hardening guide"},{"location":"reference/hardening/#running-containerssh","text":"ContainerSSH should be run as an unprivileged user (e.g. root) as it does not need access to any privileged resources. When running it from the default container image containerssh/containerssh this is the default. When running it outside a container you should keep the default bind port of 2222. On Linux you can then use iptables to redirect port 22 to the unprivileged port: iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222 When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman . Tip Don't forget to add this rule to your persistent firewall configuration.","title":"Running ContainerSSH"},{"location":"reference/hardening/#securing-authentication","text":"","title":"Securing authentication"},{"location":"reference/hardening/#authentication-server-connection","text":"ContainerSSH talks to an authentication server over HTTP. There are two potential attacks here: If an attacker can intercept the connection between ContainerSSH and the authentication server the attacker can read the passwords for password authentication. If an attacker can send requests to the authentication server they can brute force SSH passwords. Therefore, the connection between ContainerSSH and the authentication server should be secured in the following 3 ways: Implement firewalls such that only ContainerSSH can access the authentication server. Only use HTTPS with certificate verification to access the authentication server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the authentication server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the authentication chapter .","title":"Authentication server connection"},{"location":"reference/hardening/#rate-limiting","text":"ContainerSSH contains no rate limiting for authentication across connections, this is the job of the authentication server. The number of authentication attempts within a connection is limited to 6. The authentication server must take care to do rate limiting right: within a single connection multiple authentication attempts may be made and if the authentication server returns a non-200 response code ContainerSSH will retry connections. It is recommended that the authentication server use the connectionId field to distinguish between SSH connections as this field is guaranteed to be unique for a connection.","title":"Rate limiting"},{"location":"reference/hardening/#client-credential-security","text":"Passwords are vulnerable to being stolen and cannot be transferred to hardware keys. For the most security it is recommended to disable password authentication and only use SSH keys. When storing SSH keys on the client computer they should be protected by a passphrase and limited permissions on the key file. If possible, however, SSH keys should be transferred to a hardware token like the Yubikey . The Yubikey should be configured to require a physical touch on every authentication and should be unlocked with a passcode to prevent unauthorized applications on the client accessing the key for connections.","title":"Client credential security"},{"location":"reference/hardening/#securing-the-configuration-server","text":"ContainerSSH can optionally reach out to a configuration server to fetch dynamic backend configuration based on the username. The backend configuration may contain secrets, such as certificates for accessing Docker and Kubernetes, or application-specific secrets. Therefore, if an attacker can access the configuration server they can extract secrets from the returned configuration. This can be mitigated similar to the authentication server: Implement firewalls such that only ContainerSSH can access the configuration server. Only use HTTPS with certificate verification to access the configuration server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the configuration server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the configuration server chapter .","title":"Securing the configuration server"},{"location":"reference/hardening/#limiting-ssh-requests","text":"The security module provides the ability to limit which requests are allowed from a client. As ContainerSSH is upgraded the default is to allow new features that will come in with future releases (e.g. TCP port forwarding). In order to secure ContainerSSH for future releases it is recommended to set the defaultMode to disable and enable the modes that you need. For subsystems specifically we recommend filtering and allowing only the sftp subsystem as future ContainerSSH versions may support more subsystems. security : defaultMode : disable env : mode : enable command : mode : enable shell : mode : enable subsystem : mode : filter allow : - sftp tty : mode : enable signal : mode : enable","title":"Limiting SSH requests"},{"location":"reference/hardening/#securing-docker","text":"Docker-specific settings for security are described in the Docker documentation .","title":"Securing Docker"},{"location":"reference/hardening/#securing-kubernetes","text":"Kubernetes-specific settings for security are described in the Kubernetes documentation .","title":"Securing Kubernetes"},{"location":"reference/image/","text":"Building a container image for ContainerSSH ContainerSSH can run any Linux container image. However, it is strongly recommended that you install the ContainerSSH guest agent into the image to make all features available. If you wish to use SFTP you have to add an SFTP server ( apt install openssh-sftp-server on Ubuntu) to the container image and configure the path of the SFTP server correctly in your config.yaml. The sample image containerssh/containerssh-guest-image contains an SFTP server. Integrating the guest agent \u00b6 Using the base image (recommended) This method uses the containerssh/agent container image as part of a multistage build: FROM containerssh/agent AS agent FROM your-base-image COPY --from=agent /usr/bin/containerssh-agent /usr/bin/containerssh-agent # Your other build commands here Installing on Debian/Ubuntu We have an experimental Debian repository containing the agent package. Once you have set up the repository you can install the agent like this: apt-get install containerssh-agent Installing the binaries To use this method go to the latest release from the releases section and verify it against our https://containerssh.io/gpg.txt key ( 3EE5B012FA7B400CD952601E4689F1F0F358FABA ). On an Ubuntu image build this would involve the following steps: ARG AGENT_GPG_FINGERPRINT = 3EE5B012FA7B400CD952601E4689F1F0F358FABA ARG AGENT_GPG_SOURCE = https://containerssh.io/gpg.txt RUN echo \"\\e[1;32mInstalling ContainerSSH guest agent...\\e[0m\" && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' update && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages install gpg && \\ wget -q -O - https://api.github.com/repos/containerssh/agent/releases/latest | grep browser_download_url | grep -e \"agent_.*_linux_amd64.deb\" | awk ' { print $2 } ' | sed -e 's/\"//g' > /tmp/assets.txt && \\ wget -q -O /tmp/agent.deb $( cat /tmp/assets.txt | grep -v .sig ) && \\ wget -q -O /tmp/agent.deb.sig $( cat /tmp/assets.txt | grep .sig ) && \\ wget -q -O - $AGENT_GPG_SOURCE | gpg --import && \\ echo -e \"5\\ny\\n\" | gpg --command-fd 0 --batch --expert --edit-key $AGENT_GPG_FINGERPRINT trust && \\ test $( gpg --status-fd = 1 --verify /tmp/agent.deb.sig /tmp/agent.deb | grep VALIDSIG | grep $AGENT_GPG_FINGERPRINT | wc -l ) -eq 1 && \\ dpkg -i /tmp/agent.deb && \\ rm -rf /tmp/* && \\ rm -rf ~/.gnupg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages remove gpg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -y clean && \\ /usr/bin/containerssh-agent -h Warning The release signing process is experimental and is likely to change in the future. Guest image support is enabled by default in the Docker and Kubernetes backends, but can be disabled as shown below. The KubeRun and DockerRun backends do not support the guest agent. Docker docker : execution : disableAgent : true Kubernetes kubernetes : pod : disableAgent : true","title":"Creating Guest Images"},{"location":"reference/image/#integrating-the-guest-agent","text":"Using the base image (recommended) This method uses the containerssh/agent container image as part of a multistage build: FROM containerssh/agent AS agent FROM your-base-image COPY --from=agent /usr/bin/containerssh-agent /usr/bin/containerssh-agent # Your other build commands here Installing on Debian/Ubuntu We have an experimental Debian repository containing the agent package. Once you have set up the repository you can install the agent like this: apt-get install containerssh-agent Installing the binaries To use this method go to the latest release from the releases section and verify it against our https://containerssh.io/gpg.txt key ( 3EE5B012FA7B400CD952601E4689F1F0F358FABA ). On an Ubuntu image build this would involve the following steps: ARG AGENT_GPG_FINGERPRINT = 3EE5B012FA7B400CD952601E4689F1F0F358FABA ARG AGENT_GPG_SOURCE = https://containerssh.io/gpg.txt RUN echo \"\\e[1;32mInstalling ContainerSSH guest agent...\\e[0m\" && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' update && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages install gpg && \\ wget -q -O - https://api.github.com/repos/containerssh/agent/releases/latest | grep browser_download_url | grep -e \"agent_.*_linux_amd64.deb\" | awk ' { print $2 } ' | sed -e 's/\"//g' > /tmp/assets.txt && \\ wget -q -O /tmp/agent.deb $( cat /tmp/assets.txt | grep -v .sig ) && \\ wget -q -O /tmp/agent.deb.sig $( cat /tmp/assets.txt | grep .sig ) && \\ wget -q -O - $AGENT_GPG_SOURCE | gpg --import && \\ echo -e \"5\\ny\\n\" | gpg --command-fd 0 --batch --expert --edit-key $AGENT_GPG_FINGERPRINT trust && \\ test $( gpg --status-fd = 1 --verify /tmp/agent.deb.sig /tmp/agent.deb | grep VALIDSIG | grep $AGENT_GPG_FINGERPRINT | wc -l ) -eq 1 && \\ dpkg -i /tmp/agent.deb && \\ rm -rf /tmp/* && \\ rm -rf ~/.gnupg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages remove gpg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -y clean && \\ /usr/bin/containerssh-agent -h Warning The release signing process is experimental and is likely to change in the future. Guest image support is enabled by default in the Docker and Kubernetes backends, but can be disabled as shown below. The KubeRun and DockerRun backends do not support the guest agent. Docker docker : execution : disableAgent : true Kubernetes kubernetes : pod : disableAgent : true","title":"Integrating the guest agent"},{"location":"reference/installation/","text":"Installation Standalone ContainerSSH can be deployed outside of a container. On our downloads page we provide binaries for Linux, Windows, and MacOS. We also provide DEB, RPM and APK (Alpine Linux) packages. Before running ContainerSSH you will need to create a config.yaml file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /path/to/your/host.key auth : url : http://your-auth-server/ Tip You can generate a new host key using openssl genrsa . Please don't use ssh-keygen as it regenerates OpenSSH-specific keys. Tip Details about the authentication server are described in the Authentication section . ContainerSSH can then be started by running ./containerssh --config /path/to/your/config.yaml Docker When deploying in Docker you must first prepare a configuration file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /var/run/secrets/host.key auth : url : http://your-auth-server/ Tip You can generate a new host key using openssl genrsa Tip Details about the authentication server are described in the Authentication section . You can then run ContainerSSH with the following command line: docker run -d \\ -v /srv/containerssh/config.yaml:/etc/containerssh/config.yaml \\ -v /srv/containerssh/host.key:/var/run/secrets/host.key \\ -p 2222 :2222 \\ containerssh/containerssh:0.4 Kubernetes When running ContainerSSH inside a Kubernetes cluster you must furst create a Secret that contains the host key. openssl genrsa | kubectl create secret generic containerssh-hostkey --from-file = host.key = /dev/stdin Next, you can create a ConfigMap to hold the ContainerSSH configuration: ( cat << EOF ssh: hostkeys: - /etc/containerssh/host.key auth: url: http://your-auth-server/ EOF ) | kubectl create configmap containerssh-config --from-file = config.yaml = /dev/stdin Tip Details about the authentication server are described in the Authentication section . Then you can create a deployment to run ContainerSSH: ( cat << EOF apiVersion: apps/v1 kind: Deployment metadata: name: containerssh labels: app: containerssh spec: replicas: 1 selector: matchLabels: app: containerssh template: metadata: labels: app: containerssh spec: containers: - name: containerssh image: containerssh/containerssh:0.4 ports: - containerPort: 2222 volumeMounts: - name: hostkey mountPath: /etc/containerssh/host.key subPath: host.key readOnly: true - name: config mountPath: /etc/containerssh/config.yaml subPath: config.yaml readOnly: true volumes: - name: hostkey secret: secretName: containerssh-hostkey - name: config configMap: name: containerssh-config EOF ) | kubectl apply -f - Finally, you can create a service to expose the SSH port. You can customize this to create a loadbalancer or nodeport to make SSH publicly available. See kubectl expose --help for details. kubectl expose deployment containerssh \\ --port = 2222 --target-port = 2222 \\ --name = containerssh Note This still does not configure ContainerSSH to use Kubernetes as a container backend. This is described in detail in the Kubernetes backend section .","title":"Installation"},{"location":"reference/kubernetes/","text":"The Kubernetes backend The Kubernetes backend runs and is tested against all currently actively maintained Kubernetes versions . For ContainerSSH version 0.4.1 these are: 1.21, 1.20, and 1.19. Tip This is the documentation for the Kubernetes backend . For deploying ContainerSSH inside Kubernetes please see the installation guide . The base configuration structure \u00b6 In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kubernetes kubernetes : connection : <connection configuration here> pod : <pod configuration here> timeouts : <timeouts configuration here> Configuring connection parameters \u00b6 In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kubernetes : connection : host : <...> <...> Tip See the Securing Kubernetes section below for a detailed walkthrough for provisioning limited service accounts for ContainerSSH. Base configuration \u00b6 Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. Set to /var/run/secrets/kubernetes.io/serviceaccount/ca.crt when running with a service account. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. qps float32 Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. HTTP basic authentication (username and password) \u00b6 Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) x509 certificate authentication \u00b6 Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format. Bearer token authentication \u00b6 This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount/token to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API. Pod configuration \u00b6 The pod configuration contains the information which pod to run. The structure is very similar to the Pod object in Kubernetes, and we add a few extra options: kubernetes : pod : metadata : <metadata configuration here> spec : <pod spec here> <ContainerSSH-specific options here> Note Do not include the apiVersion , kind , or status types from the Kubernetes structure. Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod , kubectl explain pod.spec , and kubectl explain pod.metadata . Basic pod configuration \u00b6 ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kubernetes : pod : metadata : namespace : default spec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world! Running multiple containers \u00b6 When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kubernetes: pod: consoleContainerNumber: 1 spec: containers: - name: container1 image: ... - name: container2 image: ... Mounting volumes \u00b6 In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage. Forcing the pod to run on a specific node \u00b6 In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kubernetes : pod : spec : nodeName : <insert node name here> Other options \u00b6 Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. Configuration restrictions \u00b6 In connection mode the idleCommand and shellCommand options are required. In session mode the restart policy must be empty or Never . Configuring timeouts \u00b6 The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description podStart The time to wait for the pod to start. podStop The time to wait for the pod to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete. Securing Kubernetes \u00b6 Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment Creating a service account \u00b6 When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh:containerssh Let's test if the permissions are correct: $ kubectl auth can-i create pod --as containerssh no $ kubectl auth can-i create pod --namespace containerssh-guests --as containerssh yes Docker Desktop Docker Desktop Kubernetes contains a cluster role binding called docker-for-desktop-binding that allows all service accounts to perform every action. To secure your Docker Desktop installation you will need to delete this CRB. Deploying inside of Kubernetes \u00b6 When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kubernetes: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token Deploying outside of Kubernetes \u00b6 Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : ZGVmYXVsdA== token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kubernetes : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here> Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kubernetes : pod : spec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kubernetes : pod : spec : securityContext : runAsUser : 1000 Preventing storage exhaustion \u00b6 A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kubernetes : pod : spec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation . Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kubernetes : pod : spec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation . Limiting network access \u00b6 Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Kubernetes"},{"location":"reference/kubernetes/#the-base-configuration-structure","text":"In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kubernetes kubernetes : connection : <connection configuration here> pod : <pod configuration here> timeouts : <timeouts configuration here>","title":"The base configuration structure"},{"location":"reference/kubernetes/#configuring-connection-parameters","text":"In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kubernetes : connection : host : <...> <...> Tip See the Securing Kubernetes section below for a detailed walkthrough for provisioning limited service accounts for ContainerSSH.","title":"Configuring connection parameters"},{"location":"reference/kubernetes/#base-configuration","text":"Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. Set to /var/run/secrets/kubernetes.io/serviceaccount/ca.crt when running with a service account. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. qps float32 Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling.","title":"Base configuration"},{"location":"reference/kubernetes/#http-basic-authentication-username-and-password","text":"Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2)","title":"HTTP basic authentication (username and password)"},{"location":"reference/kubernetes/#x509-certificate-authentication","text":"Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format.","title":"x509 certificate authentication"},{"location":"reference/kubernetes/#bearer-token-authentication","text":"This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount/token to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API.","title":"Bearer token authentication"},{"location":"reference/kubernetes/#pod-configuration","text":"The pod configuration contains the information which pod to run. The structure is very similar to the Pod object in Kubernetes, and we add a few extra options: kubernetes : pod : metadata : <metadata configuration here> spec : <pod spec here> <ContainerSSH-specific options here> Note Do not include the apiVersion , kind , or status types from the Kubernetes structure. Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod , kubectl explain pod.spec , and kubectl explain pod.metadata .","title":"Pod configuration"},{"location":"reference/kubernetes/#basic-pod-configuration","text":"ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kubernetes : pod : metadata : namespace : default spec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world!","title":"Basic pod configuration"},{"location":"reference/kubernetes/#running-multiple-containers","text":"When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kubernetes: pod: consoleContainerNumber: 1 spec: containers: - name: container1 image: ... - name: container2 image: ...","title":"Running multiple containers"},{"location":"reference/kubernetes/#mounting-volumes","text":"In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage.","title":"Mounting volumes"},{"location":"reference/kubernetes/#forcing-the-pod-to-run-on-a-specific-node","text":"In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kubernetes : pod : spec : nodeName : <insert node name here>","title":"Forcing the pod to run on a specific node"},{"location":"reference/kubernetes/#other-options","text":"Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it.","title":"Other options"},{"location":"reference/kubernetes/#configuration-restrictions","text":"In connection mode the idleCommand and shellCommand options are required. In session mode the restart policy must be empty or Never .","title":"Configuration restrictions"},{"location":"reference/kubernetes/#configuring-timeouts","text":"The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description podStart The time to wait for the pod to start. podStop The time to wait for the pod to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete.","title":"Configuring timeouts"},{"location":"reference/kubernetes/#securing-kubernetes","text":"Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment","title":"Securing Kubernetes"},{"location":"reference/kubernetes/#creating-a-service-account","text":"When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh:containerssh Let's test if the permissions are correct: $ kubectl auth can-i create pod --as containerssh no $ kubectl auth can-i create pod --namespace containerssh-guests --as containerssh yes Docker Desktop Docker Desktop Kubernetes contains a cluster role binding called docker-for-desktop-binding that allows all service accounts to perform every action. To secure your Docker Desktop installation you will need to delete this CRB.","title":"Creating a service account"},{"location":"reference/kubernetes/#deploying-inside-of-kubernetes","text":"When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kubernetes: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token","title":"Deploying inside of Kubernetes"},{"location":"reference/kubernetes/#deploying-outside-of-kubernetes","text":"Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : ZGVmYXVsdA== token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kubernetes : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here>","title":"Deploying outside of Kubernetes"},{"location":"reference/kubernetes/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kubernetes : pod : spec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kubernetes : pod : spec : securityContext : runAsUser : 1000","title":"Preventing root escalation"},{"location":"reference/kubernetes/#preventing-storage-exhaustion","text":"A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk.","title":"Preventing storage exhaustion"},{"location":"reference/kubernetes/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kubernetes : pod : spec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing memory exhaustion"},{"location":"reference/kubernetes/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kubernetes : pod : spec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing CPU exhaustion"},{"location":"reference/kubernetes/#limiting-network-access","text":"Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Limiting network access"},{"location":"reference/kuberun/","text":"The KubeRun backend Warning The KubeRun backend is deprecated and the Kubernetes backend should be used instead. Read the migration guide here \u00bb The KubeRun backend runs and is tested against all currently actively maintained Kubernetes versions . For ContainerSSH version 0.4 these are: 1.20, 1.19, and 1.18. Tip This is the documentation for the KubeRun backend . For deploying ContainerSSH inside Kubernetes please see the installation guide . The base configuration structure \u00b6 In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kuberun kuberun : connection : <connection configuration here> pod : <pod configuration here> Configuring connection parameters \u00b6 In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kuberun : connection : host : <...> <...> Base configuration \u00b6 Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. insecure bool Disable certificate verification on the Kubernetes API. This is a very bad idea as anyone on the network will be able to intercept your credentials. qps float32` Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. timeout string Timeout for pod operations in nanoseconds. Time units can be used. HTTP basic authentication (username and password) \u00b6 Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) x509 certificate authentication \u00b6 Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format. Bearer token authentication \u00b6 This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API. Pod configuration \u00b6 The pod configuration contains the information which pod to run. kuberun : pod : namespace : <namespace name> podSpec : <pod spec here> <ContainerSSH-specific options here> Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod.spec . Basic pod configuration \u00b6 ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kuberun : pod : namespace : default podSpec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world! Running multiple containers \u00b6 When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kuberun: pod: namespace: default consoleContainerNumber: 1 podSpec: containers: - name: container1 image: ... - name: container2 image: ... Mounting volumes \u00b6 In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage. Forcing the pod to run on a specific node \u00b6 In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kuberun : pod : podSpec : nodeName : <insert node name here> Other options \u00b6 Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. enableAgent bool Enable the ContainerSSH guest agent. This enables the ContainerSSH guest agent. subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. Securing Kubernetes \u00b6 Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment Creating a service account \u00b6 When deploying ContainerSSH with a Kubernetes backend you should never use an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace. First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh Deploying inside of Kubernetes \u00b6 When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kuberun: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token Deploying outside of Kubernetes \u00b6 Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : <base64-encoded namespace here> token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kuberun : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here> Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kuberun : pod : podSpec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kuberun : pod : podSpec : securityContext : runAsUser : 1000 Preventing storage exhaustion \u00b6 A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kuberun : pod : podSpec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation . Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kuberun : pod : podSpec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation . Limiting network access \u00b6 Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"KubeRun (deprecated)"},{"location":"reference/kuberun/#the-base-configuration-structure","text":"In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kuberun kuberun : connection : <connection configuration here> pod : <pod configuration here>","title":"The base configuration structure"},{"location":"reference/kuberun/#configuring-connection-parameters","text":"In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kuberun : connection : host : <...> <...>","title":"Configuring connection parameters"},{"location":"reference/kuberun/#base-configuration","text":"Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. insecure bool Disable certificate verification on the Kubernetes API. This is a very bad idea as anyone on the network will be able to intercept your credentials. qps float32` Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. timeout string Timeout for pod operations in nanoseconds. Time units can be used.","title":"Base configuration"},{"location":"reference/kuberun/#http-basic-authentication-username-and-password","text":"Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2)","title":"HTTP basic authentication (username and password)"},{"location":"reference/kuberun/#x509-certificate-authentication","text":"Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format.","title":"x509 certificate authentication"},{"location":"reference/kuberun/#bearer-token-authentication","text":"This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API.","title":"Bearer token authentication"},{"location":"reference/kuberun/#pod-configuration","text":"The pod configuration contains the information which pod to run. kuberun : pod : namespace : <namespace name> podSpec : <pod spec here> <ContainerSSH-specific options here> Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod.spec .","title":"Pod configuration"},{"location":"reference/kuberun/#basic-pod-configuration","text":"ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kuberun : pod : namespace : default podSpec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world!","title":"Basic pod configuration"},{"location":"reference/kuberun/#running-multiple-containers","text":"When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kuberun: pod: namespace: default consoleContainerNumber: 1 podSpec: containers: - name: container1 image: ... - name: container2 image: ...","title":"Running multiple containers"},{"location":"reference/kuberun/#mounting-volumes","text":"In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage.","title":"Mounting volumes"},{"location":"reference/kuberun/#forcing-the-pod-to-run-on-a-specific-node","text":"In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kuberun : pod : podSpec : nodeName : <insert node name here>","title":"Forcing the pod to run on a specific node"},{"location":"reference/kuberun/#other-options","text":"Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. enableAgent bool Enable the ContainerSSH guest agent. This enables the ContainerSSH guest agent. subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution.","title":"Other options"},{"location":"reference/kuberun/#securing-kubernetes","text":"Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment","title":"Securing Kubernetes"},{"location":"reference/kuberun/#creating-a-service-account","text":"When deploying ContainerSSH with a Kubernetes backend you should never use an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace. First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh","title":"Creating a service account"},{"location":"reference/kuberun/#deploying-inside-of-kubernetes","text":"When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kuberun: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token","title":"Deploying inside of Kubernetes"},{"location":"reference/kuberun/#deploying-outside-of-kubernetes","text":"Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : <base64-encoded namespace here> token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kuberun : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here>","title":"Deploying outside of Kubernetes"},{"location":"reference/kuberun/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kuberun : pod : podSpec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kuberun : pod : podSpec : securityContext : runAsUser : 1000","title":"Preventing root escalation"},{"location":"reference/kuberun/#preventing-storage-exhaustion","text":"A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk.","title":"Preventing storage exhaustion"},{"location":"reference/kuberun/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kuberun : pod : podSpec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing memory exhaustion"},{"location":"reference/kuberun/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kuberun : pod : podSpec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing CPU exhaustion"},{"location":"reference/kuberun/#limiting-network-access","text":"Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Limiting network access"},{"location":"reference/logging/","text":"Logging ContainerSSH comes with configurable logging facilities. You can configure the following options: The minimum log level to filter unnecessary log messages The log format The log destination Configuring the minimum log level \u00b6 You can configure the minimum log level by adding the following to your configuration: log : level : \"info\" Tip You can configure the log level on a per-user basis using the configuration server . The supported levels are in accordance with the Syslog standard: debug info notice warning error crit alert emerg Configuring the log format \u00b6 ContainerSSH can log in both text and newline-delimited JSON ( ljson ) format. You can change the format with the following setting: log : format : \"ljson\" The JSON log format \u00b6 The LJSON format outputs a JSON-formatted string per log message. For file and stdout these JSON messages are separated by a newline from each other. When writing to the stdout or file destinations the format is the following: { \"timestamp\" : \"Timestamp in RFC3339 format\" , \"level\" : \"the log level\" , \"code\" : \"ERROR_CODE_HERE\" , \"message\" : \"the message (optional)\" , \"details\" : { \"the detail object if any (optional)\" } } When writing to syslog the format is the same, but does not contain the timestamp and level fields as they are redundant. The error codes are documented in the troubleshooting section . The text log format \u00b6 When writing to the stdout or file destinations the text log format has the following fields delimited by a tab ( \\t ) character: TIMESTAMP\\tLEVEL\\tMESSAGE The TIMESTAMP will be formatted according to RFC3339, while the LEVEL will be the text-representation of the log level. The MESSAGE field will contain the text representation of the message. Warning The text message is not intended for machine processing and may change across versions. If you intend to do machine processing please use the details field from the ljson format. Setting the output \u00b6 The output configuration of ContainerSSH is the following: log : destination : \"stdout|file|syslog\" <other destination options> Writing to the stdout \u00b6 The stdout destination is the simplest: it will write error messages to the standard output of ContainerSSH. This is the default logging method. log : destination : \"stdout\" Writing to a file \u00b6 You can write to a specific file using the following options: log : destination : \"file\" file : \"/var/log/containerssh/containerssh.log\" ContainerSSH will write to the specified file in the specified format. Tip ContainerSSH supports log rotation . You can trigger a log rotation by sending a HUP (hangup) signal to ContainerSSH. Writing to syslog \u00b6 ContainerSSH supports writing to syslog via a Unix socket or UDP. TCP syslog is not supported due to the complexity of maintaining a stable connection and buffering messages between disconnects. Syslog can be configured as follows: log : destination : syslog syslog : # Change to IP and port for UDP destination : /dev/log # See below for supported names facility : auth # Program name to log as tag : ContainerSSH # Log PID with program name pid : false The following facilities are supported: kern user mail daemon auth syslog lpr news uucp cron authpriv ftp ntp logaudit logalert clock local0..7 Warning ContainerSSH can log very long messages with lots of details. Please make sure to bump your maximum line length to at least 4096 characters in your Syslog server to avoid message truncation. ( rsyslog and syslog-ng have higher default values, so you don't need to change the configuration if you are using these syslog servers.)","title":"Logging"},{"location":"reference/logging/#configuring-the-minimum-log-level","text":"You can configure the minimum log level by adding the following to your configuration: log : level : \"info\" Tip You can configure the log level on a per-user basis using the configuration server . The supported levels are in accordance with the Syslog standard: debug info notice warning error crit alert emerg","title":"Configuring the minimum log level"},{"location":"reference/logging/#configuring-the-log-format","text":"ContainerSSH can log in both text and newline-delimited JSON ( ljson ) format. You can change the format with the following setting: log : format : \"ljson\"","title":"Configuring the log format"},{"location":"reference/logging/#the-json-log-format","text":"The LJSON format outputs a JSON-formatted string per log message. For file and stdout these JSON messages are separated by a newline from each other. When writing to the stdout or file destinations the format is the following: { \"timestamp\" : \"Timestamp in RFC3339 format\" , \"level\" : \"the log level\" , \"code\" : \"ERROR_CODE_HERE\" , \"message\" : \"the message (optional)\" , \"details\" : { \"the detail object if any (optional)\" } } When writing to syslog the format is the same, but does not contain the timestamp and level fields as they are redundant. The error codes are documented in the troubleshooting section .","title":"The JSON log format"},{"location":"reference/logging/#the-text-log-format","text":"When writing to the stdout or file destinations the text log format has the following fields delimited by a tab ( \\t ) character: TIMESTAMP\\tLEVEL\\tMESSAGE The TIMESTAMP will be formatted according to RFC3339, while the LEVEL will be the text-representation of the log level. The MESSAGE field will contain the text representation of the message. Warning The text message is not intended for machine processing and may change across versions. If you intend to do machine processing please use the details field from the ljson format.","title":"The text log format"},{"location":"reference/logging/#setting-the-output","text":"The output configuration of ContainerSSH is the following: log : destination : \"stdout|file|syslog\" <other destination options>","title":"Setting the output"},{"location":"reference/logging/#writing-to-the-stdout","text":"The stdout destination is the simplest: it will write error messages to the standard output of ContainerSSH. This is the default logging method. log : destination : \"stdout\"","title":"Writing to the stdout"},{"location":"reference/logging/#writing-to-a-file","text":"You can write to a specific file using the following options: log : destination : \"file\" file : \"/var/log/containerssh/containerssh.log\" ContainerSSH will write to the specified file in the specified format. Tip ContainerSSH supports log rotation . You can trigger a log rotation by sending a HUP (hangup) signal to ContainerSSH.","title":"Writing to a file"},{"location":"reference/logging/#writing-to-syslog","text":"ContainerSSH supports writing to syslog via a Unix socket or UDP. TCP syslog is not supported due to the complexity of maintaining a stable connection and buffering messages between disconnects. Syslog can be configured as follows: log : destination : syslog syslog : # Change to IP and port for UDP destination : /dev/log # See below for supported names facility : auth # Program name to log as tag : ContainerSSH # Log PID with program name pid : false The following facilities are supported: kern user mail daemon auth syslog lpr news uucp cron authpriv ftp ntp logaudit logalert clock local0..7 Warning ContainerSSH can log very long messages with lots of details. Please make sure to bump your maximum line length to at least 4096 characters in your Syslog server to avoid message truncation. ( rsyslog and syslog-ng have higher default values, so you don't need to change the configuration if you are using these syslog servers.)","title":"Writing to syslog"},{"location":"reference/metrics/","text":"Metrics ContainerSSH contains a Prometheus -compatible metrics server which can be enabled using the following configuration: metrics : <options here> The metrics server has the following options: Option Type Description enable bool Enable metrics server. Defaults to false. path string HTTP path to serve metrics on. Defaults to /metrics . listen string IP and port to listen on. Defaults to 0.0.0.0:9100 . clientcacert string CA certificate in PEM format or filename that contains the CA certificate used for authenticating connecting clients. cert string Client certificate in PEM format or filename that contains the server certificate. key string Private key in PEM format or filename that contains the server certificate. tlsVersion []string Minimum TLS version to support. See the TLS version section below. curve []string Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. cipher []string,string Which cipher suites to support. See the Cipher suites section below. Available metrics \u00b6 You can configure Prometheus to grab the following metrics: containerssh_auth_server_failures Number of failed requests to the authentication server since start. containerssh_auth_success Number of successful authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_auth_failures Number of failed authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_config_server_failures Number of failed requests to the configuration server since start. containerssh_ssh_connections Number of SSH connections since start. Contains a label for country (see below). containerssh_ssh_handshake_successful Number of successful SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_handshake_failed Number of failed SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_current_connections Number of currently open SSH connections. Contains a label for country (see below). Country identification \u00b6 Country identification works using GeoIP2 or GeoLite2 from MaxMind . This database needs to be provided to ContainerSSH externally due to licensing concerns. The default path for the GeoIP database is /var/lib/GeoIP/GeoIP2-Country.mmdb , but you can change that using the following configuration snippet: geoip : provider : \"maxmind\" maxmind-geoip2-file : '/var/lib/GeoIP/GeoIP2-Country.mmdb' Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Elliptic curve algorithms \u00b6 The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1 Cipher suites \u00b6 The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Client authentication \u00b6 In order to safeguard the metrics ContainerSSH supports authenticating connecting clients using x509 mutual TLS authentication. For this you will need to generate a CA certificate and configure the metrics service with it, as well as client certificates that your connecting clients must use. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added to the metrics configuration: metrics : clientcacert : /path/to/ca.pem Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem can be used in your connecting client. For an example see the Prometheus documentation .","title":"Metrics"},{"location":"reference/metrics/#available-metrics","text":"You can configure Prometheus to grab the following metrics: containerssh_auth_server_failures Number of failed requests to the authentication server since start. containerssh_auth_success Number of successful authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_auth_failures Number of failed authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_config_server_failures Number of failed requests to the configuration server since start. containerssh_ssh_connections Number of SSH connections since start. Contains a label for country (see below). containerssh_ssh_handshake_successful Number of successful SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_handshake_failed Number of failed SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_current_connections Number of currently open SSH connections. Contains a label for country (see below).","title":"Available metrics"},{"location":"reference/metrics/#country-identification","text":"Country identification works using GeoIP2 or GeoLite2 from MaxMind . This database needs to be provided to ContainerSSH externally due to licensing concerns. The default path for the GeoIP database is /var/lib/GeoIP/GeoIP2-Country.mmdb , but you can change that using the following configuration snippet: geoip : provider : \"maxmind\" maxmind-geoip2-file : '/var/lib/GeoIP/GeoIP2-Country.mmdb'","title":"Country identification"},{"location":"reference/metrics/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/metrics/#tls-version","text":"The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported.","title":"TLS version"},{"location":"reference/metrics/#elliptic-curve-algorithms","text":"The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1","title":"Elliptic curve algorithms"},{"location":"reference/metrics/#cipher-suites","text":"The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305","title":"Cipher suites"},{"location":"reference/metrics/#client-authentication","text":"In order to safeguard the metrics ContainerSSH supports authenticating connecting clients using x509 mutual TLS authentication. For this you will need to generate a CA certificate and configure the metrics service with it, as well as client certificates that your connecting clients must use. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added to the metrics configuration: metrics : clientcacert : /path/to/ca.pem Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem can be used in your connecting client. For an example see the Prometheus documentation .","title":"Client authentication"},{"location":"reference/security/","text":"Security configuration The security module provides the ability to limit or force the behavior of SSH. It can be configured using the following structure: security : maxSessions : 10 forceCommand : \"/run/this/command\" defaultMode : enable|filter|disable env : mode : enable|filter|disable allow : - ENV_VARIABLE_NAME deny : - ENV_VARIABLE_NAME command : mode : enable|filter|disable allow : - /allow/this/command shell : mode : enable|disable subsystem : mode : enable|filter|disable allow : - <enable-this-subsystem> deny : - <disable-this-subsystem> tty : mode : enable|disable signal : mode : enable|filter|disable allow : - TERM deny : - KILL Maximum sessions \u00b6 The maxSessions option lets you limit the number of parallel sessions a client can open. When this number of sessions is reached further session requests are rejected until a session is closed. The recommended value for this option is 10 . Forcing commands \u00b6 The forceCommand option lets you force the execution of a command even when the client has specified a different command to be run. This turns all shell and subsystem requests into command execution requests to run the specified command. The original command will be available in the SSH_ORIGINAL_COMMAND environment variable. Filtering requests \u00b6 SSH allows a client to request a multitude of things. The security module allows you to either enable, filter, or deny requests. allow will allow all requests except the ones specified in the deny list. filter will only allow requests specified in the allow list. deny denies all requests. You can configure the settings either individually, or using the defaultMode setting. It is strongly recommended to set a default mode so future ContainerSSH versions adding new features don't accidentally allow something you don't want to enable. Environment variable filtering \u00b6 Using the env option you can filter which environment variables the client can set. In enable mode you can deny specific environment variables by specifying disallowed variables in the deny list. In filter mode you can specify allowed variables in the allow list. If you want to completely disable setting environment variables you can set the mode to disable . Command execution \u00b6 A client can explicitly request running a specific command by specifying it in the command line: ssh yourserver.com run-this-program In enable mode command execution is enabled with no filtering. There is no deny option as it is trivially easy to work around a simple matching. In filter mode only the commands that are specified in the allow list are executed. The match must be exact. In deny mode all command execution is disabled. Shell execution \u00b6 When not specifying a command to the SSH client by default a shell is launched. You can only set the enable and disable modes. The filter mode is valid, but is equal to the disable mode. Subsystem execution \u00b6 SSH clients can also execute well-known subsystems, such as sftp . The server then decides which binary to execute for the requested subsystem. When set to enable all subsystems except the ones in the deny list. In filter mode only subsystems in the allow list are allowed. In deny mode no subsystem execution is allowed. TTY/PTY requests \u00b6 When a client wants to use the SSH server interactively they can send a PTY request to the server before executing the program. The only security options for TTY are enable and disable . filter mode is not explicitly invalid, but behaves like deny . Signals \u00b6 Although not used very often, SSH clients can request signals to be delivered to the running program. In enable mode all signals except for the ones listed in the deny list are allowed. In filter mode only the signals in the allow list are allowed. In disable mode no signal delivery is allowed. Warning Signal names have to be specified without the SIG prefix. ContainerSSH supports the following signals: ABRT ALRM FPE HUP ILL INT KILL PIPE QUIT SEGV TERM USR1 USR2","title":"Restrictions"},{"location":"reference/security/#maximum-sessions","text":"The maxSessions option lets you limit the number of parallel sessions a client can open. When this number of sessions is reached further session requests are rejected until a session is closed. The recommended value for this option is 10 .","title":"Maximum sessions"},{"location":"reference/security/#forcing-commands","text":"The forceCommand option lets you force the execution of a command even when the client has specified a different command to be run. This turns all shell and subsystem requests into command execution requests to run the specified command. The original command will be available in the SSH_ORIGINAL_COMMAND environment variable.","title":"Forcing commands"},{"location":"reference/security/#filtering-requests","text":"SSH allows a client to request a multitude of things. The security module allows you to either enable, filter, or deny requests. allow will allow all requests except the ones specified in the deny list. filter will only allow requests specified in the allow list. deny denies all requests. You can configure the settings either individually, or using the defaultMode setting. It is strongly recommended to set a default mode so future ContainerSSH versions adding new features don't accidentally allow something you don't want to enable.","title":"Filtering requests"},{"location":"reference/security/#environment-variable-filtering","text":"Using the env option you can filter which environment variables the client can set. In enable mode you can deny specific environment variables by specifying disallowed variables in the deny list. In filter mode you can specify allowed variables in the allow list. If you want to completely disable setting environment variables you can set the mode to disable .","title":"Environment variable filtering"},{"location":"reference/security/#command-execution","text":"A client can explicitly request running a specific command by specifying it in the command line: ssh yourserver.com run-this-program In enable mode command execution is enabled with no filtering. There is no deny option as it is trivially easy to work around a simple matching. In filter mode only the commands that are specified in the allow list are executed. The match must be exact. In deny mode all command execution is disabled.","title":"Command execution"},{"location":"reference/security/#shell-execution","text":"When not specifying a command to the SSH client by default a shell is launched. You can only set the enable and disable modes. The filter mode is valid, but is equal to the disable mode.","title":"Shell execution"},{"location":"reference/security/#subsystem-execution","text":"SSH clients can also execute well-known subsystems, such as sftp . The server then decides which binary to execute for the requested subsystem. When set to enable all subsystems except the ones in the deny list. In filter mode only subsystems in the allow list are allowed. In deny mode no subsystem execution is allowed.","title":"Subsystem execution"},{"location":"reference/security/#ttypty-requests","text":"When a client wants to use the SSH server interactively they can send a PTY request to the server before executing the program. The only security options for TTY are enable and disable . filter mode is not explicitly invalid, but behaves like deny .","title":"TTY/PTY requests"},{"location":"reference/security/#signals","text":"Although not used very often, SSH clients can request signals to be delivered to the running program. In enable mode all signals except for the ones listed in the deny list are allowed. In filter mode only the signals in the allow list are allowed. In disable mode no signal delivery is allowed. Warning Signal names have to be specified without the SIG prefix. ContainerSSH supports the following signals: ABRT ALRM FPE HUP ILL INT KILL PIPE QUIT SEGV TERM USR1 USR2","title":"Signals"},{"location":"reference/ssh/","text":"SSH configuration SSH is the main service of ContainerSSH. It has the following configuration structure: ssh : <options> The options are as follows: Name Type Description listen string IP and port pair to bind the SSH service to. Defaults to 0.0.0.0:2222 serverVersion string Server version string presented to any connecting client. Must start with SSH-2.0- . Defaults to SSH-2.0-ContainerSSH . cipher []string List of ciphers the server should support. See the Ciphers section below. kex []string List of key exchange algorithms the server should support. See the Key exchange section below. macs []string List of MAC algorithms the server should support. See the MAC section below. banner []string The banner text to presented to any connecting client. hostkeys []string List of host keys in PEM format, or file names to read the key from. Generate with openssl genrsa Configuring the server version \u00b6 The SSH server version is presented to any connecting client in plain text upon connection. It has the following format: SSH-2.0-softwareversion <SP> comments The softwareversion can only contain printable US-ASCII characters without whitespace and minus ( - ) signs. The comments field is optional and is separated from the softwareversion with a single space. The maximum length of the version string is 255 characters. Configuring a banner \u00b6 SSH offers the ability to output a message to the clients before they enter passwords. This can be configured in the banner option. The banner can contain multiple lines. Ciphers \u00b6 ContainerSSH supports the following ciphers. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID Key exchange \u00b6 ContainerSSH supports the following key exchange algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1 MAC \u00b6 ContainerSSH supports the following MAC algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96","title":"Configuration"},{"location":"reference/ssh/#configuring-the-server-version","text":"The SSH server version is presented to any connecting client in plain text upon connection. It has the following format: SSH-2.0-softwareversion <SP> comments The softwareversion can only contain printable US-ASCII characters without whitespace and minus ( - ) signs. The comments field is optional and is separated from the softwareversion with a single space. The maximum length of the version string is 255 characters.","title":"Configuring the server version"},{"location":"reference/ssh/#configuring-a-banner","text":"SSH offers the ability to output a message to the clients before they enter passwords. This can be configured in the banner option. The banner can contain multiple lines.","title":"Configuring a banner"},{"location":"reference/ssh/#ciphers","text":"ContainerSSH supports the following ciphers. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID","title":"Ciphers"},{"location":"reference/ssh/#key-exchange","text":"ContainerSSH supports the following key exchange algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1","title":"Key exchange"},{"location":"reference/ssh/#mac","text":"ContainerSSH supports the following MAC algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96","title":"MAC"},{"location":"reference/sshproxy/","text":"The SSH proxy backend The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend. This allows for using the audit log to inspect SSH traffic, or to dynamically forwarding connections using the configuration webhook . The base configuration structure \u00b6 The minimum configuration is the following: backend : sshproxy sshproxy : # Add the backend server here server : 127.0.0.1 # Set the following option to true to reuse the connecting user's username. usernamePassThrough : true # Or specify a username manually username : root # Specify the password password : changeme # Or the private key. This can reference a file or be added directly. privateKey : | -----BEGIN OPENSSH PRIVATE KEY----- ... # Provide all fingerprints of the backing SSH server's host keys: allowedHostKeyFingerprints : - SHA256:... Tip You can obtain the fingerprints of OpenSSH host keys by running the following script: for i in /etc/ssh/ssh_host_*.pub; do ssh-keygen -l -f $i; done | cut -d ' ' -f 2 Warning ContainerSSH does not support passing through passwords or public key authentication to the backing server. We recommend setting up private-public key authentication with the backing server. Configuration options \u00b6 Option Type Description server string Host name or IP address of the backing SSH server. Required. port uint16 Port number of the backing SSH service. Defaults to 22. usernamePassThrough bool Take username from the connecting client. username string Explicitly set the username to use for the backing connection. Required if usernamePassThrough is false . password string Password to use to authenticate with the backing SSH server. privateKey string Private key to use to authenticate with the backing SSH server. Can be a reference to a file or the private key in PEM or OpenSSH format. allowedHostKeyFingerprints []string List of SHA256 fingerprints of the backing SSH server. ciphers []string List of SSH ciphers to use. See Ciphers below. kex []string List of key exchange algorithms to use. See Key exchange algorithms below. macs []string List of MAC algorithms to use. See MAC algorithms below. hostKeyAlgorithms []string List of host key algorithms to request from the backing server. See Host key algorithms below. timeout string Timeout for connecting / retrying the SSH connection. clientVersion string Client version string to send to the backing server. Must be in the format of SSH-protoversion-softwareversion SPACE comments . See RFC 4235 section 4.2. Protocol Version Exchange for details. The trailing CR and LF characters should not be added to this string. Ciphers \u00b6 ContainerSSH supports the following ciphers for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID Key exchange algorithms \u00b6 ContainerSSH supports the following key exchange algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1 MAC algorithms \u00b6 ContainerSSH supports the following MAC algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96 Host key algorithms \u00b6 ContainerSSH supports the following host key algorithms for verifying the backing server identity. Algorithm Default ssh-rsa-cert-v01@openssh.com ssh-dss-cert-v01@openssh.com ecdsa-sha2-nistp256-cert-v01@openssh.com ecdsa-sha2-nistp384-cert-v01@openssh.com ecdsa-sha2-nistp521-cert-v01@openssh.com ssh-ed25519-cert-v01@openssh.com ssh-rsa ssh-dss ssh-ed25519","title":"SSH proxy"},{"location":"reference/sshproxy/#the-base-configuration-structure","text":"The minimum configuration is the following: backend : sshproxy sshproxy : # Add the backend server here server : 127.0.0.1 # Set the following option to true to reuse the connecting user's username. usernamePassThrough : true # Or specify a username manually username : root # Specify the password password : changeme # Or the private key. This can reference a file or be added directly. privateKey : | -----BEGIN OPENSSH PRIVATE KEY----- ... # Provide all fingerprints of the backing SSH server's host keys: allowedHostKeyFingerprints : - SHA256:... Tip You can obtain the fingerprints of OpenSSH host keys by running the following script: for i in /etc/ssh/ssh_host_*.pub; do ssh-keygen -l -f $i; done | cut -d ' ' -f 2 Warning ContainerSSH does not support passing through passwords or public key authentication to the backing server. We recommend setting up private-public key authentication with the backing server.","title":"The base configuration structure"},{"location":"reference/sshproxy/#configuration-options","text":"Option Type Description server string Host name or IP address of the backing SSH server. Required. port uint16 Port number of the backing SSH service. Defaults to 22. usernamePassThrough bool Take username from the connecting client. username string Explicitly set the username to use for the backing connection. Required if usernamePassThrough is false . password string Password to use to authenticate with the backing SSH server. privateKey string Private key to use to authenticate with the backing SSH server. Can be a reference to a file or the private key in PEM or OpenSSH format. allowedHostKeyFingerprints []string List of SHA256 fingerprints of the backing SSH server. ciphers []string List of SSH ciphers to use. See Ciphers below. kex []string List of key exchange algorithms to use. See Key exchange algorithms below. macs []string List of MAC algorithms to use. See MAC algorithms below. hostKeyAlgorithms []string List of host key algorithms to request from the backing server. See Host key algorithms below. timeout string Timeout for connecting / retrying the SSH connection. clientVersion string Client version string to send to the backing server. Must be in the format of SSH-protoversion-softwareversion SPACE comments . See RFC 4235 section 4.2. Protocol Version Exchange for details. The trailing CR and LF characters should not be added to this string.","title":"Configuration options"},{"location":"reference/sshproxy/#ciphers","text":"ContainerSSH supports the following ciphers for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID","title":"Ciphers"},{"location":"reference/sshproxy/#key-exchange-algorithms","text":"ContainerSSH supports the following key exchange algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1","title":"Key exchange algorithms"},{"location":"reference/sshproxy/#mac-algorithms","text":"ContainerSSH supports the following MAC algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96","title":"MAC algorithms"},{"location":"reference/sshproxy/#host-key-algorithms","text":"ContainerSSH supports the following host key algorithms for verifying the backing server identity. Algorithm Default ssh-rsa-cert-v01@openssh.com ssh-dss-cert-v01@openssh.com ecdsa-sha2-nistp256-cert-v01@openssh.com ecdsa-sha2-nistp384-cert-v01@openssh.com ecdsa-sha2-nistp521-cert-v01@openssh.com ssh-ed25519-cert-v01@openssh.com ssh-rsa ssh-dss ssh-ed25519","title":"Host key algorithms"},{"location":"reference/troubleshooting/","text":"Troubleshooting ContainerSSH When ContainerSSH doesn't work several components can be involved. Is it ContainerSSH? Can it connect to the authentication server ? How about the configuration server ? Is the backend working ? This guide will lead you through the steps of debugging issues with ContainerSSH. Turning on debug logging \u00b6 The first step in determining any potential issues is to enable debug logging . This will output a slew of log messages into the designated log destination that you can use to determine what's going wrong. If you are trying to debug production failures with lots of traffic it can sometimes be hard to read these logs. That's why a useful field in all connection-related messages is the connectionId field. This can be used to tie related log messages together. If your configuration server is flexible enough you can pass the debug log level on a per-user basis to increase the log verbosity for a single user only. Each message has a unique code. The list of codes is documented in the codes section . Connecting in debug mode \u00b6 Most SSH clients have a debug mode. In the command line SSH you can add the -vvv flag to get a lot more verbose output: ssh youruser@youserver.com -vvv Turning on audit logging \u00b6 Another useful tool to turn on is audit logging . Audit logging can be another helpful tool when trying to figure out what a certain user is doing. It can record every single interaction that happens over SSH. More importantly, audit logs are also tied to the connectionId that is also present in the logs above. Debugging webhook server failures \u00b6 Apart from turning on logging you can also use network monitoring to debug authentication and configuration webhook failures. Even if the connection itself is encrypted, using a tool like tcpdump or Wireshark can give you useful clues if the connection is even established correctly or if something is failing on a connection level. Debugging container backend failures \u00b6 When it comes to interacting with container backend you can also use logs as well as network monitoring to determine basic failures. Furthermore, both Docker and Kubernetes provide the ability to monitor events that are happening to get an idea of what's going on. Docker docker events Kubernetes kubectl get events --watch Debugging with strace \u00b6 If none of the above steps help it is time to unpack the big tools. strace is a Linux utility that lets you list all system calls ContainerSSH is doing. This is not the easiest log to read, but the following snippet should help: strace \\ -e trace=open,read,write,readv,writev,recv,recvfrom,send,sendto \\ -s 999 \\ -p CONTAINERSSH-PID-HERE If all else fails: ask for help \u00b6 If all else fails we are here for you. Please collect the following items: The debug logs from above. Your configuration file without any sensitive details (credentials, IPs). Please also prepare the following items if you can, but don't submit them as they may contain sensitive credentials: Audit logs (if you have them). Any pcap files from tcpdump or Wireshark you may have. Any strace outputs you may have. Any Docker or Kubernetes events you may have recorded. You can raise your question in one of the following channels: As a GitHub issue. As a discussion post. On the debugged.it Discord. Please link the debug logs and configuration from a GitHub Gist or a Pastebin . Don't worry about submitting duplicate issues, just make sure to describe your issue in as much detail as possible.","title":"Troubleshooting guide"},{"location":"reference/troubleshooting/#turning-on-debug-logging","text":"The first step in determining any potential issues is to enable debug logging . This will output a slew of log messages into the designated log destination that you can use to determine what's going wrong. If you are trying to debug production failures with lots of traffic it can sometimes be hard to read these logs. That's why a useful field in all connection-related messages is the connectionId field. This can be used to tie related log messages together. If your configuration server is flexible enough you can pass the debug log level on a per-user basis to increase the log verbosity for a single user only. Each message has a unique code. The list of codes is documented in the codes section .","title":"Turning on debug logging"},{"location":"reference/troubleshooting/#connecting-in-debug-mode","text":"Most SSH clients have a debug mode. In the command line SSH you can add the -vvv flag to get a lot more verbose output: ssh youruser@youserver.com -vvv","title":"Connecting in debug mode"},{"location":"reference/troubleshooting/#turning-on-audit-logging","text":"Another useful tool to turn on is audit logging . Audit logging can be another helpful tool when trying to figure out what a certain user is doing. It can record every single interaction that happens over SSH. More importantly, audit logs are also tied to the connectionId that is also present in the logs above.","title":"Turning on audit logging"},{"location":"reference/troubleshooting/#debugging-webhook-server-failures","text":"Apart from turning on logging you can also use network monitoring to debug authentication and configuration webhook failures. Even if the connection itself is encrypted, using a tool like tcpdump or Wireshark can give you useful clues if the connection is even established correctly or if something is failing on a connection level.","title":"Debugging webhook server failures"},{"location":"reference/troubleshooting/#debugging-container-backend-failures","text":"When it comes to interacting with container backend you can also use logs as well as network monitoring to determine basic failures. Furthermore, both Docker and Kubernetes provide the ability to monitor events that are happening to get an idea of what's going on. Docker docker events Kubernetes kubectl get events --watch","title":"Debugging container backend failures"},{"location":"reference/troubleshooting/#debugging-with-strace","text":"If none of the above steps help it is time to unpack the big tools. strace is a Linux utility that lets you list all system calls ContainerSSH is doing. This is not the easiest log to read, but the following snippet should help: strace \\ -e trace=open,read,write,readv,writev,recv,recvfrom,send,sendto \\ -s 999 \\ -p CONTAINERSSH-PID-HERE","title":"Debugging with strace"},{"location":"reference/troubleshooting/#if-all-else-fails-ask-for-help","text":"If all else fails we are here for you. Please collect the following items: The debug logs from above. Your configuration file without any sensitive details (credentials, IPs). Please also prepare the following items if you can, but don't submit them as they may contain sensitive credentials: Audit logs (if you have them). Any pcap files from tcpdump or Wireshark you may have. Any strace outputs you may have. Any Docker or Kubernetes events you may have recorded. You can raise your question in one of the following channels: As a GitHub issue. As a discussion post. On the debugged.it Discord. Please link the debug logs and configuration from a GitHub Gist or a Pastebin . Don't worry about submitting duplicate issues, just make sure to describe your issue in as much detail as possible.","title":"If all else fails: ask for help"},{"location":"reference/0.3/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb ContainerSSH Reference Manual The Reference Manual provides reference material for ContainerSSH 0.3 and is oriented towards system operators wishing to use ContainerSSH on their system. Introduction \u00b6 This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation. Changes since ContainerSSH 0.2 \u00b6 This release adds the new metrics service which exposes several key metrics in a Prometheus-compatible format. It also adds configurable logging and an option to disable command execution in the DockerRun and KubeRun backends. This is the first version of the Reference Manual, previous versions did not have a manual. The changes from previous versions can be found on GitHub: 0.2.2 0.2.1 0.2.0 0.1.1","title":"Overview"},{"location":"reference/0.3/#introduction","text":"This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation.","title":"Introduction"},{"location":"reference/0.3/#changes-since-containerssh-02","text":"This release adds the new metrics service which exposes several key metrics in a Prometheus-compatible format. It also adds configurable logging and an option to disable command execution in the DockerRun and KubeRun backends. This is the first version of the Reference Manual, previous versions did not have a manual. The changes from previous versions can be found on GitHub: 0.2.2 0.2.1 0.2.0 0.1.1","title":"Changes since ContainerSSH 0.2"},{"location":"reference/0.3/auth/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Authentication ContainerSSH does not know your users and their passwords. Therefore, it calls out to a microservice that you have to provide. Your service can verify the users, passwords, and SSH keys. You will have to provide the microservice URL in the configuration. Configuration \u00b6 The authentication webhook can be configured in the main configuration using the following structure: auth : <options> The following options are supported: Name Type Description password bool Enable password authentication. pubkey bool Enable public key authentication. url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum TLS version for ContainerSSH 0.3 is 1.3. Server certificates must use Subject Alternative Names (SAN's) for proper server verification. Client authentication \u00b6 In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem The authentication webhook \u00b6 The authentication webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Password authentication \u00b6 On password authentication the authentication server will receive the following request to the /password endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"sessionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" } Public key authentication \u00b6 On public key authentication the authentication server will receive the following request to the /pubkey endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"sessionId\" : \"An opaque ID for the SSH connection\" , \"publicKeyBase64\" : \"Base64-encoded public key in the OpenSSH wire format\" } The public key will be sent in the authorized key format. Response \u00b6 Both endpoints need to respond with an application/json response of the following content: { \"success\" : true }","title":"Authentication"},{"location":"reference/0.3/auth/#configuration","text":"The authentication webhook can be configured in the main configuration using the following structure: auth : <options> The following options are supported: Name Type Description password bool Enable password authentication. pubkey bool Enable public key authentication. url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server.","title":"Configuration"},{"location":"reference/0.3/auth/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/0.3/auth/#tls-version","text":"The minimum TLS version for ContainerSSH 0.3 is 1.3. Server certificates must use Subject Alternative Names (SAN's) for proper server verification.","title":"TLS version"},{"location":"reference/0.3/auth/#client-authentication","text":"In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem","title":"Client authentication"},{"location":"reference/0.3/auth/#the-authentication-webhook","text":"The authentication webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code.","title":"The authentication webhook"},{"location":"reference/0.3/auth/#password-authentication","text":"On password authentication the authentication server will receive the following request to the /password endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"sessionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" }","title":"Password authentication"},{"location":"reference/0.3/auth/#public-key-authentication","text":"On public key authentication the authentication server will receive the following request to the /pubkey endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"sessionId\" : \"An opaque ID for the SSH connection\" , \"publicKeyBase64\" : \"Base64-encoded public key in the OpenSSH wire format\" } The public key will be sent in the authorized key format.","title":"Public key authentication"},{"location":"reference/0.3/auth/#response","text":"Both endpoints need to respond with an application/json response of the following content: { \"success\" : true }","title":"Response"},{"location":"reference/0.3/backends/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Backend selection ContainerSSH is built to support multiple backends. The backend can be changed in the configuration file: # change to `kubernetes` to talk to Kubernetes backend : docker","title":"Backend selection"},{"location":"reference/0.3/configserver/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Configuration Server ContainerSSH has the ability to configure the backend and the launched container dynamically based on the username and/or IP address. To do this ContainerSSH calls out to a configuration server if configured. Configuration \u00b6 The configserver webhook can be configured in the main configuration using the following structure: configuration : <options> The following options are supported: Name Type Description url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum TLS version for ContainerSSH 0.3 is 1.3. Client authentication \u00b6 In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem The configuration webhook \u00b6 The configuration webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. The config server will receive a request in following format: { \"username\" : \"ssh username\" , \"sessionId\" : \"ssh session ID\" } The configuration server will have to respond with the following response accompanied with the content type of application/json . { \"config\" : { // Provide a par t ial co nf igura t io n here } } The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running ./containerssh --dump-config . The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file. Currently only the following options can be set from the configuration server: Backend KubeRun DockerRun","title":"Configuration server"},{"location":"reference/0.3/configserver/#configuration","text":"The configserver webhook can be configured in the main configuration using the following structure: configuration : <options> The following options are supported: Name Type Description url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server.","title":"Configuration"},{"location":"reference/0.3/configserver/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/0.3/configserver/#tls-version","text":"The minimum TLS version for ContainerSSH 0.3 is 1.3.","title":"TLS version"},{"location":"reference/0.3/configserver/#client-authentication","text":"In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem","title":"Client authentication"},{"location":"reference/0.3/configserver/#the-configuration-webhook","text":"The configuration webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. The config server will receive a request in following format: { \"username\" : \"ssh username\" , \"sessionId\" : \"ssh session ID\" } The configuration server will have to respond with the following response accompanied with the content type of application/json . { \"config\" : { // Provide a par t ial co nf igura t io n here } } The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running ./containerssh --dump-config . The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file. Currently only the following options can be set from the configuration server: Backend KubeRun DockerRun","title":"The configuration webhook"},{"location":"reference/0.3/dockerrun/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The DockerRun backend The DockerRun backend should work with any Docker Engine version starting with 1.6 thanks to the version negotiation present. We fix issues starting with Docker version 18.02. The base configuration structure \u00b6 In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : dockerrun dockerrun : <connection configuration here> config : <execution configuration here> Configuring connection parameters \u00b6 The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Configuring container execution \u00b6 Container execution options can be specified as follows: docker : config : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , and network options contain settings described in the Docker API . The containerName option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases. Basic container configuration \u00b6 The basic configuration options are as follows: docker : config : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu Mounting volumes \u00b6 Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs Bind mounts \u00b6 Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : config : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume. Mounts \u00b6 The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : config : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755 tmpfs \u00b6 The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : config : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page . Other options \u00b6 Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. timeout string Timeout for container operations in nanoseconds. Time units can be set. Securing Docker \u00b6 Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server. Securing the Docker socket \u00b6 Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates! Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : config : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : config : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty. Preventing storage exhaustion \u00b6 A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : config : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : config : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : config : host : readonlyrootfs : true Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to crash the server. This can be prevented using the following configuration: docker : config : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the totoal memory + swap usage: docker : config : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : config : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : config : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000 Preventing process exhaustion \u00b6 You can also limit the number of processes that can be launched inside the container: docker : config : host : resources : pidslimit : 1000 Limiting network access \u00b6 In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : config : host : networkDisabled : true Limiting disk I/O \u00b6 Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : config : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"DockerRun"},{"location":"reference/0.3/dockerrun/#the-base-configuration-structure","text":"In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : dockerrun dockerrun : <connection configuration here> config : <execution configuration here>","title":"The base configuration structure"},{"location":"reference/0.3/dockerrun/#configuring-connection-parameters","text":"The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE-----","title":"Configuring connection parameters"},{"location":"reference/0.3/dockerrun/#configuring-container-execution","text":"Container execution options can be specified as follows: docker : config : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , and network options contain settings described in the Docker API . The containerName option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases.","title":"Configuring container execution"},{"location":"reference/0.3/dockerrun/#basic-container-configuration","text":"The basic configuration options are as follows: docker : config : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu","title":"Basic container configuration"},{"location":"reference/0.3/dockerrun/#mounting-volumes","text":"Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs","title":"Mounting volumes"},{"location":"reference/0.3/dockerrun/#bind-mounts","text":"Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : config : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume.","title":"Bind mounts"},{"location":"reference/0.3/dockerrun/#mounts","text":"The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : config : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755","title":"Mounts"},{"location":"reference/0.3/dockerrun/#tmpfs","text":"The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : config : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page .","title":"tmpfs"},{"location":"reference/0.3/dockerrun/#other-options","text":"Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. timeout string Timeout for container operations in nanoseconds. Time units can be set.","title":"Other options"},{"location":"reference/0.3/dockerrun/#securing-docker","text":"Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server.","title":"Securing Docker"},{"location":"reference/0.3/dockerrun/#securing-the-docker-socket","text":"Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates!","title":"Securing the Docker socket"},{"location":"reference/0.3/dockerrun/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : config : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : config : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty.","title":"Preventing root escalation"},{"location":"reference/0.3/dockerrun/#preventing-storage-exhaustion","text":"A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : config : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : config : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : config : host : readonlyrootfs : true","title":"Preventing storage exhaustion"},{"location":"reference/0.3/dockerrun/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to crash the server. This can be prevented using the following configuration: docker : config : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the totoal memory + swap usage: docker : config : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : config : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true","title":"Preventing memory exhaustion"},{"location":"reference/0.3/dockerrun/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : config : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000","title":"Preventing CPU exhaustion"},{"location":"reference/0.3/dockerrun/#preventing-process-exhaustion","text":"You can also limit the number of processes that can be launched inside the container: docker : config : host : resources : pidslimit : 1000","title":"Preventing process exhaustion"},{"location":"reference/0.3/dockerrun/#limiting-network-access","text":"In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : config : host : networkDisabled : true","title":"Limiting network access"},{"location":"reference/0.3/dockerrun/#limiting-disk-io","text":"Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : config : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Limiting disk I/O"},{"location":"reference/0.3/hardening/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Hardening ContainerSSH ContainerSSH is built to secure its inner workings as much as possible. You can take several steps to secure it further. Running ContainerSSH \u00b6 The default ContainerSSH image runs as a non-root user by default and exposes itself on port 2222. If you decide to build your own installation make sure ContainerSSH does not run as root as it is not required. Secure your Docker/Kubernetes \u00b6 Depending on which backend you are using you have to take different steps to secure it. When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman When running Kubernetes it is strongly advised that you deploy a pod security policy and a network policy. You should also make sure that ContainerSSH uses a restricted service account that can only access its own namespace. Securing your auth server \u00b6 Your authentication server contains all your secrets and is therefore a prime target. ContainerSSH delegates any and all access checking to the authentication server. Therefore, you need to make sure it prevents brute force attacks. Furthermore, you should make sure that the authentication server cannot be accessed from anywhere else. You can do this using firewalls. Alternatively, you can configure ContainerSSH to use client certificates to authenticate itself: auth : url : http://127.0.0.1:8080 cacert : \"insert your expected CA certificate in PEM format here\" timeout : 2s cert : \"insert your client certificate in PEM format here\" key : \"insert your client key in PEM format here\" Securing your config server \u00b6 Similar to your authentication server you can also secure the config server in a similar manner: configserver : timeout : 2s url : http://127.0.0.1:8080/config cacert : \"insert your expected CA certificate in PEM format here\" cert : \"insert your client certificate in PEM format here\" key : \"insert your client key in PEM format here\" Disabling command execution \u00b6 You can disable the execution of custom SSH commands through the configuration: dockerrun : config : disableCommand : true kuberun : pod : disableCommand : true Note Disabling command execution also disables SFTP integration.","title":"Hardening guide"},{"location":"reference/0.3/hardening/#running-containerssh","text":"The default ContainerSSH image runs as a non-root user by default and exposes itself on port 2222. If you decide to build your own installation make sure ContainerSSH does not run as root as it is not required.","title":"Running ContainerSSH"},{"location":"reference/0.3/hardening/#secure-your-dockerkubernetes","text":"Depending on which backend you are using you have to take different steps to secure it. When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman When running Kubernetes it is strongly advised that you deploy a pod security policy and a network policy. You should also make sure that ContainerSSH uses a restricted service account that can only access its own namespace.","title":"Secure your Docker/Kubernetes"},{"location":"reference/0.3/hardening/#securing-your-auth-server","text":"Your authentication server contains all your secrets and is therefore a prime target. ContainerSSH delegates any and all access checking to the authentication server. Therefore, you need to make sure it prevents brute force attacks. Furthermore, you should make sure that the authentication server cannot be accessed from anywhere else. You can do this using firewalls. Alternatively, you can configure ContainerSSH to use client certificates to authenticate itself: auth : url : http://127.0.0.1:8080 cacert : \"insert your expected CA certificate in PEM format here\" timeout : 2s cert : \"insert your client certificate in PEM format here\" key : \"insert your client key in PEM format here\"","title":"Securing your auth server"},{"location":"reference/0.3/hardening/#securing-your-config-server","text":"Similar to your authentication server you can also secure the config server in a similar manner: configserver : timeout : 2s url : http://127.0.0.1:8080/config cacert : \"insert your expected CA certificate in PEM format here\" cert : \"insert your client certificate in PEM format here\" key : \"insert your client key in PEM format here\"","title":"Securing your config server"},{"location":"reference/0.3/hardening/#disabling-command-execution","text":"You can disable the execution of custom SSH commands through the configuration: dockerrun : config : disableCommand : true kuberun : pod : disableCommand : true Note Disabling command execution also disables SFTP integration.","title":"Disabling command execution"},{"location":"reference/0.3/image/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Building a container image for ContainerSSH ContainerSSH can run any Linux container image. If you wish to use SFTP you have to add an SFTP server ( apt install openssh-sftp-server on Ubuntu) to the container image and configure the path of the SFTP server correctly in your config.yaml. The sample image containerssh/containerssh-guest-image contains an SFTP server.","title":"Creating a guest image"},{"location":"reference/0.3/installation/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Installation Standalone ContainerSSH can be deployed outside of a container. On our downloads page we provide binaries for Linux, Windows, and MacOS. We also provide DEB and RPM packages. Before running ContainerSSH you will need to create a config.yaml file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /path/to/your/host.key auth : url : http://your-auth-server/ Tip You can generate a new host key using openssl genrsa . Please don't use ssh-keygen as it generates OpenSSH-specific keys. Tip Details about the authentication server are described in the Authentication section . ContainerSSH can then be started by running ./containerssh --config /path/to/your/config.yaml Docker When deploying in Docker you must first prepare a configuration file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /var/run/secrets/host.key auth : url : http://your-auth-server/ Tip You can generate a new host key using openssl genrsa Tip Details about the authentication server are described in the Authentication section . You can then run ContainerSSH with the following command line: docker run -d \\ -v /srv/containerssh/config.yaml:/etc/containerssh/config.yaml \\ -v /srv/containerssh/host.key:/var/run/secrets/host.key \\ -p 2222 :2222 \\ containerssh/containerssh:0.3.1 Kubernetes When running ContainerSSH inside a Kubernetes cluster you must furst create a Secret that contains the host key. openssl genrsa | kubectl create secret generic containerssh-hostkey --from-file = host.key = /dev/stdin Next, you can create a ConfigMap to hold the ContainerSSH configuration: ( cat << EOF ssh: hostkeys: - /etc/containerssh/host.key auth: url: http://your-auth-server/ EOF ) | kubectl create configmap containerssh-config --from-file = config.yaml = /dev/stdin Tip Details about the authentication server are described in the Authentication section . Then you can create a deployment to run ContainerSSH: ( cat << EOF apiVersion: apps/v1 kind: Deployment metadata: name: containerssh labels: app: containerssh spec: replicas: 1 selector: matchLabels: app: containerssh template: metadata: labels: app: containerssh spec: containers: - name: containerssh image: containerssh/containerssh:0.3.1 ports: - containerPort: 2222 volumeMounts: - name: hostkey mountPath: /etc/containerssh/host.key subPath: host.key readOnly: true - name: config mountPath: /etc/containerssh/config.yaml subPath: config.yaml readOnly: true volumes: - name: hostkey secret: secretName: containerssh-hostkey - name: config configMap: name: containerssh-config EOF ) | kubectl apply -f - Finally, you can create a service to expose the SSH port. You can customize this to create a loadbalancer or nodeport to make SSH publicly available. See kubectl expose --help for details. kubectl expose deployment containerssh \\ --port = 2222 --target-port = 2222 \\ --name = containerssh Note This still does not configure ContainerSSH to use Kubernetes as a container backend. This is described in detail in the KubeRun backend section .","title":"Installation"},{"location":"reference/0.3/kuberun/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The KubeRun backend The KubeRun backend runs and is tested against all currently actively maintained Kubernetes versions . For ContainerSSH version 0.3 these are: 1.19, and 1.18. The base configuration structure \u00b6 In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kuberun kuberun : connection : <connection configuration here> pod : <pod configuration here> Configuring connection parameters \u00b6 In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kuberun : connection : host : <...> <...> Base configuration \u00b6 Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. insecure bool Disable certificate verification on the Kubernetes API. This is a very bad idea as anyone on the network will be able to intercept your credentials. qps float32` Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. timeout string Timeout for pod operations in nanoseconds. Time units can be used. HTTP basic authentication (username and password) \u00b6 Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) x509 certificate authentication \u00b6 Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format. Bearer token authentication \u00b6 This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API. Pod configuration \u00b6 The pod configuration contains the information which pod to run. kuberun : pod : namespace : <namespace name> podSpec : <pod spec here> <ContainerSSH-specific options here> Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod.spec . Basic pod configuration \u00b6 ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kuberun : pod : namespace : default podSpec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world! Running multiple containers \u00b6 When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kuberun: pod: namespace: default consoleContainerNumber: 1 podSpec: containers: - name: container1 image: ... - name: container2 image: ... Mounting volumes \u00b6 In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage. Forcing the pod to run on a specific node \u00b6 In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kuberun : pod : podSpec : nodeName : <insert node name here> Other options \u00b6 Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. enableAgent bool Enable the ContainerSSH guest agent. This enables the ContainerSSH guest agent. subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. Securing Kubernetes \u00b6 Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment Creating a service account \u00b6 When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh Deploying inside of Kubernetes \u00b6 When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kuberun: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token Deploying outside of Kubernetes \u00b6 Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : <base64-encoded namespace here> token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kuberun : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here> Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kuberun : pod : podSpec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kuberun : pod : podSpec : securityContext : runAsUser : 1000 Preventing storage exhaustion \u00b6 A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kuberun : pod : podSpec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation . Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kuberun : pod : podSpec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation . Limiting network access \u00b6 Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"KubeRun"},{"location":"reference/0.3/kuberun/#the-base-configuration-structure","text":"In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kuberun kuberun : connection : <connection configuration here> pod : <pod configuration here>","title":"The base configuration structure"},{"location":"reference/0.3/kuberun/#configuring-connection-parameters","text":"In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kuberun : connection : host : <...> <...>","title":"Configuring connection parameters"},{"location":"reference/0.3/kuberun/#base-configuration","text":"Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. insecure bool Disable certificate verification on the Kubernetes API. This is a very bad idea as anyone on the network will be able to intercept your credentials. qps float32` Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. timeout string Timeout for pod operations in nanoseconds. Time units can be used.","title":"Base configuration"},{"location":"reference/0.3/kuberun/#http-basic-authentication-username-and-password","text":"Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2)","title":"HTTP basic authentication (username and password)"},{"location":"reference/0.3/kuberun/#x509-certificate-authentication","text":"Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format.","title":"x509 certificate authentication"},{"location":"reference/0.3/kuberun/#bearer-token-authentication","text":"This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API.","title":"Bearer token authentication"},{"location":"reference/0.3/kuberun/#pod-configuration","text":"The pod configuration contains the information which pod to run. kuberun : pod : namespace : <namespace name> podSpec : <pod spec here> <ContainerSSH-specific options here> Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod.spec .","title":"Pod configuration"},{"location":"reference/0.3/kuberun/#basic-pod-configuration","text":"ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kuberun : pod : namespace : default podSpec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world!","title":"Basic pod configuration"},{"location":"reference/0.3/kuberun/#running-multiple-containers","text":"When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kuberun: pod: namespace: default consoleContainerNumber: 1 podSpec: containers: - name: container1 image: ... - name: container2 image: ...","title":"Running multiple containers"},{"location":"reference/0.3/kuberun/#mounting-volumes","text":"In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage.","title":"Mounting volumes"},{"location":"reference/0.3/kuberun/#forcing-the-pod-to-run-on-a-specific-node","text":"In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kuberun : pod : podSpec : nodeName : <insert node name here>","title":"Forcing the pod to run on a specific node"},{"location":"reference/0.3/kuberun/#other-options","text":"Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. enableAgent bool Enable the ContainerSSH guest agent. This enables the ContainerSSH guest agent. subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution.","title":"Other options"},{"location":"reference/0.3/kuberun/#securing-kubernetes","text":"Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment","title":"Securing Kubernetes"},{"location":"reference/0.3/kuberun/#creating-a-service-account","text":"When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh","title":"Creating a service account"},{"location":"reference/0.3/kuberun/#deploying-inside-of-kubernetes","text":"When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kuberun: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token","title":"Deploying inside of Kubernetes"},{"location":"reference/0.3/kuberun/#deploying-outside-of-kubernetes","text":"Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : <base64-encoded namespace here> token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kuberun : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here>","title":"Deploying outside of Kubernetes"},{"location":"reference/0.3/kuberun/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kuberun : pod : podSpec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kuberun : pod : podSpec : securityContext : runAsUser : 1000","title":"Preventing root escalation"},{"location":"reference/0.3/kuberun/#preventing-storage-exhaustion","text":"A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk.","title":"Preventing storage exhaustion"},{"location":"reference/0.3/kuberun/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kuberun : pod : podSpec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing memory exhaustion"},{"location":"reference/0.3/kuberun/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kuberun : pod : podSpec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing CPU exhaustion"},{"location":"reference/0.3/kuberun/#limiting-network-access","text":"Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Limiting network access"},{"location":"reference/0.3/logging/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Logging ContainerSSH comes with configurable logging facilities. At this time only JSON logging is supported, but the log level can be configured. The configuration can be done from the config file: log: level: \"warning\" Tip You can configure the log level on a per-user basis using the configuration server . The supported levels are in accordance with the Syslog standard: debug info notice warning error crit alert emerg Note In case of a fatal application crash (panic), the crash log will end up on the stderr. Make sure to capture that as well for emergency debugging. The JSON log format \u00b6 The JSON log format outputs one line to the output per message. The message format is: { \"timestamp\" : \"Timestamp in RFC3339 format\" , \"level\" : \"the log level\" , \"message\" : \"the message (optional)\" , \"details\" : { \"the detail object if any (optional)\" } } Note The JSON logger writes to the standard output regardless of log level. Note In case of a fatal application crash (panic), the crash log will end up on the stderr. Make sure to capture that as well for emergency debugging.","title":"Logging"},{"location":"reference/0.3/logging/#the-json-log-format","text":"The JSON log format outputs one line to the output per message. The message format is: { \"timestamp\" : \"Timestamp in RFC3339 format\" , \"level\" : \"the log level\" , \"message\" : \"the message (optional)\" , \"details\" : { \"the detail object if any (optional)\" } } Note The JSON logger writes to the standard output regardless of log level. Note In case of a fatal application crash (panic), the crash log will end up on the stderr. Make sure to capture that as well for emergency debugging.","title":"The JSON log format"},{"location":"reference/0.3/metrics/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Metrics ContainerSSH contains a Prometheus -compatible metrics server which can be enabled using the following configuration: metrics : enable : true # Defaults to false listen : \"0.0.0.0:9100\" # Set the listen address here path : \"/metrics\" # Defaults to /metrics You can configure Prometheus to grab the following metrics: containerssh_auth_server_failures Number of failed requests to the authentication server since start. containerssh_auth_success Number of successful authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_auth_failures Number of failed authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_config_server_failures Number of failed requests to the configuration server since start. containerssh_ssh_connections Number of SSH connections since start. Contains a label for country (see below). containerssh_ssh_handshake_successful Number of successful SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_handshake_failed Number of failed SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_current_connections Number of currently open SSH connections. Contains a label for country (see below). Country identification \u00b6 Country identification works using GeoIP2 or GeoLite2 from MaxMind . This database needs to be provided to ContainerSSH externally due to licensing concerns. The default path for the GeoIP database is /var/lib/GeoIP/GeoIP2-Country.mmdb , but you can change that using the following configuration snippet: geoip : maxmind-geoip2-file : '/var/lib/GeoIP/GeoIP2-Country.mmdb'","title":"Metrics"},{"location":"reference/0.3/metrics/#country-identification","text":"Country identification works using GeoIP2 or GeoLite2 from MaxMind . This database needs to be provided to ContainerSSH externally due to licensing concerns. The default path for the GeoIP database is /var/lib/GeoIP/GeoIP2-Country.mmdb , but you can change that using the following configuration snippet: geoip : maxmind-geoip2-file : '/var/lib/GeoIP/GeoIP2-Country.mmdb'","title":"Country identification"},{"location":"reference/0.3/api/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb ContainerSSH itself does not have an API apart from exposing the metrics . However, it requires two external APIs: the authentication server for user authentication and the config server for supplying a user-specific container configuration. We are providing an OpenAPI document for both. Access the OpenAPI docs \u00bb","title":"API"},{"location":"reference/0.4.0/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb ContainerSSH Reference Manual The Reference Manual provides reference material for ContainerSSH 0.4 and is oriented towards system operators wishing to use ContainerSSH on their system. Introduction \u00b6 This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation. If you need a one minute primer on how ContainerSSH works please watch this video . Changes since ContainerSSH 0.3 \u00b6 ContainerSSH 0.4 is major overhaul to the internal structure. As such, this release contains several major improvements to the stability of ContainerSSH. The reference manual for ContainerSSH 0.3 is available here . Audit logging \u00b6 The most visible improvement of this release is the new audit logging facility . Audit logging allows operators to capture everything that is happening within an SSH connection, including passwords, keys, typed commands, or SFTP uploads. The audit log can automatically be uploaded to an S3-compatible object storage. SSH proxying \u00b6 This release also adds a new SSH proxy backend that can be used to forward connections to a backing SSH server. Using this in conjunction with the audit logging facility makes it possible to use ContainerSSH as an auditing and dynamic forwarding SSH proxy. Improved logging \u00b6 This release also adds significant improvements to logging. This release adds several hundred log messages across all levels to make debugging potential failures and reporting errors much easier. Most of these log messages have been added with the average operator in mind and the details are sent in the debug log level, which is disabled by default. Furthermore, we have added a new log format as well as two new log outputs. You can now log in text and JSON formats to stdout, files, or syslog. Finally, we have added a unique message code to each log message that makes it easier to look up the corresponding documentation in the code list and determine if a log message is cause for concern or not. The details are described in the logging reference . Security filters \u00b6 The new security module adds a the ability to create a fine-grained filter what interactions over SSH are allowed and which ones should be blocked. The Kubernetes backend \u00b6 The new Kubernetes backend replaces the previous KubeRun backend . The new Kubernetes backend supports two modes of operation: running a pod per session (multiple per connection) or one pod per connection, using the exec facility. Support for the new ContainerSSH agent means that the Kubernetes backend now has support for all functions in SSH. The full list of changes is described in the KubeRun deprecation notice . The Docker backend \u00b6 The new Docker backend replaces the previous DockerRun backend . The new Docker backend supports two modes of operation: running a pod per session (multiple per connection) or one pod per connection, using the exec facility. Support for the new ContainerSSH agent means that the Docker backend now has support for all functions in SSH. The full list of changes is described in the DockerRun deprecation notice . Scoped SSH configuration \u00b6 In order to facilitate multiple daemons in a single binary the SSH listen configuration is now located under the ssh key. The change is described in the listen deprecation notice . Unified connectionId for authentication and configuration webhooks \u00b6 In the previous version ContainerSSH sent a sessionId field to the authentication and configuration servers. This is now replaced with the opaque connectionId , which is mirrored in log files. The change is described in the sessionId deprecation notice . SSH key format \u00b6 In the previous version ContainerSSH sent the SSH key in the OpenSSH binary format in the publicKeyBase64 field. This format was not easy to integrate and it is now replaced with the publicKey field containing the SSH key in the authorized key format. The change is described in the publicKeyBase64 deprecation notice . Webhooks now use POST \u00b6 In the previous version of ContainerSSH the webhooks were sent using the GET method. This is now changed to POST .","title":"Overview"},{"location":"reference/0.4.0/#introduction","text":"This manual contains documentation on how to set up, configure, monitor, and secure the ContainerSSH installation. If you need a one minute primer on how ContainerSSH works please watch this video .","title":"Introduction"},{"location":"reference/0.4.0/#changes-since-containerssh-03","text":"ContainerSSH 0.4 is major overhaul to the internal structure. As such, this release contains several major improvements to the stability of ContainerSSH. The reference manual for ContainerSSH 0.3 is available here .","title":"Changes since ContainerSSH 0.3"},{"location":"reference/0.4.0/#audit-logging","text":"The most visible improvement of this release is the new audit logging facility . Audit logging allows operators to capture everything that is happening within an SSH connection, including passwords, keys, typed commands, or SFTP uploads. The audit log can automatically be uploaded to an S3-compatible object storage.","title":"Audit logging"},{"location":"reference/0.4.0/#ssh-proxying","text":"This release also adds a new SSH proxy backend that can be used to forward connections to a backing SSH server. Using this in conjunction with the audit logging facility makes it possible to use ContainerSSH as an auditing and dynamic forwarding SSH proxy.","title":"SSH proxying"},{"location":"reference/0.4.0/#improved-logging","text":"This release also adds significant improvements to logging. This release adds several hundred log messages across all levels to make debugging potential failures and reporting errors much easier. Most of these log messages have been added with the average operator in mind and the details are sent in the debug log level, which is disabled by default. Furthermore, we have added a new log format as well as two new log outputs. You can now log in text and JSON formats to stdout, files, or syslog. Finally, we have added a unique message code to each log message that makes it easier to look up the corresponding documentation in the code list and determine if a log message is cause for concern or not. The details are described in the logging reference .","title":"Improved logging"},{"location":"reference/0.4.0/#security-filters","text":"The new security module adds a the ability to create a fine-grained filter what interactions over SSH are allowed and which ones should be blocked.","title":"Security filters"},{"location":"reference/0.4.0/#the-kubernetes-backend","text":"The new Kubernetes backend replaces the previous KubeRun backend . The new Kubernetes backend supports two modes of operation: running a pod per session (multiple per connection) or one pod per connection, using the exec facility. Support for the new ContainerSSH agent means that the Kubernetes backend now has support for all functions in SSH. The full list of changes is described in the KubeRun deprecation notice .","title":"The Kubernetes backend"},{"location":"reference/0.4.0/#the-docker-backend","text":"The new Docker backend replaces the previous DockerRun backend . The new Docker backend supports two modes of operation: running a pod per session (multiple per connection) or one pod per connection, using the exec facility. Support for the new ContainerSSH agent means that the Docker backend now has support for all functions in SSH. The full list of changes is described in the DockerRun deprecation notice .","title":"The Docker backend"},{"location":"reference/0.4.0/#scoped-ssh-configuration","text":"In order to facilitate multiple daemons in a single binary the SSH listen configuration is now located under the ssh key. The change is described in the listen deprecation notice .","title":"Scoped SSH configuration"},{"location":"reference/0.4.0/#unified-connectionid-for-authentication-and-configuration-webhooks","text":"In the previous version ContainerSSH sent a sessionId field to the authentication and configuration servers. This is now replaced with the opaque connectionId , which is mirrored in log files. The change is described in the sessionId deprecation notice .","title":"Unified connectionId for authentication and configuration webhooks"},{"location":"reference/0.4.0/#ssh-key-format","text":"In the previous version ContainerSSH sent the SSH key in the OpenSSH binary format in the publicKeyBase64 field. This format was not easy to integrate and it is now replaced with the publicKey field containing the SSH key in the authorized key format. The change is described in the publicKeyBase64 deprecation notice .","title":"SSH key format"},{"location":"reference/0.4.0/#webhooks-now-use-post","text":"In the previous version of ContainerSSH the webhooks were sent using the GET method. This is now changed to POST .","title":"Webhooks now use POST"},{"location":"reference/0.4.0/audit/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Audit logging ContainerSSH contains an audit logging facility that can log every interaction happening over SSH. This functionality is disabled by default as it has serious security and privacy implications, as well as severe resource requirements. Audit logging can be enabled in the configuration using the following structure: audit : enable : true format : none|binary|asciinema # Which format to log in. Defaults to none. storage : none|s3|file # Where to write audit log. Defaults to none. intercept : stdin : true|false # Intercept keystrokes from user stdout : true|false # Intercept standard output stderr : true|false # Intercept standard error passwords : true|false # Intercept passwords during authentication Audit logging is a powerful tool. It can capture the following events. Connections Authentication attempts, optionally with credentials Global and channel-specific SSH requests Programs launched from SSH Input from the user (optional) Output and errors to the user (optional) The events recorded depend on the chosen format. With the audit format all information is recorded with nanosecond timing, so events can be accurately reconstructed after the fact. About interceptions \u00b6 The intercept options give you a wide range of options when it comes to detailed logging of actions by users. You may want to, for example, enable stdout logging while keeping stdin disabled to avoid accidentally capturing passwords typed into the console. However, this approach may fail if SFTP is enabled as you will fail to capture binaries uploaded to the server. Audit logging should therefore be enjoyed with great care and the logs should always be stored on an encrypted storage device. Log formats \u00b6 The binary format (recommended) \u00b6 The binary format is intended for an accurate reconstruction of everything happening during an SSH session. It allows for accurate reconstruction of what happened during the session. Audit logs are stored in a compressed binary format and can be decoded to a series of JSON messages using the containerssh-auditlog-decoder supplied as part of the ContainerSSH release. Alternatively, you can implement your own decoder . We are providing a Go library for decoding audit log messages . This format can be decoded using the containerssh-auditlog-decoder application supplied with ContainerSSH. The asciinema format \u00b6 The asciinema format stores logs in a format suitable for replay in the Asciinema player . Note Make sure you enable the stdout and stderr interceptions otherwise the asciinema encoder won't capture anything. Warning Asciinema is intended for entertainment purposes only and doesn't store all relevant information required for an accurate audit log. Storage backends \u00b6 The s3 storage (recommended) \u00b6 The S3 storage sends the logs to an S3-compatible object storage for long term storage. This is the recommended way of storing audit logs because it is a server-independent storage device that supports permissions. The S3 storage stores the logs in a local directory and uploads them once an upload part is full (default: 5MB) or the connection closes. If the upload fails, ContainerSSH will retry the upload as soon as possible. If ContainerSSH is stopped and restarted it will attempt to upload the audit logs still in the local directory, but no guarantee is made that these logs will not be corrupt after a crash. Warning The local directory should be stored on a persistent storage and must not be shared between ContainerSSH instances. It must be large enough to host all sessions in their entirety that are currently running. When IO interception is enabled and your users are downloading or uploading large amounts of data this can run you up to several GB of storage needed locally. We recommend turning off IO interception for cases where large amounts of data are being transferred. The S3 storage can be configured as follows: audit : ... storage : s3 s3 : local : /local/storage/directory accessKey : \"your-access-key-here\" secretKey : \"your-secret-key-here\" bucket : \"your-existing-bucket-name-here\" region : \"your-region-name-here\" endpoint : \"https://your-custom-s3-url\" # Optional uploadPartSize : 5242880 # In bytes, min: 5MB, max: 5GB acl : \"public-read\" # Optional, in case you want to set an ACL metadata : username : true # Expose username via S3 metadata. Defaults to false. ip : true # Expose IP address via S3 metadata. Defaults to false. cacert : | # Optional Your trusted CA certificate in PEM format here for your S3 server. Tip You can restrict the access key permissions to PutObject , CreateMultipartUpload , UploadPart , CompleteMultipartUpload , ListMultipartUploads , and AbortMultipartUpload . Other permissions are not required. Tip You may also want to investigate if your S3 provider supports WORM / object locking, object lifecycles, or server side encryption for compliance. The file storage \u00b6 The file storage writes audit logs to files on the disk. The storage location can be configured using the following option: audit : type : file file : directory : /var/log/audit","title":"Audit logging"},{"location":"reference/0.4.0/audit/#about-interceptions","text":"The intercept options give you a wide range of options when it comes to detailed logging of actions by users. You may want to, for example, enable stdout logging while keeping stdin disabled to avoid accidentally capturing passwords typed into the console. However, this approach may fail if SFTP is enabled as you will fail to capture binaries uploaded to the server. Audit logging should therefore be enjoyed with great care and the logs should always be stored on an encrypted storage device.","title":"About interceptions"},{"location":"reference/0.4.0/audit/#log-formats","text":"","title":"Log formats"},{"location":"reference/0.4.0/audit/#the-binary-format-recommended","text":"The binary format is intended for an accurate reconstruction of everything happening during an SSH session. It allows for accurate reconstruction of what happened during the session. Audit logs are stored in a compressed binary format and can be decoded to a series of JSON messages using the containerssh-auditlog-decoder supplied as part of the ContainerSSH release. Alternatively, you can implement your own decoder . We are providing a Go library for decoding audit log messages . This format can be decoded using the containerssh-auditlog-decoder application supplied with ContainerSSH.","title":"The binary format (recommended)"},{"location":"reference/0.4.0/audit/#the-asciinema-format","text":"The asciinema format stores logs in a format suitable for replay in the Asciinema player . Note Make sure you enable the stdout and stderr interceptions otherwise the asciinema encoder won't capture anything. Warning Asciinema is intended for entertainment purposes only and doesn't store all relevant information required for an accurate audit log.","title":"The asciinema format"},{"location":"reference/0.4.0/audit/#storage-backends","text":"","title":"Storage backends"},{"location":"reference/0.4.0/audit/#the-s3-storage-recommended","text":"The S3 storage sends the logs to an S3-compatible object storage for long term storage. This is the recommended way of storing audit logs because it is a server-independent storage device that supports permissions. The S3 storage stores the logs in a local directory and uploads them once an upload part is full (default: 5MB) or the connection closes. If the upload fails, ContainerSSH will retry the upload as soon as possible. If ContainerSSH is stopped and restarted it will attempt to upload the audit logs still in the local directory, but no guarantee is made that these logs will not be corrupt after a crash. Warning The local directory should be stored on a persistent storage and must not be shared between ContainerSSH instances. It must be large enough to host all sessions in their entirety that are currently running. When IO interception is enabled and your users are downloading or uploading large amounts of data this can run you up to several GB of storage needed locally. We recommend turning off IO interception for cases where large amounts of data are being transferred. The S3 storage can be configured as follows: audit : ... storage : s3 s3 : local : /local/storage/directory accessKey : \"your-access-key-here\" secretKey : \"your-secret-key-here\" bucket : \"your-existing-bucket-name-here\" region : \"your-region-name-here\" endpoint : \"https://your-custom-s3-url\" # Optional uploadPartSize : 5242880 # In bytes, min: 5MB, max: 5GB acl : \"public-read\" # Optional, in case you want to set an ACL metadata : username : true # Expose username via S3 metadata. Defaults to false. ip : true # Expose IP address via S3 metadata. Defaults to false. cacert : | # Optional Your trusted CA certificate in PEM format here for your S3 server. Tip You can restrict the access key permissions to PutObject , CreateMultipartUpload , UploadPart , CompleteMultipartUpload , ListMultipartUploads , and AbortMultipartUpload . Other permissions are not required. Tip You may also want to investigate if your S3 provider supports WORM / object locking, object lifecycles, or server side encryption for compliance.","title":"The s3 storage (recommended)"},{"location":"reference/0.4.0/audit/#the-file-storage","text":"The file storage writes audit logs to files on the disk. The storage location can be configured using the following option: audit : type : file file : directory : /var/log/audit","title":"The file storage"},{"location":"reference/0.4.0/auth/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Authentication ContainerSSH does not know your users and their passwords. Therefore, it calls out to a microservice that you have to provide. Your service can verify the users, passwords, and SSH keys. You will have to provide the microservice URL in the configuration. Configuration \u00b6 The authentication webhook can be configured in the main configuration using the following structure: auth : <options> The following options are supported: Name Type Description password bool Enable password authentication. pubkey bool Enable public key authentication. url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. authTimeout string Timeout for the authentication process. HTTP calls that result in a non-200 response call will be retried until this timeout is reached. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion []string Minimum TLS version to support. See the TLS version section below. curve []string Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. cipher []string,string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false. Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Server certificates must use Subject Alternative Names (SAN's) for proper server verification. Elliptic curve algorithms \u00b6 The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1 Cipher suites \u00b6 The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Tip Cipher suites can be provided as a list or as a colon ( : ) separated string. Client authentication \u00b6 In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem The authentication webhook \u00b6 The authentication webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create an authentication server. Warning A warning about rate limiting: if the authentication server desires to do rate limiting for connecting users it should take into account that a user is allowed to try multiple authentication attempts (currently hard-coded to 6 per connection) before they are disconnected. Some of the authentication attempts (e.g. public keys) happen automatically on the client side without the user having any influence on them. Furthermore, ContainerSSH retries failed HTTP calls. To be effective the authentication server should count the unique connection identifiers seen in the connectionId field and implement a lock-out based on these. Password authentication \u00b6 On password authentication the authentication server will receive the following request to the /password endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" } Public key authentication \u00b6 On public key authentication the authentication server will receive the following request to the /pubkey endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"publicKey\" : \"ssh-rsa ...\" } The public key will be sent in the authorized key format. Response \u00b6 Both endpoints need to respond with an application/json response of the following content: { \"success\" : true } Tip We provide a Go library to implement a authentication server .","title":"Authentication"},{"location":"reference/0.4.0/auth/#configuration","text":"The authentication webhook can be configured in the main configuration using the following structure: auth : <options> The following options are supported: Name Type Description password bool Enable password authentication. pubkey bool Enable public key authentication. url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. authTimeout string Timeout for the authentication process. HTTP calls that result in a non-200 response call will be retried until this timeout is reached. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion []string Minimum TLS version to support. See the TLS version section below. curve []string Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. cipher []string,string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false.","title":"Configuration"},{"location":"reference/0.4.0/auth/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/0.4.0/auth/#tls-version","text":"The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Server certificates must use Subject Alternative Names (SAN's) for proper server verification.","title":"TLS version"},{"location":"reference/0.4.0/auth/#elliptic-curve-algorithms","text":"The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1","title":"Elliptic curve algorithms"},{"location":"reference/0.4.0/auth/#cipher-suites","text":"The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Tip Cipher suites can be provided as a list or as a colon ( : ) separated string.","title":"Cipher suites"},{"location":"reference/0.4.0/auth/#client-authentication","text":"In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem","title":"Client authentication"},{"location":"reference/0.4.0/auth/#the-authentication-webhook","text":"The authentication webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create an authentication server. Warning A warning about rate limiting: if the authentication server desires to do rate limiting for connecting users it should take into account that a user is allowed to try multiple authentication attempts (currently hard-coded to 6 per connection) before they are disconnected. Some of the authentication attempts (e.g. public keys) happen automatically on the client side without the user having any influence on them. Furthermore, ContainerSSH retries failed HTTP calls. To be effective the authentication server should count the unique connection identifiers seen in the connectionId field and implement a lock-out based on these.","title":"The authentication webhook"},{"location":"reference/0.4.0/auth/#password-authentication","text":"On password authentication the authentication server will receive the following request to the /password endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"passwordBase64\" : \"Base 64-encoded password\" }","title":"Password authentication"},{"location":"reference/0.4.0/auth/#public-key-authentication","text":"On public key authentication the authentication server will receive the following request to the /pubkey endpoint: { \"username\" : \"username\" , \"remoteAddress\" : \"127.0.0.1:1234\" , \"connectionId\" : \"An opaque ID for the SSH connection\" , \"publicKey\" : \"ssh-rsa ...\" } The public key will be sent in the authorized key format.","title":"Public key authentication"},{"location":"reference/0.4.0/auth/#response","text":"Both endpoints need to respond with an application/json response of the following content: { \"success\" : true } Tip We provide a Go library to implement a authentication server .","title":"Response"},{"location":"reference/0.4.0/backends/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb ContainerSSH is built to support multiple backends. The backend can be changed in the configuration file: backend : <backend type> ContainerSSH currently supports the following backends: Backend Description docker Runs Docker containers. kubernetes Runs Kubernetes containers. sshproxy Forwards SSH connections to a backend server. dockerrun Deprecated backend that runs Docker containers. kuberrun Deprecated backend that runs Kubernetes pods.","title":"Backend selection"},{"location":"reference/0.4.0/codes/","text":"Message codes \u00b6 Old manual You are reading the reference manual of an older release. Read the current manual \u00bb This page contains all message codes logged by ContainerSSH. Some of these are errors, while others only give you status information. Many of these messages are only logged when the log level is set to debug . Core \u00b6 Code Explanation CORE_CONFIG_CANNOT_WRITE_FILE ContainerSSH cannot update the configuration file with the new host keys and will only use the host key for the current run. CORE_CONFIG_ERROR ContainerSSH encountered an error in the configuration. CORE_CONFIG_FILE ContainerSSH is reading the configuration file CORE_HOST_KEY_GENERATION_FAILED ContainerSSH could not generate host keys and is aborting the run. CORE_NO_HOST_KEYS The configuration does not contain host keys. ContainerSSH will attempt to generate host keys and update the configuration file. Auditlog \u00b6 Code Explanation AUDIT_S3_CANNOT_CLOSE_METADATA_FILE_HANDLE ContainerSSH could not close the metadata file in the local folder. This typically happens when the local folder is on an NFS share. (This is NOT supported.) AUDIT_S3_CLOSE_FAILED ContainerSSH failed to close an audit log file in the local directory. This usually happens when the local directory is on an NFS share. (This is NOT supported.) AUDIT_S3_FAILED_CREATING_METADATA_FILE ContainerSSH failed to create the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is writable and has enough disk space. AUDIT_S3_FAILED_METADATA_JSON_ENCODING ContainerSSH failed to encode the metadata file. This is a bug, please report it. AUDIT_S3_FAILED_READING_METADATA_FILE ContainerSSH failed to read the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is readable and the files have not been corrupted. AUDIT_S3_FAILED_STAT_QUEUE_ENTRY ContainerSSH failed to stat the queue file. This usually happens when the local directory is being manually manipulated. AUDIT_S3_FAILED_WRITING_METADATA_FILE ContainerSSH failed to write the local metadata file. Please check if your disk has enough disk space. AUDIT_S3_MULTIPART_ABORTING ContainerSSH is aborting a multipart upload. Check the log message for details. AUDIT_S3_MULTIPART_FAILED_ABORT ContainerSSH failed aborting a multipart upload from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_FAILED_LIST ContainerSSH failed to list multipart uploads on the object storage bucket. This is needed to abort uploads from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_PART_UPLOADING ContainerSSH is uploading a part of an audit log to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_COMPLETE ContainerSSH completed the upload of an audit log part to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_FAILED ContainerSSH failed to upload a part to the S3-compatible object storage. Check the message for details. AUDIT_S3_MULTIPART_UPLOAD ContainerSSH is starting a new S3 multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZATION_FAILED ContainerSSH has uploaded all audit log parts, but could not finalize the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZED ContainerSSH has uploaded all audit log parts and has successfully finalized the upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZING ContainerSSH has uploaded all audit log parts and is now finalizing the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_INITIALIZATION_FAILED ContainerSSH failed to initialize a new multipart upload to the S3-compatible object storage. Check if the S3 configuration is correct and the provided S3 access key and secrets have permissions to start a multipart upload. AUDIT_S3_NO_SUCH_QUEUE_ENTRY ContainerSSH was trying to upload an audit log from the metadata file, but the audit log does not exist. AUDIT_S3_RECOVERING ContainerSSH found a previously aborted multipart upload locally and is now attempting to recover the upload. AUDIT_S3_REMOVE_FAILED ContainerSSH failed to remove an uploaded audit log from the local directory. This usually happens on Windows when a different process has the audit log open. (This is not a supported setup.) AUDIT_S3_SINGLE_UPLOAD ContainerSSH is uploading the full audit log in a single upload to the S3-compatible object storage. This happens when the audit log size is below the minimum size for a multi-part upload. AUDIT_S3_SINGLE_UPLOAD_COMPLETE ContainerSSH successfully uploaded the audit log as a single upload. AUDIT_S3_SINGLE_UPLOAD_FAILED ContainerSSH failed to upload the audit log as a single upload. Authentication \u00b6 Code Explanation AUTH ContainerSSH is trying to contact the authentication backend to verify the user credentials. AUTH_BACKEND_ERROR The ContainerSSH authentication server responded with a non-200 status code. ContainerSSH will retry the authentication for a few times before giving up. This is most likely a bug in your authentication server, please check your logs. AUTH_FAILED The user has provided invalid credentials and the authentication is rejected. AUTH_INVALID_STATUS This message indicates that the authentication server returned an invalid HTTP status code. AUTH_NOT_SUPPORTED The authentication method the client requested is not supported by ContainerSSH. AUTH_SUCCESSFUL The user has provided the correct credentials and the authentication is accepted. Backend \u00b6 Code Explanation BACKEND_CONFIG_ERROR The backend retreived from the configuration server is invalid. See the error message for details. Configuration \u00b6 Code Explanation CONFIG_BACKEND_ERROR ContainerSSH has received an invalid response from the configuration server or the network connection broke. ContainerSSH will retry fetching the user-specific configuration until the timeout. If this error persists check the connectivity to the configuration server, or the logs of the configuration server itself to find out of there may be a specific error. CONFIG_INVALID_STATUS_CODE ContainerSSH has received a non-200 response code when calling a per-user backend configuration from the configuration server. CONFIG_REQUEST ContainerSSH is sending a quest to the configuration server to obtain a per-user backend configuration. CONFIG_RESPONSE ContainerSSH has received a per-user backend configuration from the configuration server. CONFIG_SERVER_AVAILABLE The ContainerSSH configuration server is now available at the specified address. Docker \u00b6 Code Explanation DOCKER_AGENT_READ_FAILED The ContainerSSH Docker module failed to read from the ContainerSSH agent. This is most likely because the ContainerSSH guest agent is not present in the guest image, but agent support is enabled. DOCKER_CLOSE_INPUT_FAILED The ContainerSSH Docker module attempted to close the input (stdin) for reading but failed to do so. DOCKER_CLOSE_OUTPUT_FAILED The ContainerSSH Docker module attempted to close the output (stdout and stderr) for writing but failed to do so. DOCKER_CONFIG_ERROR The ContainerSSH Docker module detected a configuration error. Please check your configuration. DOCKER_CONTAINER_ATTACH The ContainerSSH Docker module is attaching to a container in session mode. DOCKER_CONTAINER_ATTACH_FAILED The ContainerSSH Docker module has failed to attach to a container in session mode. DOCKER_CONTAINER_CREATE The ContainerSSH Docker module is creating a container. DOCKER_CONTAINER_CREATE_FAILED The ContainerSSH Docker module failed to create a container. This may be a temporary and retried or a permanent error message. Check the log message for details. DOCKER_CONTAINER_REMOVE The ContainerSSH Docker module os removing the container. DOCKER_CONTAINER_REMOVE_FAILED The ContainerSSH Docker module could not remove the container. This message may be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_REMOVE_SUCCESSFUL The ContainerSSH Docker module has successfully removed the container. DOCKER_CONTAINER_SHUTTING_DOWN The ContainerSSH Docker module is shutting down a container. DOCKER_CONTAINER_SIGNAL The ContainerSSH Docker module is sending a signal to the container. DOCKER_CONTAINER_SIGNAL_FAILED The ContainerSSH Docker module has failed to send a signal to the container. DOCKER_CONTAINER_START The ContainerSSH Docker module is starting the previously-created container. DOCKER_CONTAINER_START_FAILED The ContainerSSH docker module failed to start the container. This message can either be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_STOP The ContainerSSH Docker module is stopping the container. DOCKER_CONTAINER_STOP_FAILED The ContainerSSH Docker module failed to stop the container. This message can be either temporary and retried or permanent. Check the log message for details. DOCKER_EXEC The ContainerSSH Docker module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the container. DOCKER_EXEC_ATTACH The ContainerSSH Docker module is attaching to the previously-created execution. DOCKER_EXEC_ATTACH_FAILED The ContainerSSH Docker module could not attach to the previously-created execution. DOCKER_EXEC_CREATE The ContainerSSH Docker module is creating an execution. DOCKER_EXEC_CREATE_FAILED The ContainerSSH Docker module has failed to create an execution. This can be temporary and retried or permanent. See the error message for details. DOCKER_EXEC_PID_READ_FAILED The ContainerSSH Docker module has failed to read the process ID from the ContainerSSH Guest Agent . This is most likely because the guest image does not contain the guest agent, but guest agent support has been enabled. DOCKER_EXEC_RESIZE The ContainerSSH Docker module is resizing the console. DOCKER_EXEC_RESIZE_FAILED The ContainerSSH Docker module failed to resize the console. DOCKER_EXEC_SIGNAL The ContainerSSH Docker module is delivering a signal in container mode. DOCKER_EXEC_SIGNAL_FAILED The ContainerSSH Docker module failed to deliver a signal. DOCKER_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Docker module failed to deliver a signal because ContainerSSH Guest Agent support is disabled. DOCKER_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Docker module successfully delivered the requested signal. DOCKER_EXIT_CODE The ContainerSSH Docker module is fetching the exit code from the program. DOCKER_EXIT_CODE_CONTAINER_RESTARTING The ContainerSSH Docker module could not fetch the exit code from the program because the container is restarting. This is typically a misconfiguration as ContainerSSH containers should not automatically restart. DOCKER_EXIT_CODE_FAILED The ContainerSSH Docker module has failed to fetch the exit code of the program. DOCKER_EXIT_CODE_NEGATIVE The ContainerSSH Docker module has received a negative exit code from Docker. This should never happen and is most likely a bug. DOCKER_EXIT_CODE_STILL_RUNNING The ContainerSSH Docker module could not fetch the program exit code because the program is still running. This error may be temporary and retried or permanent. DOCKER_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the container image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. DOCKER_IMAGE_LISTING The ContainerSSH Docker module is listing the locally present container images to determine if the specified container image needs to be pulled. DOCKER_IMAGE_LISTING_FAILED The ContainerSSH Docker module failed to list the images present in the local Docker daemon. This is used to determine if the image needs to be pulled. This can be because the Docker daemon is not reachable, the certificate is invalid, or there is something else interfering with listing the images. DOCKER_IMAGE_PULL The ContainerSSH Docker module is pulling the container image. DOCKER_IMAGE_PULL_FAILED The ContainerSSH Docker module failed to pull the specified container image. This can be because of connection issues to the Docker daemon, or because the Docker daemon itself can't pull the image. If you don't intend to have the image pulled you should set the ImagePullPolicy to Never . See the Docker documentation for details. DOCKER_IMAGE_PULL_NEEDED_CHECKING The ContainerSSH Docker module is checking if an image pull is needed. DOCKER_PROGRAM_ALREADY_RUNNING The ContainerSSH Docker module can't execute the request because the program is already running. This is a client error. DOCKER_SIGNAL_FAILED_NO_PID The ContainerSSH Docker module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. DOCKER_STREAM_INPUT_FAILED The ContainerSSH Docker module failed to stream stdin to the Docker engine. DOCKER_STREAM_OUTPUT_FAILED The ContainerSSH Docker module failed to stream stdout and stderr from the Docker engine. DOCKER_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Docker module is not configured to run the requested subsystem. HTTP \u00b6 Code Explanation HTTP_CLIENT_CONNECTION_FAILED This message indicates a connection failure on the network level. HTTP_CLIENT_DECODE_FAILED This message indicates that decoding the JSON response has failed. The status code is set for this code. HTTP_CLIENT_ENCODE_FAILED This message indicates that JSON encoding the request failed. This is usually a bug. HTTP_CLIENT_REDIRECT This message indicates that the server responded with a HTTP redirect. HTTP_CLIENT_REDIRECTS_DISABLED This message indicates that ContainerSSH is not following a HTTP redirect sent by the server. Use the allowRedirects option to allow following HTTP redirects. HTTP_CLIENT_REQUEST This message indicates that a HTTP request is being sent from ContainerSSH HTTP_CLIENT_RESPONSE This message indicates that ContainerSSH received a HTTP response from a server. HTTP_SERVER_ENCODE_FAILED The HTTP server failed to encode the response object. This is a bug, please report it. HTTP_SERVER_RESPONSE_WRITE_FAILED The HTTP server failed to write the response. Kubernetes \u00b6 Code Explanation KUBERNETES_CLOSE_OUTPUT_FAILED The ContainerSSH Kubernetes module attempted to close the output (stdout and stderr) for writing but failed to do so. KUBERNETES_CONFIG_ERROR The ContainerSSH Kubernetes module detected a configuration error. Please check your configuration. KUBERNETES_EXEC The ContainerSSH Kubernetes module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the pod. KUBERNETES_EXEC_RESIZE The ContainerSSH Kubernetes module is resizing the terminal window. KUBERNETES_EXEC_RESIZE_FAILED The ContainerSSH Kubernetes module failed to resize the console. KUBERNETES_EXEC_SIGNAL The ContainerSSH Kubernetes module is delivering a signal. KUBERNETES_EXEC_SIGNAL_FAILED The ContainerSSH Kubernetes module failed to deliver a signal. KUBERNETES_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Kubernetes module failed to deliver a signal because guest agent support is disabled. KUBERNETES_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Kubernetes module successfully delivered the requested signal. KUBERNETES_EXIT_CODE_FAILED The ContainerSSH Kubernetes module has failed to fetch the exit code of the program. KUBERNETES_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the pod image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. KUBERNETES_PID_RECEIVED The ContainerSSH Kubernetes module has received a PID from the Kubernetes guest agent. KUBERNETES_POD_ATTACH The ContainerSSH Kubernetes module is attaching to a pod in session mode. KUBERNETES_POD_CREATE The ContainerSSH Kubernetes module is creating a pod. KUBERNETES_POD_CREATE_FAILED The ContainerSSH Kubernetes module failed to create a pod. This may be a temporary and retried or a permanent error message. Check the log message for details. KUBERNETES_POD_REMOVE The ContainerSSH Kubernetes module is removing a pod. KUBERNETES_POD_REMOVE_FAILED The ContainerSSH Kubernetes module could not remove the pod. This message may be temporary and retried or permanent. Check the log message for details. KUBERNETES_POD_REMOVE_SUCCESSFUL The ContainerSSH Kubernetes module has successfully removed the pod. KUBERNETES_POD_SHUTTING_DOWN The ContainerSSH Kubernetes module is shutting down a pod. KUBERNETES_POD_WAIT The ContainerSSH Kubernetes module is waiting for the pod to come up. KUBERNETES_POD_WAIT_FAILED The ContainerSSH Kubernetes module failed to wait for the pod to come up. Check the error message for details. KUBERNETES_PROGRAM_ALREADY_RUNNING The ContainerSSH Kubernetes module can't execute the request because the program is already running. This is a client error. KUBERNETES_PROGRAM_NOT_RUNNING This message indicates that the user requested an action that can only be performed when a program is running, but there is currently no program running. KUBERNETES_SIGNAL_FAILED_EXITED The ContainerSSH Kubernetes module can't deliver a signal because the program already exited. KUBERNETES_SIGNAL_FAILED_NO_PID The ContainerSSH Kubernetes module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. KUBERNETES_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Kubernetes module is not configured to run the requested subsystem. KUBERUN_DEPRECATED This message indicates that you are still using the deprecated KubeRun backend. This backend doesn't support all safety and functionality improvements and will be removed in the future. Please read the deprecation notice for a migration guide KUBERUN_EXEC_DISABLED This message indicates that the user tried to execute a program, but program execution is disabled in the legacy KubeRun configuration. KUBERUN_INSECURE This message indicates that you are using Kubernetes in the \"insecure\" mode where certificate verification is disabled. This is a major security flaw, has been deprecated and is removed in the new Kubernetes backend. Please change your configuration to properly validates the server certificates. Log \u00b6 Code Explanation LOG_FILE_OPEN_FAILED ContainerSSH failed to open the specified log file. LOG_ROTATE_FAILED ContainerSSH cannot rotate the logs as requested because of an underlying error. LOG_WRITE_FAILED ContainerSSH cannot write to the specified log file. This usually happens because the underlying filesystem is full or the log is located on a non-local storage (e.g. NFS), which is not supported. TEST This is message that should only be seen in unit and component tests, never in production. UNKNOWN_ERROR This is an untyped error. If you see this in a log that is a bug and should be reported. Metrics \u00b6 Code Explanation METRICS_AVAILABLE The metrics service is now online and ready for service. Security \u00b6 Code Explanation SECURITY_ENV_REJECTED ContainerSSH rejected setting the environment variable because it does not pass the security settings. SECURITY_EXEC_FAILED_SETENV Program execution failed in conjunction with the forceCommand option because ContainerSSH could not set the SSH_ORIGINAL_COMMAND environment variable on the backend. SECURITY_EXEC_FORCING_COMMAND ContainerSSH is replacing the command passed from the client (if any) to the specified command and is setting the SSH_ORIGINAL_COMMAND environment variable. SECURITY_EXEC_REJECTED A program execution request has been rejected because it doesn't conform to the security settings. SECURITY_SHELL_REJECTED ContainerSSH rejected launching a shell due to the security settings. SECURITY_SIGNAL_REJECTED ContainerSSH rejected delivering a signal because it does not pass the security settings. SECURITY_SUBSYSTEM_REJECTED ContainerSSH rejected the subsystem because it does pass the security settings. SECURITY_TTY_REJECTED ContainerSSH rejected the pseudoterminal request because of the security settings. Service \u00b6 Code Explanation SERVICE_CRASHED A ContainerSSH has stopped improperly. SERVICE_POOL_RUNNING All ContainerSSH services are now running. SERVICE_POOL_STARTING All ContainerSSH services are starting. SERVICE_POOL_STOPPED ContainerSSH has stopped all services. SERVICE_POOL_STOPPING ContainerSSH is stopping all services. SERVICE_RUNNING A ContainerSSH service is now running SERVICE_STARTING ContainerSSH is starting a component service SERVICE_STOPPED A ContainerSSH service has stopped. SERVICE_STOPPING A ContainerSSH service is now stopping. SSH \u00b6 Code Explanation SSH_ALREADY_RUNNING The SSH server is already running and has been started again. This is a bug, please report it. SSH_AUTH_FAILED The user has provided invalid credentials. SSH_AUTH_SUCCESSFUL The user has provided valid credentials and is now authenticated. SSH_AUTH_UNAVAILABLE The user has requested an authentication method that is currently unavailable. SSH_AVAILABLE The SSH service is now online and ready for service. SSH_BACKEND_REJECTED_HANDSHAKE The backend has rejected the connecting user after successful authentication. SSH_CHANNEL_REQUEST The user has send a new channel-specific request. SSH_CHANNEL_REQUEST_FAILED ContainerSSH couldn't fulfil the channel-specific request. SSH_CHANNEL_REQUEST_SUCCESSFUL ContainerSSH has successfully processed the channel-specific request. SSH_CONNECTED A user has connected over SSH. SSH_DECODE_FAILED ContainerSSH failed to decode something from the user. This is either a bug in ContainerSSH or in the connecting client. SSH_DISCONNECTED An SSH connection has been severed. SSH_EXIT ContainerSSH is sending the exit code of the program to the user. SSH_EXIT_CODE_FAILED ContainerSSH failed to obtain and send the exit code of the program to the user. SSH_EXIT_SIGNAL ContainerSSH is sending the exit signal from an abnormally exited program to the user. SSH_HANDSHAKE_FAILED The connecting party failed to establish a secure SSH connection. This is most likely due to invalid credentials or a backend error. SSH_HANDSHAKE_SUCCESSFUL The user has provided valid credentials and has now established an SSH connection. SSH_LISTEN_CLOSE_FAILED ContainerSSH failed to close the listen socket. SSH_NEW_CHANNEL A user has established a new SSH channel. (Not connection!) SSH_NEW_CHANNEL_REJECTED The user has requested a new channel to be opened, but was rejected. SSH_REPLY_SEND_FAILED ContainerSSH couldn't send the reply to a request to the user. This is usually the case if the user suddenly disconnects. SSH_START_FAILED ContainerSSH failed to start the SSH service. This is usually because of invalid configuration. SSH_UNSUPPORTED_CHANNEL_TYPE The user requested a channel type that ContainerSSH doesn't support (e.g. TCP/IP forwarding). SSH_UNSUPPORTED_GLOBAL_REQUEST The users client has send a global request ContainerSSH does not support. This is nothing to worry about.","title":"Message codes"},{"location":"reference/0.4.0/codes/#message-codes","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb This page contains all message codes logged by ContainerSSH. Some of these are errors, while others only give you status information. Many of these messages are only logged when the log level is set to debug .","title":"Message codes"},{"location":"reference/0.4.0/codes/#core","text":"Code Explanation CORE_CONFIG_CANNOT_WRITE_FILE ContainerSSH cannot update the configuration file with the new host keys and will only use the host key for the current run. CORE_CONFIG_ERROR ContainerSSH encountered an error in the configuration. CORE_CONFIG_FILE ContainerSSH is reading the configuration file CORE_HOST_KEY_GENERATION_FAILED ContainerSSH could not generate host keys and is aborting the run. CORE_NO_HOST_KEYS The configuration does not contain host keys. ContainerSSH will attempt to generate host keys and update the configuration file.","title":"Core"},{"location":"reference/0.4.0/codes/#auditlog","text":"Code Explanation AUDIT_S3_CANNOT_CLOSE_METADATA_FILE_HANDLE ContainerSSH could not close the metadata file in the local folder. This typically happens when the local folder is on an NFS share. (This is NOT supported.) AUDIT_S3_CLOSE_FAILED ContainerSSH failed to close an audit log file in the local directory. This usually happens when the local directory is on an NFS share. (This is NOT supported.) AUDIT_S3_FAILED_CREATING_METADATA_FILE ContainerSSH failed to create the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is writable and has enough disk space. AUDIT_S3_FAILED_METADATA_JSON_ENCODING ContainerSSH failed to encode the metadata file. This is a bug, please report it. AUDIT_S3_FAILED_READING_METADATA_FILE ContainerSSH failed to read the metadata file for the S3 upload in the local temporary directory. Check if the local directory specified is readable and the files have not been corrupted. AUDIT_S3_FAILED_STAT_QUEUE_ENTRY ContainerSSH failed to stat the queue file. This usually happens when the local directory is being manually manipulated. AUDIT_S3_FAILED_WRITING_METADATA_FILE ContainerSSH failed to write the local metadata file. Please check if your disk has enough disk space. AUDIT_S3_MULTIPART_ABORTING ContainerSSH is aborting a multipart upload. Check the log message for details. AUDIT_S3_MULTIPART_FAILED_ABORT ContainerSSH failed aborting a multipart upload from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_FAILED_LIST ContainerSSH failed to list multipart uploads on the object storage bucket. This is needed to abort uploads from a previously crashed ContainerSSH run. AUDIT_S3_MULTIPART_PART_UPLOADING ContainerSSH is uploading a part of an audit log to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_COMPLETE ContainerSSH completed the upload of an audit log part to the S3-compatible object storage. AUDIT_S3_MULTIPART_PART_UPLOAD_FAILED ContainerSSH failed to upload a part to the S3-compatible object storage. Check the message for details. AUDIT_S3_MULTIPART_UPLOAD ContainerSSH is starting a new S3 multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZATION_FAILED ContainerSSH has uploaded all audit log parts, but could not finalize the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZED ContainerSSH has uploaded all audit log parts and has successfully finalized the upload. AUDIT_S3_MULTIPART_UPLOAD_FINALIZING ContainerSSH has uploaded all audit log parts and is now finalizing the multipart upload. AUDIT_S3_MULTIPART_UPLOAD_INITIALIZATION_FAILED ContainerSSH failed to initialize a new multipart upload to the S3-compatible object storage. Check if the S3 configuration is correct and the provided S3 access key and secrets have permissions to start a multipart upload. AUDIT_S3_NO_SUCH_QUEUE_ENTRY ContainerSSH was trying to upload an audit log from the metadata file, but the audit log does not exist. AUDIT_S3_RECOVERING ContainerSSH found a previously aborted multipart upload locally and is now attempting to recover the upload. AUDIT_S3_REMOVE_FAILED ContainerSSH failed to remove an uploaded audit log from the local directory. This usually happens on Windows when a different process has the audit log open. (This is not a supported setup.) AUDIT_S3_SINGLE_UPLOAD ContainerSSH is uploading the full audit log in a single upload to the S3-compatible object storage. This happens when the audit log size is below the minimum size for a multi-part upload. AUDIT_S3_SINGLE_UPLOAD_COMPLETE ContainerSSH successfully uploaded the audit log as a single upload. AUDIT_S3_SINGLE_UPLOAD_FAILED ContainerSSH failed to upload the audit log as a single upload.","title":"Auditlog"},{"location":"reference/0.4.0/codes/#authentication","text":"Code Explanation AUTH ContainerSSH is trying to contact the authentication backend to verify the user credentials. AUTH_BACKEND_ERROR The ContainerSSH authentication server responded with a non-200 status code. ContainerSSH will retry the authentication for a few times before giving up. This is most likely a bug in your authentication server, please check your logs. AUTH_FAILED The user has provided invalid credentials and the authentication is rejected. AUTH_INVALID_STATUS This message indicates that the authentication server returned an invalid HTTP status code. AUTH_NOT_SUPPORTED The authentication method the client requested is not supported by ContainerSSH. AUTH_SUCCESSFUL The user has provided the correct credentials and the authentication is accepted.","title":"Authentication"},{"location":"reference/0.4.0/codes/#backend","text":"Code Explanation BACKEND_CONFIG_ERROR The backend retreived from the configuration server is invalid. See the error message for details.","title":"Backend"},{"location":"reference/0.4.0/codes/#configuration","text":"Code Explanation CONFIG_BACKEND_ERROR ContainerSSH has received an invalid response from the configuration server or the network connection broke. ContainerSSH will retry fetching the user-specific configuration until the timeout. If this error persists check the connectivity to the configuration server, or the logs of the configuration server itself to find out of there may be a specific error. CONFIG_INVALID_STATUS_CODE ContainerSSH has received a non-200 response code when calling a per-user backend configuration from the configuration server. CONFIG_REQUEST ContainerSSH is sending a quest to the configuration server to obtain a per-user backend configuration. CONFIG_RESPONSE ContainerSSH has received a per-user backend configuration from the configuration server. CONFIG_SERVER_AVAILABLE The ContainerSSH configuration server is now available at the specified address.","title":"Configuration"},{"location":"reference/0.4.0/codes/#docker","text":"Code Explanation DOCKER_AGENT_READ_FAILED The ContainerSSH Docker module failed to read from the ContainerSSH agent. This is most likely because the ContainerSSH guest agent is not present in the guest image, but agent support is enabled. DOCKER_CLOSE_INPUT_FAILED The ContainerSSH Docker module attempted to close the input (stdin) for reading but failed to do so. DOCKER_CLOSE_OUTPUT_FAILED The ContainerSSH Docker module attempted to close the output (stdout and stderr) for writing but failed to do so. DOCKER_CONFIG_ERROR The ContainerSSH Docker module detected a configuration error. Please check your configuration. DOCKER_CONTAINER_ATTACH The ContainerSSH Docker module is attaching to a container in session mode. DOCKER_CONTAINER_ATTACH_FAILED The ContainerSSH Docker module has failed to attach to a container in session mode. DOCKER_CONTAINER_CREATE The ContainerSSH Docker module is creating a container. DOCKER_CONTAINER_CREATE_FAILED The ContainerSSH Docker module failed to create a container. This may be a temporary and retried or a permanent error message. Check the log message for details. DOCKER_CONTAINER_REMOVE The ContainerSSH Docker module os removing the container. DOCKER_CONTAINER_REMOVE_FAILED The ContainerSSH Docker module could not remove the container. This message may be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_REMOVE_SUCCESSFUL The ContainerSSH Docker module has successfully removed the container. DOCKER_CONTAINER_SHUTTING_DOWN The ContainerSSH Docker module is shutting down a container. DOCKER_CONTAINER_SIGNAL The ContainerSSH Docker module is sending a signal to the container. DOCKER_CONTAINER_SIGNAL_FAILED The ContainerSSH Docker module has failed to send a signal to the container. DOCKER_CONTAINER_START The ContainerSSH Docker module is starting the previously-created container. DOCKER_CONTAINER_START_FAILED The ContainerSSH docker module failed to start the container. This message can either be temporary and retried or permanent. Check the log message for details. DOCKER_CONTAINER_STOP The ContainerSSH Docker module is stopping the container. DOCKER_CONTAINER_STOP_FAILED The ContainerSSH Docker module failed to stop the container. This message can be either temporary and retried or permanent. Check the log message for details. DOCKER_EXEC The ContainerSSH Docker module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the container. DOCKER_EXEC_ATTACH The ContainerSSH Docker module is attaching to the previously-created execution. DOCKER_EXEC_ATTACH_FAILED The ContainerSSH Docker module could not attach to the previously-created execution. DOCKER_EXEC_CREATE The ContainerSSH Docker module is creating an execution. DOCKER_EXEC_CREATE_FAILED The ContainerSSH Docker module has failed to create an execution. This can be temporary and retried or permanent. See the error message for details. DOCKER_EXEC_PID_READ_FAILED The ContainerSSH Docker module has failed to read the process ID from the ContainerSSH Guest Agent . This is most likely because the guest image does not contain the guest agent, but guest agent support has been enabled. DOCKER_EXEC_RESIZE The ContainerSSH Docker module is resizing the console. DOCKER_EXEC_RESIZE_FAILED The ContainerSSH Docker module failed to resize the console. DOCKER_EXEC_SIGNAL The ContainerSSH Docker module is delivering a signal in container mode. DOCKER_EXEC_SIGNAL_FAILED The ContainerSSH Docker module failed to deliver a signal. DOCKER_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Docker module failed to deliver a signal because ContainerSSH Guest Agent support is disabled. DOCKER_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Docker module successfully delivered the requested signal. DOCKER_EXIT_CODE The ContainerSSH Docker module is fetching the exit code from the program. DOCKER_EXIT_CODE_CONTAINER_RESTARTING The ContainerSSH Docker module could not fetch the exit code from the program because the container is restarting. This is typically a misconfiguration as ContainerSSH containers should not automatically restart. DOCKER_EXIT_CODE_FAILED The ContainerSSH Docker module has failed to fetch the exit code of the program. DOCKER_EXIT_CODE_NEGATIVE The ContainerSSH Docker module has received a negative exit code from Docker. This should never happen and is most likely a bug. DOCKER_EXIT_CODE_STILL_RUNNING The ContainerSSH Docker module could not fetch the program exit code because the program is still running. This error may be temporary and retried or permanent. DOCKER_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the container image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. DOCKER_IMAGE_LISTING The ContainerSSH Docker module is listing the locally present container images to determine if the specified container image needs to be pulled. DOCKER_IMAGE_LISTING_FAILED The ContainerSSH Docker module failed to list the images present in the local Docker daemon. This is used to determine if the image needs to be pulled. This can be because the Docker daemon is not reachable, the certificate is invalid, or there is something else interfering with listing the images. DOCKER_IMAGE_PULL The ContainerSSH Docker module is pulling the container image. DOCKER_IMAGE_PULL_FAILED The ContainerSSH Docker module failed to pull the specified container image. This can be because of connection issues to the Docker daemon, or because the Docker daemon itself can't pull the image. If you don't intend to have the image pulled you should set the ImagePullPolicy to Never . See the Docker documentation for details. DOCKER_IMAGE_PULL_NEEDED_CHECKING The ContainerSSH Docker module is checking if an image pull is needed. DOCKER_PROGRAM_ALREADY_RUNNING The ContainerSSH Docker module can't execute the request because the program is already running. This is a client error. DOCKER_SIGNAL_FAILED_NO_PID The ContainerSSH Docker module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. DOCKER_STREAM_INPUT_FAILED The ContainerSSH Docker module failed to stream stdin to the Docker engine. DOCKER_STREAM_OUTPUT_FAILED The ContainerSSH Docker module failed to stream stdout and stderr from the Docker engine. DOCKER_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Docker module is not configured to run the requested subsystem.","title":"Docker"},{"location":"reference/0.4.0/codes/#http","text":"Code Explanation HTTP_CLIENT_CONNECTION_FAILED This message indicates a connection failure on the network level. HTTP_CLIENT_DECODE_FAILED This message indicates that decoding the JSON response has failed. The status code is set for this code. HTTP_CLIENT_ENCODE_FAILED This message indicates that JSON encoding the request failed. This is usually a bug. HTTP_CLIENT_REDIRECT This message indicates that the server responded with a HTTP redirect. HTTP_CLIENT_REDIRECTS_DISABLED This message indicates that ContainerSSH is not following a HTTP redirect sent by the server. Use the allowRedirects option to allow following HTTP redirects. HTTP_CLIENT_REQUEST This message indicates that a HTTP request is being sent from ContainerSSH HTTP_CLIENT_RESPONSE This message indicates that ContainerSSH received a HTTP response from a server. HTTP_SERVER_ENCODE_FAILED The HTTP server failed to encode the response object. This is a bug, please report it. HTTP_SERVER_RESPONSE_WRITE_FAILED The HTTP server failed to write the response.","title":"HTTP"},{"location":"reference/0.4.0/codes/#kubernetes","text":"Code Explanation KUBERNETES_CLOSE_OUTPUT_FAILED The ContainerSSH Kubernetes module attempted to close the output (stdout and stderr) for writing but failed to do so. KUBERNETES_CONFIG_ERROR The ContainerSSH Kubernetes module detected a configuration error. Please check your configuration. KUBERNETES_EXEC The ContainerSSH Kubernetes module is creating an execution. This may be in connection mode, or it may be the module internally using the exec mechanism to deliver a payload into the pod. KUBERNETES_EXEC_RESIZE The ContainerSSH Kubernetes module is resizing the terminal window. KUBERNETES_EXEC_RESIZE_FAILED The ContainerSSH Kubernetes module failed to resize the console. KUBERNETES_EXEC_SIGNAL The ContainerSSH Kubernetes module is delivering a signal. KUBERNETES_EXEC_SIGNAL_FAILED The ContainerSSH Kubernetes module failed to deliver a signal. KUBERNETES_EXEC_SIGNAL_FAILED_NO_AGENT The ContainerSSH Kubernetes module failed to deliver a signal because guest agent support is disabled. KUBERNETES_EXEC_SIGNAL_SUCCESSFUL The ContainerSSH Kubernetes module successfully delivered the requested signal. KUBERNETES_EXIT_CODE_FAILED The ContainerSSH Kubernetes module has failed to fetch the exit code of the program. KUBERNETES_GUEST_AGENT_DISABLED The ContainerSSH Guest Agent has been disabled, which is strongly discouraged. ContainerSSH requires the guest agent to be installed in the pod image to facilitate all SSH features. Disabling the guest agent will result in breaking the expectations a user has towards an SSH server. We provide the ability to disable guest agent support only for cases where the guest agent binary cannot be installed in the image at all. KUBERNETES_PID_RECEIVED The ContainerSSH Kubernetes module has received a PID from the Kubernetes guest agent. KUBERNETES_POD_ATTACH The ContainerSSH Kubernetes module is attaching to a pod in session mode. KUBERNETES_POD_CREATE The ContainerSSH Kubernetes module is creating a pod. KUBERNETES_POD_CREATE_FAILED The ContainerSSH Kubernetes module failed to create a pod. This may be a temporary and retried or a permanent error message. Check the log message for details. KUBERNETES_POD_REMOVE The ContainerSSH Kubernetes module is removing a pod. KUBERNETES_POD_REMOVE_FAILED The ContainerSSH Kubernetes module could not remove the pod. This message may be temporary and retried or permanent. Check the log message for details. KUBERNETES_POD_REMOVE_SUCCESSFUL The ContainerSSH Kubernetes module has successfully removed the pod. KUBERNETES_POD_SHUTTING_DOWN The ContainerSSH Kubernetes module is shutting down a pod. KUBERNETES_POD_WAIT The ContainerSSH Kubernetes module is waiting for the pod to come up. KUBERNETES_POD_WAIT_FAILED The ContainerSSH Kubernetes module failed to wait for the pod to come up. Check the error message for details. KUBERNETES_PROGRAM_ALREADY_RUNNING The ContainerSSH Kubernetes module can't execute the request because the program is already running. This is a client error. KUBERNETES_PROGRAM_NOT_RUNNING This message indicates that the user requested an action that can only be performed when a program is running, but there is currently no program running. KUBERNETES_SIGNAL_FAILED_EXITED The ContainerSSH Kubernetes module can't deliver a signal because the program already exited. KUBERNETES_SIGNAL_FAILED_NO_PID The ContainerSSH Kubernetes module can't deliver a signal because no PID has been recorded. This is most likely because guest agent support is disabled. KUBERNETES_SUBSYSTEM_NOT_SUPPORTED The ContainerSSH Kubernetes module is not configured to run the requested subsystem. KUBERUN_DEPRECATED This message indicates that you are still using the deprecated KubeRun backend. This backend doesn't support all safety and functionality improvements and will be removed in the future. Please read the deprecation notice for a migration guide KUBERUN_EXEC_DISABLED This message indicates that the user tried to execute a program, but program execution is disabled in the legacy KubeRun configuration. KUBERUN_INSECURE This message indicates that you are using Kubernetes in the \"insecure\" mode where certificate verification is disabled. This is a major security flaw, has been deprecated and is removed in the new Kubernetes backend. Please change your configuration to properly validates the server certificates.","title":"Kubernetes"},{"location":"reference/0.4.0/codes/#log","text":"Code Explanation LOG_FILE_OPEN_FAILED ContainerSSH failed to open the specified log file. LOG_ROTATE_FAILED ContainerSSH cannot rotate the logs as requested because of an underlying error. LOG_WRITE_FAILED ContainerSSH cannot write to the specified log file. This usually happens because the underlying filesystem is full or the log is located on a non-local storage (e.g. NFS), which is not supported. TEST This is message that should only be seen in unit and component tests, never in production. UNKNOWN_ERROR This is an untyped error. If you see this in a log that is a bug and should be reported.","title":"Log"},{"location":"reference/0.4.0/codes/#metrics","text":"Code Explanation METRICS_AVAILABLE The metrics service is now online and ready for service.","title":"Metrics"},{"location":"reference/0.4.0/codes/#security","text":"Code Explanation SECURITY_ENV_REJECTED ContainerSSH rejected setting the environment variable because it does not pass the security settings. SECURITY_EXEC_FAILED_SETENV Program execution failed in conjunction with the forceCommand option because ContainerSSH could not set the SSH_ORIGINAL_COMMAND environment variable on the backend. SECURITY_EXEC_FORCING_COMMAND ContainerSSH is replacing the command passed from the client (if any) to the specified command and is setting the SSH_ORIGINAL_COMMAND environment variable. SECURITY_EXEC_REJECTED A program execution request has been rejected because it doesn't conform to the security settings. SECURITY_SHELL_REJECTED ContainerSSH rejected launching a shell due to the security settings. SECURITY_SIGNAL_REJECTED ContainerSSH rejected delivering a signal because it does not pass the security settings. SECURITY_SUBSYSTEM_REJECTED ContainerSSH rejected the subsystem because it does pass the security settings. SECURITY_TTY_REJECTED ContainerSSH rejected the pseudoterminal request because of the security settings.","title":"Security"},{"location":"reference/0.4.0/codes/#service","text":"Code Explanation SERVICE_CRASHED A ContainerSSH has stopped improperly. SERVICE_POOL_RUNNING All ContainerSSH services are now running. SERVICE_POOL_STARTING All ContainerSSH services are starting. SERVICE_POOL_STOPPED ContainerSSH has stopped all services. SERVICE_POOL_STOPPING ContainerSSH is stopping all services. SERVICE_RUNNING A ContainerSSH service is now running SERVICE_STARTING ContainerSSH is starting a component service SERVICE_STOPPED A ContainerSSH service has stopped. SERVICE_STOPPING A ContainerSSH service is now stopping.","title":"Service"},{"location":"reference/0.4.0/codes/#ssh","text":"Code Explanation SSH_ALREADY_RUNNING The SSH server is already running and has been started again. This is a bug, please report it. SSH_AUTH_FAILED The user has provided invalid credentials. SSH_AUTH_SUCCESSFUL The user has provided valid credentials and is now authenticated. SSH_AUTH_UNAVAILABLE The user has requested an authentication method that is currently unavailable. SSH_AVAILABLE The SSH service is now online and ready for service. SSH_BACKEND_REJECTED_HANDSHAKE The backend has rejected the connecting user after successful authentication. SSH_CHANNEL_REQUEST The user has send a new channel-specific request. SSH_CHANNEL_REQUEST_FAILED ContainerSSH couldn't fulfil the channel-specific request. SSH_CHANNEL_REQUEST_SUCCESSFUL ContainerSSH has successfully processed the channel-specific request. SSH_CONNECTED A user has connected over SSH. SSH_DECODE_FAILED ContainerSSH failed to decode something from the user. This is either a bug in ContainerSSH or in the connecting client. SSH_DISCONNECTED An SSH connection has been severed. SSH_EXIT ContainerSSH is sending the exit code of the program to the user. SSH_EXIT_CODE_FAILED ContainerSSH failed to obtain and send the exit code of the program to the user. SSH_EXIT_SIGNAL ContainerSSH is sending the exit signal from an abnormally exited program to the user. SSH_HANDSHAKE_FAILED The connecting party failed to establish a secure SSH connection. This is most likely due to invalid credentials or a backend error. SSH_HANDSHAKE_SUCCESSFUL The user has provided valid credentials and has now established an SSH connection. SSH_LISTEN_CLOSE_FAILED ContainerSSH failed to close the listen socket. SSH_NEW_CHANNEL A user has established a new SSH channel. (Not connection!) SSH_NEW_CHANNEL_REJECTED The user has requested a new channel to be opened, but was rejected. SSH_REPLY_SEND_FAILED ContainerSSH couldn't send the reply to a request to the user. This is usually the case if the user suddenly disconnects. SSH_START_FAILED ContainerSSH failed to start the SSH service. This is usually because of invalid configuration. SSH_UNSUPPORTED_CHANNEL_TYPE The user requested a channel type that ContainerSSH doesn't support (e.g. TCP/IP forwarding). SSH_UNSUPPORTED_GLOBAL_REQUEST The users client has send a global request ContainerSSH does not support. This is nothing to worry about.","title":"SSH"},{"location":"reference/0.4.0/configserver/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Configuration Server ContainerSSH has the ability to configure the backend and the launched container dynamically based on the username and/or IP address. To do this ContainerSSH calls out to a configuration server if configured. Configuration \u00b6 The configserver webhook can be configured in the main configuration using the following structure: configuration : <options> The following options are supported: Name Type Description url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion string Minimum TLS version to support. See the TLS version section below. curve string Elliptic curve algorithms to support. See the [Elliptic curve algorithms][] section below. cipher []string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false. Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Middle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Elliptic curve algorithms \u00b6 The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1 Cipher suites \u00b6 The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Client authentication \u00b6 In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem The configuration webhook \u00b6 The configuration webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create a configuration server. The config server will receive a request in following format: { \"username\" : \"ssh username\" , \"connectionId\" : \"ssh session ID\" } The configuration server will have to respond with the following response accompanied with the content type of application/json . { \"config\" : { // Provide a par t ial co nf igura t io n here } } The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running ./containerssh --dump-config . The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file. Currently only the following options can be set from the configuration server: Backend Docker Kubernetes DockerRun KubeRun Security Tip We provide a Go library to implement a config server .","title":"Configuration server"},{"location":"reference/0.4.0/configserver/#configuration","text":"The configserver webhook can be configured in the main configuration using the following structure: configuration : <options> The following options are supported: Name Type Description url string HTTP URL of the configuration server to call. Leaving this field empty disables the webhook. timeout string Timeout for the webhook. Can be provided with time units (e.g. 6s ), defaults to nanoseconds if provided without a time unit. cacert string CA certificate in PEM format or filename that contains the CA certificate. This is field is required for https:// URL's on Windows because of Golang issue #16736 cert string Client certificate in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. key string Private key in PEM format or filename that contains the client certificate for x509 authentication with the configuration server. tlsVersion string Minimum TLS version to support. See the TLS version section below. curve string Elliptic curve algorithms to support. See the [Elliptic curve algorithms][] section below. cipher []string Which cipher suites to support. See the Cipher suites section below. allowRedirects bool Allow following HTTP redirects. Defaults to false.","title":"Configuration"},{"location":"reference/0.4.0/configserver/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Middle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/0.4.0/configserver/#tls-version","text":"The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported.","title":"TLS version"},{"location":"reference/0.4.0/configserver/#elliptic-curve-algorithms","text":"The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1","title":"Elliptic curve algorithms"},{"location":"reference/0.4.0/configserver/#cipher-suites","text":"The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305","title":"Cipher suites"},{"location":"reference/0.4.0/configserver/#client-authentication","text":"In order to safeguard secrets in the configuration the configuration server should be protected by either firewalling it appropriately, but it is better to use x509 client certificates as a means of authentication. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added as a client CA in your configuration server. This CA does not have to be the same used to sign the server certificate. Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem should then be added to the configuration as client credentials: configuration : cert : /path/to/containerssh.pem key : /path/to/containerssh-key.pem","title":"Client authentication"},{"location":"reference/0.4.0/configserver/#the-configuration-webhook","text":"The configuration webhook is a simple JSON POST request to which the server must respond with a JSON response. Note We have an OpenAPI document available for the authentication and configuration server. You can check the exact values available there, or use the OpenAPI document to generate parts of your server code. Tip We provide a Go library to create a configuration server. The config server will receive a request in following format: { \"username\" : \"ssh username\" , \"connectionId\" : \"ssh session ID\" } The configuration server will have to respond with the following response accompanied with the content type of application/json . { \"config\" : { // Provide a par t ial co nf igura t io n here } } The configuration JSON structure is identical to the YAML described in this reference manual and the full configuration can be dumped by running ./containerssh --dump-config . The server is free to return only partial options that it wants to set. Any options that are sent overwrite the ones from the configuration file. Currently only the following options can be set from the configuration server: Backend Docker Kubernetes DockerRun KubeRun Security Tip We provide a Go library to implement a config server .","title":"The configuration webhook"},{"location":"reference/0.4.0/docker/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The Docker backend The Docker backend should work with any Docker Engine version starting with 1.6 thanks to the version negotiation present. We fix issues starting with Docker version 18.02. Tip This is the documentation for the Docker backend . For deploying ContainerSSH inside Docker please see the installation guide . The base configuration structure \u00b6 In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : docker docker : connection : <connection configuration here> execution : <execution configuration here> timeouts : <timeouts configuration here> Configuring connection parameters \u00b6 The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : connection : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Configuring container execution \u00b6 Container execution options can be specified as follows: docker : execution : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases. Basic container configuration \u00b6 The basic configuration options are as follows: docker : execution : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu Mounting volumes \u00b6 Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs Bind mounts \u00b6 Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : execution : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume. Mounts \u00b6 The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : execution : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755 tmpfs \u00b6 The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : execution : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page . Other options \u00b6 Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description mode string Specify connection to launch one container per SSH connection or session to run one container per SSH session (multiple containers per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how docker exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. imagePullPolicy Never,IfNotPresent,Always Specifies when to pull the container image. Defaults to IfNotPresent , which pulls the image when it is not locally present or if the image has no tag/has the latest tag. It is recommended that you provide a custom, versioned image name to prevent pulling the image at every connection. Configuring timeouts \u00b6 The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description containerStart The time to wait for the container to start. containerStop The time to wait for the container to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete. Securing Docker \u00b6 Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server. Securing the Docker socket \u00b6 Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates! Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : execution : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : execution : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty. Preventing storage exhaustion \u00b6 A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : execution : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : execution : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : execution : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: docker : execution : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the totoal memory + swap usage: docker : execution : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : execution : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : execution : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000 Preventing process exhaustion \u00b6 You can also limit the number of processes that can be launched inside the container: docker : execution : host : resources : pidslimit : 1000 Limiting network access \u00b6 In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : execution : container : networkdisabled : true Limiting disk I/O \u00b6 Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : execution : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Docker"},{"location":"reference/0.4.0/docker/#the-base-configuration-structure","text":"In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : docker docker : connection : <connection configuration here> execution : <execution configuration here> timeouts : <timeouts configuration here>","title":"The base configuration structure"},{"location":"reference/0.4.0/docker/#configuring-connection-parameters","text":"The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : connection : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE-----","title":"Configuring connection parameters"},{"location":"reference/0.4.0/docker/#configuring-container-execution","text":"Container execution options can be specified as follows: docker : execution : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If the a container already exists with the same name the container creation will fail, so this should be left empty for most use cases.","title":"Configuring container execution"},{"location":"reference/0.4.0/docker/#basic-container-configuration","text":"The basic configuration options are as follows: docker : execution : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu","title":"Basic container configuration"},{"location":"reference/0.4.0/docker/#mounting-volumes","text":"Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs","title":"Mounting volumes"},{"location":"reference/0.4.0/docker/#bind-mounts","text":"Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : execution : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume.","title":"Bind mounts"},{"location":"reference/0.4.0/docker/#mounts","text":"The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : execution : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755","title":"Mounts"},{"location":"reference/0.4.0/docker/#tmpfs","text":"The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : execution : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page .","title":"tmpfs"},{"location":"reference/0.4.0/docker/#other-options","text":"Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description mode string Specify connection to launch one container per SSH connection or session to run one container per SSH session (multiple containers per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how docker exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. imagePullPolicy Never,IfNotPresent,Always Specifies when to pull the container image. Defaults to IfNotPresent , which pulls the image when it is not locally present or if the image has no tag/has the latest tag. It is recommended that you provide a custom, versioned image name to prevent pulling the image at every connection.","title":"Other options"},{"location":"reference/0.4.0/docker/#configuring-timeouts","text":"The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description containerStart The time to wait for the container to start. containerStop The time to wait for the container to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete.","title":"Configuring timeouts"},{"location":"reference/0.4.0/docker/#securing-docker","text":"Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server.","title":"Securing Docker"},{"location":"reference/0.4.0/docker/#securing-the-docker-socket","text":"Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : connection : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates!","title":"Securing the Docker socket"},{"location":"reference/0.4.0/docker/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : execution : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : execution : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty.","title":"Preventing root escalation"},{"location":"reference/0.4.0/docker/#preventing-storage-exhaustion","text":"A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : execution : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : execution : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : execution : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted.","title":"Preventing storage exhaustion"},{"location":"reference/0.4.0/docker/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: docker : execution : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the totoal memory + swap usage: docker : execution : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : execution : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true","title":"Preventing memory exhaustion"},{"location":"reference/0.4.0/docker/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : execution : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000","title":"Preventing CPU exhaustion"},{"location":"reference/0.4.0/docker/#preventing-process-exhaustion","text":"You can also limit the number of processes that can be launched inside the container: docker : execution : host : resources : pidslimit : 1000","title":"Preventing process exhaustion"},{"location":"reference/0.4.0/docker/#limiting-network-access","text":"In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : execution : container : networkdisabled : true","title":"Limiting network access"},{"location":"reference/0.4.0/docker/#limiting-disk-io","text":"Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : execution : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Limiting disk I/O"},{"location":"reference/0.4.0/dockerrun/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The DockerRun backend Warning The DockerRun backend is deprecated and the Docker backend should be used instead. Read the migration guide here \u00bb The DockerRun backend should work with any Docker Engine version starting with 1.6 thanks to the version negotiation present. We fix issues starting with Docker version 18.02. Tip This is the documentation for the DockerRun backend . For deploying ContainerSSH inside Docker please see the installation guide . The base configuration structure \u00b6 In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : dockerrun dockerrun : <connection configuration here> config : <execution configuration here> Configuring connection parameters \u00b6 The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Configuring container execution \u00b6 Container execution options can be specified as follows: docker : config : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If a container already exists with the same name the container creation will fail, so this should be left empty for most use cases. Basic container configuration \u00b6 The basic configuration options are as follows: docker : config : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu Mounting volumes \u00b6 Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs Bind mounts \u00b6 Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : config : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume. Mounts \u00b6 The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : config : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755 tmpfs \u00b6 The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : config : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page . Other options \u00b6 Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. timeout string Timeout for container operations in nanoseconds. Time units can be set. Securing Docker \u00b6 Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server. Securing the Docker socket \u00b6 Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates! Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : config : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : config : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty. Preventing storage exhaustion \u00b6 A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : config : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : config : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : config : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to crash the server. This can be prevented using the following configuration: docker : config : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the total memory + swap usage: docker : config : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : config : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : config : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000 Preventing process exhaustion \u00b6 You can also limit the number of processes that can be launched inside the container: docker : config : host : resources : pidslimit : 1000 Limiting network access \u00b6 In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : config : container : networkdisabled : true Limiting disk I/O \u00b6 Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : config : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"DockerRun (deprecated)"},{"location":"reference/0.4.0/dockerrun/#the-base-configuration-structure","text":"In order to use the Docker backend you must specify the following configuration entries via the configuration file or the configuration server: backend : dockerrun dockerrun : <connection configuration here> config : <execution configuration here>","title":"The base configuration structure"},{"location":"reference/0.4.0/dockerrun/#configuring-connection-parameters","text":"The Docker backend defaults to connecting to the Docker Engine using the Docker socket. The default location for this is on UNIX systems is unix:///var/run/docker.sock , on Windows npipe:////./pipe/docker_engine . ContainerSSH must have permissions to access the Docker socket. The Docker socket location can be changed with the host option: docker : host : 127.0.0.1:2375 However, exposing Docker without certificate authentication is dangerous . It is recommended to generate a certificate for authentication and pass it to ContainerSSH using the following options: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE-----","title":"Configuring connection parameters"},{"location":"reference/0.4.0/dockerrun/#configuring-container-execution","text":"Container execution options can be specified as follows: docker : config : container : image : containerssh/containerssh <other container options> host : <host options> network : <network options> platform : <platform options> containerName : \"<container name here>\" <ContainerSSH-specific options here> The container , host , network , and platform options contain settings described in the Docker API . The containerName option contains the name for the container. If a container already exists with the same name the container creation will fail, so this should be left empty for most use cases.","title":"Configuring container execution"},{"location":"reference/0.4.0/dockerrun/#basic-container-configuration","text":"The basic configuration options are as follows: docker : config : container : image : containerssh/containerssh env : - VAR=value # cmd is only respected in session mode, see below. cmd : - /run/this/command user : ubuntu","title":"Basic container configuration"},{"location":"reference/0.4.0/dockerrun/#mounting-volumes","text":"Volumes can be mounted in 3 ways: Using bind mounts Using mounts Using tmpfs","title":"Mounting volumes"},{"location":"reference/0.4.0/dockerrun/#bind-mounts","text":"Bind mounts mount a directory from the host into the container. The syntax is fairly simple: docker : config : host : binds : - \"/path-on-the-host:/path-in-the-container[:options]\" Instead of the host path you can also specify a volume name. The following options are suported: nocopy If you set this flag Docker will not automatically copy the data that exists in the container image to the volume on mounting. ro|rw Set volume to read-only or read-write. z Apply the shared SELinux label to the volume allowing multiple containers to write the same volume. Z Apply the private unshared SELinux label to the volume allowing only the current container to use it. [r]shared, [r]slave, [r]private Sets the mount propagation behavior of the volume.","title":"Bind mounts"},{"location":"reference/0.4.0/dockerrun/#mounts","text":"The mounts option give you more flexibility to mount something, including using a volume driver. This is especially useful when mounting a volume from an external storage, for example NFS. It can be configured as follows: docker : config : host : mounts : - target : /path/in/container source : <volume name, host path, etc> type : <bind|volume|tmpfs|npipe> readonly : <true|false> consistency : <default|consistent|cached|delegated> # For bind type only: bindoptions : propagation : <private|rprivate|shared|rshared|slave|rslave> #Disable recursive bind mount nonrecursive : <true|false> # For volume type only: volumeoptions : # Disable copying files from the image to the volume nocopy : <true|false> labels : - key : value driverconfig : name : drivername options : <driveroption> : <drivervalue> # For tmpfs type only: tmpfsoptions : sizebytes : <size in bytes> mode : 0755","title":"Mounts"},{"location":"reference/0.4.0/dockerrun/#tmpfs","text":"The tmpfs method provides a temporary filesystem in memory. The contents are deleted when the container is removed. docker : config : host : tmpfs : /container/directory : <options> The detailed options for tmpfs can be found on the tmpfs man page .","title":"tmpfs"},{"location":"reference/0.4.0/dockerrun/#other-options","text":"Apart from the container , host , network , platform and containerName options ContainerSSH has the following options for execution. These should not be changed unless required. Name Type Description subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. timeout string Timeout for container operations in nanoseconds. Time units can be set.","title":"Other options"},{"location":"reference/0.4.0/dockerrun/#securing-docker","text":"Docker is a hard backend to secure since access to the Docker socket automatically means privileged access to the host machine. For maximum security Docker should be deployed on a separate host from ContainerSSH. This prevents an attacker that is able to escape a container to extract the ContainerSSH credentials for the authentication and configuration server.","title":"Securing Docker"},{"location":"reference/0.4.0/dockerrun/#securing-the-docker-socket","text":"Since ContainerSSH should never run as root it is recommended to access the Docker socket over TCP. In order to secure the TCP connection Docker will need certificates. The detailed description of this process is described in the Docker documentation . The certificates can be provided for ContainerSSH using the following fields: docker : host : <ip and port of the Docker engine> cert : | -----BEGIN CERTIFICATE----- <client certificate here> -----END CERTIFICATE----- key : | -----BEGIN RSA PRIVATE KEY----- <client key here> -----END RSA PRIVATE KEY----- cacert : | -----BEGIN CERTIFICATE----- <CA certificate here> -----END CERTIFICATE----- Danger Never expose the Docker socket on TCP without configuring certificates!","title":"Securing the Docker socket"},{"location":"reference/0.4.0/dockerrun/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Docker it is prudent not to run container workloads as root. For example, you can set the container to run at uid 1000 as follows: docker : config : container : user : 1000 If root is required inside the container user namespace mapping . There are a few steps required to make this happen: First, you need to create the files called /etc/subuid and /etc/subgid . These files have the following format: <username/uid>:<startuid/gid>:<uid/gid count> For example: 1000:1000:65536 In this case the first UID is the UID outside of the container being mapped, the second UID is the starting UID that will map to the root user inside the container, and the third is the number of UIDs inside the container. This is the same for group IDs. Tip Using a UID instead of a username in the first field of the mapping will cause a significant performance increase when parsing the file. Then you can configure Docker to use this subordinate UID. This can be done by either configuring the Docker daemon or by configuring ContainerSSH to use this mapping. To configure the Docker daemon you can pass the following parameter to dockerd : dockerd --userns-remap = \"<UID>:<GID>\" Alternatively, you can configure this per-container in ContainerSSH: docker : config : host : usernsmode : <UID>:<GID> In both cases the passed UID and GID must map to the entries in /etc/subuid and /etc/subgid . Warning Using a large number of mappings (10k or more) will cause a performance penalty.","title":"Preventing root escalation"},{"location":"reference/0.4.0/dockerrun/#preventing-storage-exhaustion","text":"A malicious user could cause the Docker host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk and can only be mitigated partially as limiting disk space per-container is not supported in every backend configuration. You can enable storage limits in ContainerSSH like this: docker : config : host : storageopt : size : 524288000 However, make sure to test if this works on your Docker configuration. When using overlayfs2 on an ext4 filesystem this does not work, you may have to switch to xfs or move to devicemapper to make use of this option. Some directories, such as /tmp or /run can also be put on tmpfs to store in memory. This can be configured as follows: docker : config : host : tmpfs : /tmp : rw,noexec,nosuid,size=65536k /run : rw,noexec,nosuid,size=65536k To prevent storage exhaustion it is recommended to set the root FS to be read only: docker : config : host : readonlyrootfs : true Danger If you are using the auditlog make sure you put the local directory on a separate disk than the /var/lib/docker directory even if you don't use storage limits to prevent the audit log from getting corrupted.","title":"Preventing storage exhaustion"},{"location":"reference/0.4.0/dockerrun/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to crash the server. This can be prevented using the following configuration: docker : config : host : resources : memory : 26214400 # Soft limit memoryreservation : 20000000 The memory is specified in bytes. In addition it is recommended to enable cgroup swap limit support . This allows for limiting the total memory + swap usage: docker : config : host : resources : memoryswap : 26214400 # Tune how likely the container is to be swapped out memoryswappiness : 90 If the host still runs out of memory the OOM killer will try to kill a process to free up memory. The OOM killer can be tuned using the following options: docker : config : host : # Tune how likely the OOM killer is to kill this container oomscoreadj : 500 resources : # Disable OOM killer for this container oomkilldisable : true","title":"Preventing memory exhaustion"},{"location":"reference/0.4.0/dockerrun/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: docker : config : host : resources : # Limit which cores the container should run on cpusetcpus : 1-3,5 # Limit which Memory nodes the container should run on (For NUMA systems) cpusetmems : 1-3,5 # CPU scheduling period in microseconds cpuperiod : 10000 # CPU quota to allocate tho the container cpuquota : 1000 # As above, but for realtime tasks (guaranteed CPU time) cpurealtimeperiod : 10000 cpurealtimequota : 1000","title":"Preventing CPU exhaustion"},{"location":"reference/0.4.0/dockerrun/#preventing-process-exhaustion","text":"You can also limit the number of processes that can be launched inside the container: docker : config : host : resources : pidslimit : 1000","title":"Preventing process exhaustion"},{"location":"reference/0.4.0/dockerrun/#limiting-network-access","text":"In some use cases you don't want a user to be able to access resources on the network apart from the SSH connection. This can be achieved by disabling network inside the container: docker : config : container : networkdisabled : true","title":"Limiting network access"},{"location":"reference/0.4.0/dockerrun/#limiting-disk-io","text":"Docker has a built-in facility to limit the disk I/O by IOPS and by bytes per second. This can be done using the following configuration structure: docker : config : host : resources : # Set relative weight against other containers blkioweight : <weight number> # Set relative weight against other containers blkioweightdevice : - path : <device path> weight : <weight number> # Read BPS blkiodevicereadbps : - path : <device path> rate : <bytes per second> # Write BPS blkiodevicewritebps : - path : <device path> rate : <bytes per second> # Read IOps blkiodevicereadiops : - path : <device path> rate : <IO per second> # Write IOps blkiodevicewriteiops : - path : <device path> rate : <IO per second> The device path has to be a path to a block device , e.g. /dev/vda . It does not work on filesystems or character devices.","title":"Limiting disk I/O"},{"location":"reference/0.4.0/features/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Supported SSH features \u00b6 This table contains the list of currently supported SSH features in ContainerSSH. Feature Support RFC Shell execution RFC 4254 section 6.5 Command execution RFC 4254 section 6.5 Subsystem execution RFC 4254 section 6.5 Requesting a Pseudo-Terminal RFC 4254 section 6.2 Setting environment variables RFC 4254 section 6.4 Forwarding signals RFC 4254 section 6.9 Window dimension change RFC 4254 section 6.7 Return exit statuses RFC 4254 section 6.10 Return exit signals RFC 4254 section 6.10 TCP/IP port forwarding RFC 4254 section 7 X11 forwarding RFC 4254 section 6.2 SSH agent forwarding (OpenSSH extension: auth-agent-req@openssh.com ) draft-ietf-secsh-agent-02 Keepalive (OpenSSH extension: keepalive@openssh.com ) No more sessions (OpenSSH extension: no-more-sessions@openssh.com )","title":"Supported features"},{"location":"reference/0.4.0/features/#supported-ssh-features","text":"This table contains the list of currently supported SSH features in ContainerSSH. Feature Support RFC Shell execution RFC 4254 section 6.5 Command execution RFC 4254 section 6.5 Subsystem execution RFC 4254 section 6.5 Requesting a Pseudo-Terminal RFC 4254 section 6.2 Setting environment variables RFC 4254 section 6.4 Forwarding signals RFC 4254 section 6.9 Window dimension change RFC 4254 section 6.7 Return exit statuses RFC 4254 section 6.10 Return exit signals RFC 4254 section 6.10 TCP/IP port forwarding RFC 4254 section 7 X11 forwarding RFC 4254 section 6.2 SSH agent forwarding (OpenSSH extension: auth-agent-req@openssh.com ) draft-ietf-secsh-agent-02 Keepalive (OpenSSH extension: keepalive@openssh.com ) No more sessions (OpenSSH extension: no-more-sessions@openssh.com )","title":"Supported SSH features"},{"location":"reference/0.4.0/hardening/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Hardening ContainerSSH ContainerSSH is built to secure its inner workings as much as possible. You can take several steps to secure it further. Running ContainerSSH \u00b6 ContainerSSH should be run as an unprivileged user (e.g. root) as it does not need access to any privileged resources. When running it from the default container image containerssh/containerssh this is the default. When running it outside a container you should keep the default bind port of 2222. On Linux you can then use iptables to redirect port 22 to the unprivileged port: iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222 When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman . Tip Don't forget to add this rule to your persistent firewall configuration. Securing authentication \u00b6 Authentication server connection \u00b6 ContainerSSH talks to an authentication server over HTTP. There are two potential attacks here: If an attacker can intercept the connection between ContainerSSH and the authentication server the attacker can read the passwords for password authentication. If an attacker can send requests to the authentication server they can brute force SSH passwords. Therefore, the connection between ContainerSSH and the authentication server should be secured in the following 3 ways: Implement firewalls such that only ContainerSSH can access the authentication server. Only use HTTPS with certificate verification to access the authentication server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the authentication server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the authentication chapter . Rate limiting \u00b6 ContainerSSH contains no rate limiting for authentication across connections, this is the job of the authentication server. The number of authentication attempts within a connection is limited to 6. The authentication server must take care to do rate limiting right: within a single connection multiple authentication attempts may be made and if the authentication server returns a non-200 response code ContainerSSH will retry connections. It is recommended that the authentication server use the connectionId field to distinguish between SSH connections as this field is guaranteed to be unique for a connection. Client credential security \u00b6 Passwords are vulnerable to being stolen and cannot be transferred to hardware keys. For the most security it is recommended to disable password authentication and only use SSH keys. When storing SSH keys on the client computer they should be protected by a passphrase and limited permissions on the key file. If possible, however, SSH keys should be transferred to a hardware token like the Yubikey . The Yubikey should be configured to require a physical touch on every authentication and should be unlocked with a passcode to prevent unauthorized applications on the client accessing the key for connections. Securing the configuration server \u00b6 ContainerSSH can optionally reach out to a configuration server to fetch dynamic backend configuration based on the username. The backend configuration may contain secrets, such as certificates for accessing Docker and Kubernetes, or application-specific secrets. Therefore, if an attacker can access the configuration server they can extract secrets from the returned configuration. This can be mitigated similar to the authentication server: Implement firewalls such that only ContainerSSH can access the configuration server. Only use HTTPS with certificate verification to access the configuration server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the configuration server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the configuration server chapter . Limiting SSH requests \u00b6 The security module provides the ability to limit which requests are allowed from a client. As ContainerSSH is upgraded the default is to allow new features that will come in with future releases (e.g. TCP port forwarding). In order to secure ContainerSSH for future releases it is recommended to set the defaultMode to disable and enable the modes that you need. For subsystems specifically we recommend filtering and allowing only the sftp subsystem as future ContainerSSH versions may support more subsystems. security : defaultMode : disable env : mode : enable command : mode : enable shell : mode : enable subsystem : mode : filter allow : - sftp tty : mode : enable signal : mode : enable Securing Docker \u00b6 Docker-specific settings for security are described in the Docker documentation . Securing Kubernetes \u00b6 Kubernetes-specific settings for security are described in the Kubernetes documentation .","title":"Hardening guide"},{"location":"reference/0.4.0/hardening/#running-containerssh","text":"ContainerSSH should be run as an unprivileged user (e.g. root) as it does not need access to any privileged resources. When running it from the default container image containerssh/containerssh this is the default. When running it outside a container you should keep the default bind port of 2222. On Linux you can then use iptables to redirect port 22 to the unprivileged port: iptables -t nat -I PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222 When using Docker ContainerSSH will need access to the Docker socket. This undeniably means that ContainerSSH will be able to launch root processes on the host machine. You may want to look into running Docker in rootless mode or switching to Podman . Tip Don't forget to add this rule to your persistent firewall configuration.","title":"Running ContainerSSH"},{"location":"reference/0.4.0/hardening/#securing-authentication","text":"","title":"Securing authentication"},{"location":"reference/0.4.0/hardening/#authentication-server-connection","text":"ContainerSSH talks to an authentication server over HTTP. There are two potential attacks here: If an attacker can intercept the connection between ContainerSSH and the authentication server the attacker can read the passwords for password authentication. If an attacker can send requests to the authentication server they can brute force SSH passwords. Therefore, the connection between ContainerSSH and the authentication server should be secured in the following 3 ways: Implement firewalls such that only ContainerSSH can access the authentication server. Only use HTTPS with certificate verification to access the authentication server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the authentication server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the authentication chapter .","title":"Authentication server connection"},{"location":"reference/0.4.0/hardening/#rate-limiting","text":"ContainerSSH contains no rate limiting for authentication across connections, this is the job of the authentication server. The number of authentication attempts within a connection is limited to 6. The authentication server must take care to do rate limiting right: within a single connection multiple authentication attempts may be made and if the authentication server returns a non-200 response code ContainerSSH will retry connections. It is recommended that the authentication server use the connectionId field to distinguish between SSH connections as this field is guaranteed to be unique for a connection.","title":"Rate limiting"},{"location":"reference/0.4.0/hardening/#client-credential-security","text":"Passwords are vulnerable to being stolen and cannot be transferred to hardware keys. For the most security it is recommended to disable password authentication and only use SSH keys. When storing SSH keys on the client computer they should be protected by a passphrase and limited permissions on the key file. If possible, however, SSH keys should be transferred to a hardware token like the Yubikey . The Yubikey should be configured to require a physical touch on every authentication and should be unlocked with a passcode to prevent unauthorized applications on the client accessing the key for connections.","title":"Client credential security"},{"location":"reference/0.4.0/hardening/#securing-the-configuration-server","text":"ContainerSSH can optionally reach out to a configuration server to fetch dynamic backend configuration based on the username. The backend configuration may contain secrets, such as certificates for accessing Docker and Kubernetes, or application-specific secrets. Therefore, if an attacker can access the configuration server they can extract secrets from the returned configuration. This can be mitigated similar to the authentication server: Implement firewalls such that only ContainerSSH can access the configuration server. Only use HTTPS with certificate verification to access the configuration server and disable the HTTP port. Deploy client certificates to prevent unauthorized access to the configuration server. To maximize security it is recommended that you deploy a custom CA for the server and client certificates. The details of deploying a CA infrastructure with cfssl are described in the configuration server chapter .","title":"Securing the configuration server"},{"location":"reference/0.4.0/hardening/#limiting-ssh-requests","text":"The security module provides the ability to limit which requests are allowed from a client. As ContainerSSH is upgraded the default is to allow new features that will come in with future releases (e.g. TCP port forwarding). In order to secure ContainerSSH for future releases it is recommended to set the defaultMode to disable and enable the modes that you need. For subsystems specifically we recommend filtering and allowing only the sftp subsystem as future ContainerSSH versions may support more subsystems. security : defaultMode : disable env : mode : enable command : mode : enable shell : mode : enable subsystem : mode : filter allow : - sftp tty : mode : enable signal : mode : enable","title":"Limiting SSH requests"},{"location":"reference/0.4.0/hardening/#securing-docker","text":"Docker-specific settings for security are described in the Docker documentation .","title":"Securing Docker"},{"location":"reference/0.4.0/hardening/#securing-kubernetes","text":"Kubernetes-specific settings for security are described in the Kubernetes documentation .","title":"Securing Kubernetes"},{"location":"reference/0.4.0/image/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Building a container image for ContainerSSH ContainerSSH can run any Linux container image. However, it is strongly recommended that you install the ContainerSSH guest agent into the image to make all features available. If you wish to use SFTP you have to add an SFTP server ( apt install openssh-sftp-server on Ubuntu) to the container image and configure the path of the SFTP server correctly in your config.yaml. The sample image containerssh/containerssh-guest-image contains an SFTP server. Integrating the guest agent \u00b6 Using the base image (recommended) This method uses the containerssh/agent container image as part of a multistage build: FROM containerssh/agent AS agent FROM your-base-image COPY --from=agent /usr/bin/containerssh-agent /usr/bin/containerssh-agent # Your other build commands here Installing on Debian/Ubuntu We have an experimental Debian repository containing the agent package. Once you have set up the repository you can install the agent like this: apt-get install containerssh-agent Installing the binaries To use this method go to the latest release from the releases section and verify it against our https://containerssh.io/gpg.txt key ( 3EE5B012FA7B400CD952601E4689F1F0F358FABA ). On an Ubuntu image build this would involve the following steps: ARG AGENT_GPG_FINGERPRINT = 3EE5B012FA7B400CD952601E4689F1F0F358FABA ARG AGENT_GPG_SOURCE = https://containerssh.io/gpg.txt RUN echo \"\\e[1;32mInstalling ContainerSSH guest agent...\\e[0m\" && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' update && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages install gpg && \\ wget -q -O - https://api.github.com/repos/containerssh/agent/releases/latest | grep browser_download_url | grep -e \"agent_.*_linux_amd64.deb\" | awk ' { print $2 } ' | sed -e 's/\"//g' > /tmp/assets.txt && \\ wget -q -O /tmp/agent.deb $( cat /tmp/assets.txt | grep -v .sig ) && \\ wget -q -O /tmp/agent.deb.sig $( cat /tmp/assets.txt | grep .sig ) && \\ wget -q -O - $AGENT_GPG_SOURCE | gpg --import && \\ echo -e \"5\\ny\\n\" | gpg --command-fd 0 --batch --expert --edit-key $AGENT_GPG_FINGERPRINT trust && \\ test $( gpg --status-fd = 1 --verify /tmp/agent.deb.sig /tmp/agent.deb | grep VALIDSIG | grep $AGENT_GPG_FINGERPRINT | wc -l ) -eq 1 && \\ dpkg -i /tmp/agent.deb && \\ rm -rf /tmp/* && \\ rm -rf ~/.gnupg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages remove gpg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -y clean && \\ /usr/bin/containerssh-agent -h Warning The release signing process is experimental and is likely to change in the future. Guest image support is enabled by default in the Docker and Kubernetes backends, but can be disabled as shown below. The KubeRun and DockerRun backends do not support the guest agent. Docker docker : execution : disableAgent : true Kubernetes kubernetes : pod : disableAgent : true","title":"Creating Guest Images"},{"location":"reference/0.4.0/image/#integrating-the-guest-agent","text":"Using the base image (recommended) This method uses the containerssh/agent container image as part of a multistage build: FROM containerssh/agent AS agent FROM your-base-image COPY --from=agent /usr/bin/containerssh-agent /usr/bin/containerssh-agent # Your other build commands here Installing on Debian/Ubuntu We have an experimental Debian repository containing the agent package. Once you have set up the repository you can install the agent like this: apt-get install containerssh-agent Installing the binaries To use this method go to the latest release from the releases section and verify it against our https://containerssh.io/gpg.txt key ( 3EE5B012FA7B400CD952601E4689F1F0F358FABA ). On an Ubuntu image build this would involve the following steps: ARG AGENT_GPG_FINGERPRINT = 3EE5B012FA7B400CD952601E4689F1F0F358FABA ARG AGENT_GPG_SOURCE = https://containerssh.io/gpg.txt RUN echo \"\\e[1;32mInstalling ContainerSSH guest agent...\\e[0m\" && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' update && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages install gpg && \\ wget -q -O - https://api.github.com/repos/containerssh/agent/releases/latest | grep browser_download_url | grep -e \"agent_.*_linux_amd64.deb\" | awk ' { print $2 } ' | sed -e 's/\"//g' > /tmp/assets.txt && \\ wget -q -O /tmp/agent.deb $( cat /tmp/assets.txt | grep -v .sig ) && \\ wget -q -O /tmp/agent.deb.sig $( cat /tmp/assets.txt | grep .sig ) && \\ wget -q -O - $AGENT_GPG_SOURCE | gpg --import && \\ echo -e \"5\\ny\\n\" | gpg --command-fd 0 --batch --expert --edit-key $AGENT_GPG_FINGERPRINT trust && \\ test $( gpg --status-fd = 1 --verify /tmp/agent.deb.sig /tmp/agent.deb | grep VALIDSIG | grep $AGENT_GPG_FINGERPRINT | wc -l ) -eq 1 && \\ dpkg -i /tmp/agent.deb && \\ rm -rf /tmp/* && \\ rm -rf ~/.gnupg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -fuy --allow-downgrades --allow-remove-essential --allow-change-held-packages remove gpg && \\ DEBIAN_FRONTEND = noninteractive apt-get -o Dpkg::Options:: = '--force-confold' -y clean && \\ /usr/bin/containerssh-agent -h Warning The release signing process is experimental and is likely to change in the future. Guest image support is enabled by default in the Docker and Kubernetes backends, but can be disabled as shown below. The KubeRun and DockerRun backends do not support the guest agent. Docker docker : execution : disableAgent : true Kubernetes kubernetes : pod : disableAgent : true","title":"Integrating the guest agent"},{"location":"reference/0.4.0/installation/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Installation Standalone ContainerSSH can be deployed outside of a container. On our downloads page we provide binaries for Linux, Windows, and MacOS. We also provide DEB, RPM and APK (Alpine Linux) packages. Before running ContainerSSH you will need to create a config.yaml file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /path/to/your/host.key auth : url : http://your-auth-server/ Tip You can generate a new host key using openssl genrsa . Please don't use ssh-keygen as it regenerates OpenSSH-specific keys. Tip Details about the authentication server are described in the Authentication section . ContainerSSH can then be started by running ./containerssh --config /path/to/your/config.yaml Docker When deploying in Docker you must first prepare a configuration file that tells ContainerSSH where to find the SSH host key and the authentication server . The minimum configuration file looks like this: ssh : hostkeys : - /var/run/secrets/host.key auth : url : http://your-auth-server/ Tip You can generate a new host key using openssl genrsa Tip Details about the authentication server are described in the Authentication section . You can then run ContainerSSH with the following command line: docker run -d \\ -v /srv/containerssh/config.yaml:/etc/containerssh/config.yaml \\ -v /srv/containerssh/host.key:/var/run/secrets/host.key \\ -p 2222 :2222 \\ containerssh/containerssh:0.4 Kubernetes When running ContainerSSH inside a Kubernetes cluster you must furst create a Secret that contains the host key. openssl genrsa | kubectl create secret generic containerssh-hostkey --from-file = host.key = /dev/stdin Next, you can create a ConfigMap to hold the ContainerSSH configuration: ( cat << EOF ssh: hostkeys: - /etc/containerssh/host.key auth: url: http://your-auth-server/ EOF ) | kubectl create configmap containerssh-config --from-file = config.yaml = /dev/stdin Tip Details about the authentication server are described in the Authentication section . Then you can create a deployment to run ContainerSSH: ( cat << EOF apiVersion: apps/v1 kind: Deployment metadata: name: containerssh labels: app: containerssh spec: replicas: 1 selector: matchLabels: app: containerssh template: metadata: labels: app: containerssh spec: containers: - name: containerssh image: containerssh/containerssh:0.4 ports: - containerPort: 2222 volumeMounts: - name: hostkey mountPath: /etc/containerssh/host.key subPath: host.key readOnly: true - name: config mountPath: /etc/containerssh/config.yaml subPath: config.yaml readOnly: true volumes: - name: hostkey secret: secretName: containerssh-hostkey - name: config configMap: name: containerssh-config EOF ) | kubectl apply -f - Finally, you can create a service to expose the SSH port. You can customize this to create a loadbalancer or nodeport to make SSH publicly available. See kubectl expose --help for details. kubectl expose deployment containerssh \\ --port = 2222 --target-port = 2222 \\ --name = containerssh Note This still does not configure ContainerSSH to use Kubernetes as a container backend. This is described in detail in the Kubernetes backend section .","title":"Installation"},{"location":"reference/0.4.0/kubernetes/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The Kubernetes backend The Kubernetes backend runs and is tested against all currently actively maintained Kubernetes versions . For ContainerSSH version 0.4 these are: 1.20, 1.19, and 1.18. Tip This is the documentation for the Kubernetes backend . For deploying ContainerSSH inside Kubernetes please see the installation guide . The base configuration structure \u00b6 In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kubernetes kubernetes : connection : <connection configuration here> pod : <pod configuration here> timeouts : <timeouts configuration here> Configuring connection parameters \u00b6 In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kubernetes : connection : host : <...> <...> Tip See the Securing Kubernetes section below for a detailed walkthrough for provisioning limited service accounts for ContainerSSH. Base configuration \u00b6 Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. Set to /var/run/secrets/kubernetes.io/serviceaccount/ca.crt when running with a service account. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. qps float32 Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. HTTP basic authentication (username and password) \u00b6 Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) x509 certificate authentication \u00b6 Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format. Bearer token authentication \u00b6 This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount/token to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API. Pod configuration \u00b6 The pod configuration contains the information which pod to run. The structure is very similar to the Pod object in Kubernetes, and we add a few extra options: kubernetes : pod : metadata : <metadata configuration here> spec : <pod spec here> <ContainerSSH-specific options here> Note Do not include the apiVersion , kind , or status types from the Kubernetes structure. Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod , kubectl explain pod.spec , and kubectl explain pod.metadata . Basic pod configuration \u00b6 ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kubernetes : pod : metadata : namespace : default spec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world! Running multiple containers \u00b6 When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kubernetes: pod: consoleContainerNumber: 1 spec: containers: - name: container1 image: ... - name: container2 image: ... Mounting volumes \u00b6 In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage. Forcing the pod to run on a specific node \u00b6 In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kubernetes : pod : spec : nodeName : <insert node name here> Other options \u00b6 Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. Configuration restrictions \u00b6 In connection mode the idleCommand and shellCommand options are required. In session mode the restart policy must be empty or Never . Configuring timeouts \u00b6 The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description podStart The time to wait for the pod to start. podStop The time to wait for the pod to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete. Securing Kubernetes \u00b6 Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment Creating a service account \u00b6 When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh:containerssh Let's test if the permissions are correct: $ kubectl auth can-i create pod --as containerssh no $ kubectl auth can-i create pod --namespace containerssh-guests --as containerssh yes Docker Desktop Docker Desktop Kubernetes contains a cluster role binding called docker-for-desktop-binding that allows all service accounts to perform every action. To secure your Docker Desktop installation you will need to delete this CRB. Deploying inside of Kubernetes \u00b6 When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kubernetes: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token Deploying outside of Kubernetes \u00b6 Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : ZGVmYXVsdA== token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kubernetes : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here> Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kubernetes : pod : spec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kubernetes : pod : spec : securityContext : runAsUser : 1000 Preventing storage exhaustion \u00b6 A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kubernetes : pod : spec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation . Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kubernetes : pod : spec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation . Limiting network access \u00b6 Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Kubernetes"},{"location":"reference/0.4.0/kubernetes/#the-base-configuration-structure","text":"In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kubernetes kubernetes : connection : <connection configuration here> pod : <pod configuration here> timeouts : <timeouts configuration here>","title":"The base configuration structure"},{"location":"reference/0.4.0/kubernetes/#configuring-connection-parameters","text":"In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kubernetes : connection : host : <...> <...> Tip See the Securing Kubernetes section below for a detailed walkthrough for provisioning limited service accounts for ContainerSSH.","title":"Configuring connection parameters"},{"location":"reference/0.4.0/kubernetes/#base-configuration","text":"Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. Set to /var/run/secrets/kubernetes.io/serviceaccount/ca.crt when running with a service account. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. qps float32 Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling.","title":"Base configuration"},{"location":"reference/0.4.0/kubernetes/#http-basic-authentication-username-and-password","text":"Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2)","title":"HTTP basic authentication (username and password)"},{"location":"reference/0.4.0/kubernetes/#x509-certificate-authentication","text":"Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format.","title":"x509 certificate authentication"},{"location":"reference/0.4.0/kubernetes/#bearer-token-authentication","text":"This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount/token to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API.","title":"Bearer token authentication"},{"location":"reference/0.4.0/kubernetes/#pod-configuration","text":"The pod configuration contains the information which pod to run. The structure is very similar to the Pod object in Kubernetes, and we add a few extra options: kubernetes : pod : metadata : <metadata configuration here> spec : <pod spec here> <ContainerSSH-specific options here> Note Do not include the apiVersion , kind , or status types from the Kubernetes structure. Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod , kubectl explain pod.spec , and kubectl explain pod.metadata .","title":"Pod configuration"},{"location":"reference/0.4.0/kubernetes/#basic-pod-configuration","text":"ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kubernetes : pod : metadata : namespace : default spec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world!","title":"Basic pod configuration"},{"location":"reference/0.4.0/kubernetes/#running-multiple-containers","text":"When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kubernetes: pod: consoleContainerNumber: 1 spec: containers: - name: container1 image: ... - name: container2 image: ...","title":"Running multiple containers"},{"location":"reference/0.4.0/kubernetes/#mounting-volumes","text":"In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kubernetes : pod : consoleContainerNumber : 1 spec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage.","title":"Mounting volumes"},{"location":"reference/0.4.0/kubernetes/#forcing-the-pod-to-run-on-a-specific-node","text":"In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kubernetes : pod : spec : nodeName : <insert node name here>","title":"Forcing the pod to run on a specific node"},{"location":"reference/0.4.0/kubernetes/#other-options","text":"Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. disableAgent bool Disable the ContainerSSH guest agent. This will disable several functions and is not recommended . subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it.","title":"Other options"},{"location":"reference/0.4.0/kubernetes/#configuration-restrictions","text":"In connection mode the idleCommand and shellCommand options are required. In session mode the restart policy must be empty or Never .","title":"Configuration restrictions"},{"location":"reference/0.4.0/kubernetes/#configuring-timeouts","text":"The timeouts section has the following options. All options can use time units (e.g. 60s ) and default to nanoseconds without time units. Name Description podStart The time to wait for the pod to start. podStop The time to wait for the pod to stop. commandStart The time to wait for the command to start in connection mode. signal The time to wait to deliver a signal to a process. window The time to wait to deliver a window size change. http The time to wait for the underlying HTTP calls to complete.","title":"Configuring timeouts"},{"location":"reference/0.4.0/kubernetes/#securing-kubernetes","text":"Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment","title":"Securing Kubernetes"},{"location":"reference/0.4.0/kubernetes/#creating-a-service-account","text":"When deploying ContainerSSH with a Kubernetes backend you should never an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh:containerssh Let's test if the permissions are correct: $ kubectl auth can-i create pod --as containerssh no $ kubectl auth can-i create pod --namespace containerssh-guests --as containerssh yes Docker Desktop Docker Desktop Kubernetes contains a cluster role binding called docker-for-desktop-binding that allows all service accounts to perform every action. To secure your Docker Desktop installation you will need to delete this CRB.","title":"Creating a service account"},{"location":"reference/0.4.0/kubernetes/#deploying-inside-of-kubernetes","text":"When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kubernetes: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token","title":"Deploying inside of Kubernetes"},{"location":"reference/0.4.0/kubernetes/#deploying-outside-of-kubernetes","text":"Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : ZGVmYXVsdA== token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kubernetes : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here>","title":"Deploying outside of Kubernetes"},{"location":"reference/0.4.0/kubernetes/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kubernetes : pod : spec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kubernetes : pod : spec : securityContext : runAsUser : 1000","title":"Preventing root escalation"},{"location":"reference/0.4.0/kubernetes/#preventing-storage-exhaustion","text":"A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk.","title":"Preventing storage exhaustion"},{"location":"reference/0.4.0/kubernetes/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kubernetes : pod : spec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing memory exhaustion"},{"location":"reference/0.4.0/kubernetes/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kubernetes : pod : spec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing CPU exhaustion"},{"location":"reference/0.4.0/kubernetes/#limiting-network-access","text":"Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Limiting network access"},{"location":"reference/0.4.0/kuberun/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The KubeRun backend Warning The KubeRun backend is deprecated and the Kubernetes backend should be used instead. Read the migration guide here \u00bb The KubeRun backend runs and is tested against all currently actively maintained Kubernetes versions . For ContainerSSH version 0.4 these are: 1.20, 1.19, and 1.18. Tip This is the documentation for the KubeRun backend . For deploying ContainerSSH inside Kubernetes please see the installation guide . The base configuration structure \u00b6 In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kuberun kuberun : connection : <connection configuration here> pod : <pod configuration here> Configuring connection parameters \u00b6 In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kuberun : connection : host : <...> <...> Base configuration \u00b6 Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. insecure bool Disable certificate verification on the Kubernetes API. This is a very bad idea as anyone on the network will be able to intercept your credentials. qps float32` Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. timeout string Timeout for pod operations in nanoseconds. Time units can be used. HTTP basic authentication (username and password) \u00b6 Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) x509 certificate authentication \u00b6 Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format. Bearer token authentication \u00b6 This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API. Pod configuration \u00b6 The pod configuration contains the information which pod to run. kuberun : pod : namespace : <namespace name> podSpec : <pod spec here> <ContainerSSH-specific options here> Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod.spec . Basic pod configuration \u00b6 ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kuberun : pod : namespace : default podSpec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world! Running multiple containers \u00b6 When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kuberun: pod: namespace: default consoleContainerNumber: 1 podSpec: containers: - name: container1 image: ... - name: container2 image: ... Mounting volumes \u00b6 In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage. Forcing the pod to run on a specific node \u00b6 In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kuberun : pod : podSpec : nodeName : <insert node name here> Other options \u00b6 Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. enableAgent bool Enable the ContainerSSH guest agent. This enables the ContainerSSH guest agent. subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution. Securing Kubernetes \u00b6 Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment Creating a service account \u00b6 When deploying ContainerSSH with a Kubernetes backend you should never use an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace. First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh Deploying inside of Kubernetes \u00b6 When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kuberun: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token Deploying outside of Kubernetes \u00b6 Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : <base64-encoded namespace here> token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kuberun : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here> Preventing root escalation \u00b6 Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kuberun : pod : podSpec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kuberun : pod : podSpec : securityContext : runAsUser : 1000 Preventing storage exhaustion \u00b6 A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk. Preventing memory exhaustion \u00b6 Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kuberun : pod : podSpec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation . Preventing CPU exhaustion \u00b6 A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kuberun : pod : podSpec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation . Limiting network access \u00b6 Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"KubeRun (deprecated)"},{"location":"reference/0.4.0/kuberun/#the-base-configuration-structure","text":"In order to use the Kubernetes backend you must specify the following configuration entries via the configuration file or the configuration server: backend : kuberun kuberun : connection : <connection configuration here> pod : <pod configuration here>","title":"The base configuration structure"},{"location":"reference/0.4.0/kuberun/#configuring-connection-parameters","text":"In order to use Kubernetes you must provide the credentials to authenticate with the Kubernetes cluster. There are several supported authentication methods: Username and password (HTTP basic auth). x509 client certificates. Bearer token. These options should be specified like this: kuberun : connection : host : <...> <...>","title":"Configuring connection parameters"},{"location":"reference/0.4.0/kuberun/#base-configuration","text":"Name Type Description host string The hostname or ip + the port of the Kubernetes API server. Set this to kubernetes.default.svc to run inside a Kubernetes cluster, otherwise set it to the host name of your Kubernetes API. path string This is the API path of the Kubernetes API. Defaults to /api and you will typically not need to change this. cacertFile string Points to the file that contains the CA certificate in PEM format that signed the server certificate. cacert string Directly contains the CA certificate in PEM format that signed the server certificate. serverName string Sets the hostname of the server that should be sent to the Kuberentes API in the TLS SNI. This is useful when the Kubernetes API has a hostname that is not resolvable from the server ContainerSSH is running on. insecure bool Disable certificate verification on the Kubernetes API. This is a very bad idea as anyone on the network will be able to intercept your credentials. qps float32` Indicates a maximum queries per second from this client. burst int Indicates the maximum burst for query throttling. timeout string Timeout for pod operations in nanoseconds. Time units can be used.","title":"Base configuration"},{"location":"reference/0.4.0/kuberun/#http-basic-authentication-username-and-password","text":"Name Type Description username string Username for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2) password string Password for authenticating against the Kubernetes API. This is only used for HTTP basic auth and does not work with other authentication methods (e.g. OAuth2)","title":"HTTP basic authentication (username and password)"},{"location":"reference/0.4.0/kuberun/#x509-certificate-authentication","text":"Name Type Description certFile string Points to a file that contains the client certificate for x509 authentication against the Kubernetes API in PEM format. cert string Directly contains the certificate for x509 authentication against the Kubernetes API in PEM format. keyFile string Points to a file that contains the client key for x509 authentication against the Kubernetes API in PEM format. key string Directly contains the client key for x509 authentication against the Kubernetes API in PEM format.","title":"x509 certificate authentication"},{"location":"reference/0.4.0/kuberun/#bearer-token-authentication","text":"This authentication method is primarily used with service accounts . Name Type Description bearerTokenFile string Points to the file that contains the bearer token for authenticating against the Kubernetes API. Set to /var/run/secrets/kubernetes.io/serviceaccount to use the service account when running ContainerSSH inside a Kubernetes cluster. bearerToken string Directly contains the bearer token for authenticating against the Kubernetes API.","title":"Bearer token authentication"},{"location":"reference/0.4.0/kuberun/#pod-configuration","text":"The pod configuration contains the information which pod to run. kuberun : pod : namespace : <namespace name> podSpec : <pod spec here> <ContainerSSH-specific options here> Tip Did you know? You can get a full description of the Pod type by running kubectl explain pod.spec .","title":"Pod configuration"},{"location":"reference/0.4.0/kuberun/#basic-pod-configuration","text":"ContainerSSH defaults to running pods in the default namespace with the containerssh/containerssh-guest-image container image. You can change these settings with the following options: kuberun : pod : namespace : default podSpec : containers : - name : shell image : containerssh/containerssh-guest-image env : - name : VAR value : Hello world!","title":"Basic pod configuration"},{"location":"reference/0.4.0/kuberun/#running-multiple-containers","text":"When running multiple containers ContainerSSH defaults to attaching to the first container. You can change this behavior by specifying the consoleContainerNumber option. This number is 0-indexed. kuberun: pod: namespace: default consoleContainerNumber: 1 podSpec: containers: - name: container1 image: ... - name: container2 image: ...","title":"Running multiple containers"},{"location":"reference/0.4.0/kuberun/#mounting-volumes","text":"In Kubernetes volumes of various types can be mounted into pods. This is done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : <volume name here> <mount type here> : <mount options here> containers : - name : shell image : <image name here> volumeMounts : - name : <volume name here> mountPath : <where to mount> For example, mounting a path from the host machine can be done as follows: kuberun : pod : consoleContainerNumber : 1 podSpec : volumes : - name : home hostPath : path : /home/ubuntu type : Directory containers : - name : shell image : containerssh/containerssh-guest-image volumeMounts : - name : home mountPath : /home/ubuntu Tip Use kubectl explain pod.spec.volumes for details on how to configure the volume driver for your storage.","title":"Mounting volumes"},{"location":"reference/0.4.0/kuberun/#forcing-the-pod-to-run-on-a-specific-node","text":"In Kubernetes pod scheduling can be influenced either by node affinity or by explicitly binding a pod to a node . Node affinity lets you schedule pods based on various features of the node, e.g. the presence of a GPU, a disk type, etc. As the configuration can be quite complex we will not discuss it here, please read the Kubernetes manual . Binding a pod to a specific node on the other hand is rather simple: kuberun : pod : podSpec : nodeName : <insert node name here>","title":"Forcing the pod to run on a specific node"},{"location":"reference/0.4.0/kuberun/#other-options","text":"Apart from the metadata and spec options ContainerSSH has the following options on a Pod-level. These should not be changed unless required. Name Type Description consoleContainerNumber uint Specifies the number of the container to attach to. Defaults to the first container. mode string Specify connection to launch one pod per SSH connection or session to run one pod per SSH session (multiple pods per connection). In connection mode the container is started with the idleCommand as the first program and every session is launched similar to how kubectl exec runs programs. In session mode the command is launched directly. idleCommand []string Specifies the command to run as the first process in the container in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. shellCommand []string Specifies the command to run as a shell in connection mode. Parameters must be provided as separate items in the array. Has no effect in session mode. agentPath string Contains the full path to the ContainerSSH guest agent inside the shell container. The agent must be installed in the guest image. enableAgent bool Enable the ContainerSSH guest agent. This enables the ContainerSSH guest agent. subsystems map[string]string Specifies a map of subsystem names to executables. It is recommended to set at least the sftp subsystem as many users will want to use it. disableCommand bool Disable command execution.","title":"Other options"},{"location":"reference/0.4.0/kuberun/#securing-kubernetes","text":"Securing the Kubernetes installation is beyond the scope of this document. We will describe how to deploy and configure ContainerSSH for security in a Kubernetes environment","title":"Securing Kubernetes"},{"location":"reference/0.4.0/kuberun/#creating-a-service-account","text":"When deploying ContainerSSH with a Kubernetes backend you should never use an admin account for interacting with a Kubernetes cluster. ContainerSSH can run inside the same Kubernetes cluster or it can run as a standalone. When deploying inside the same Kubernetes cluster it is strongly recommended that ContainerSSH runs in a different namespace as the guest pods ContainerSSH launches. The setup below assumes you are creating a service account in the default namespace and the ContainerSSH pods will run in the containerssh-guests namespace. First, we need to create the service account. The following fragment can be applied with kubectl apply -f : apiVersion : v1 kind : ServiceAccount metadata : name : containerssh automountServiceAccountToken : false Then we create the role and rolebinding resources in the containerssh-guests namespace to allow the service accounts to create pods: kubectl create role containerssh \\ -n containerssh-guests \\ --verb = * \\ --resource = pods \\ --resource = pods/logs \\ --resource = pods/exec kubectl create rolebinding containerssh \\ -n containerssh-guests \\ --serviceaccount = containerssh","title":"Creating a service account"},{"location":"reference/0.4.0/kuberun/#deploying-inside-of-kubernetes","text":"When deploying ContainerSSH inside the same Kubernetes cluster you can simply use the service account when making your deployment: kuberun: connection: host: ... cacertFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token","title":"Deploying inside of Kubernetes"},{"location":"reference/0.4.0/kuberun/#deploying-outside-of-kubernetes","text":"Now, if you are running ContainerSSH outside of the Kubernetes cluster you can fetch the secret belonging to the service account by first looking at the service account itself: kubectl describe serviceaccount containerssh This command will output the name of the secret for this service account, which can then be extracted: kubectl get secret containerssh-token-2jrnc -o yaml The output will look as follows: apiVersion : v1 data : ca.crt : <base64-encoded CA certificate here> namespace : <base64-encoded namespace here> token : <base64-encoded bearer token here> kind : Secret Base64-decode both the ca.crt and the token fields and insert them into your ContainerSSH config as follows: kuberun : connection : bearerToken : <insert token here> cacert : | <insert ca.crt here>","title":"Deploying outside of Kubernetes"},{"location":"reference/0.4.0/kuberun/#preventing-root-escalation","text":"Under normal circumstances a user running as root inside a container cannot access resources outside the container. However, in the event of a container escape vulnerability in Kubernetes it is prudent not to run container workloads as root. You can prevent forcibly prevent any container from running as root by configuring the following setting: kuberun : pod : podSpec : securityContext : runAsNonRoot : true However, this will fail starting any container image that wants to run as root. In addition to the option above, you can also force the container to a specific UID: kuberun : pod : podSpec : securityContext : runAsUser : 1000","title":"Preventing root escalation"},{"location":"reference/0.4.0/kuberun/#preventing-storage-exhaustion","text":"A malicious user could cause the Kubernetes host to run out of disk space with a simple attack: cat /dev/zero > ~/zerofill There are two cases here: If the directory the user can write to is mounted using a volume the attacker can fill up the storage that is mounted. You can pass per-user mounts from the configuration server that mount volumes that are unique to the connecting user. This way the user can only fill up their own disk. The specific implementation depends on your volume driver. If the directory the user can write to is not mounted the user can fill up the container image. This is a much more subtle way of filling up the disk. Current Kubernetes does not support preventing this kind of attack, so it is recommended to only allow users to write to paths mounted as volumes. The readOnlyRootFilesystem PodSecurityPolicy can be applied to the namespace or the whole cluster preventing writes to the container root filesystem filling up the disk.","title":"Preventing storage exhaustion"},{"location":"reference/0.4.0/kuberun/#preventing-memory-exhaustion","text":"Users can also try to exhaust the available memory to potentially crash the server. This can be prevented using the following configuration: kuberun : pod : podSpec : resources : limits : memory : \"128Mi\" You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing memory exhaustion"},{"location":"reference/0.4.0/kuberun/#preventing-cpu-exhaustion","text":"A malicious user can also exhaust the CPU by running CPU-heavy workloads. This can be prevented by setting the following options: kuberun : pod : podSpec : resources : limits : cpu : \"500m\" In this setting 1000m corresponds to a full core or vCPU of the host. You can read more about memory requests and limits in the Kubernetes documentation .","title":"Preventing CPU exhaustion"},{"location":"reference/0.4.0/kuberun/#limiting-network-access","text":"Depending on which Container Network Interface is installed on the Kubernetes cluster you may have access to Network Policies . These work very similar to how traditional Linux firewalling works. The following network policy disables all network access within a namespace: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : containerssh-guest-policy namespace : containerssh-guests spec : podSelector : {} policyTypes : - Ingress - Egress ingress : [] egress : []","title":"Limiting network access"},{"location":"reference/0.4.0/logging/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Logging ContainerSSH comes with configurable logging facilities. You can configure the following options: The minimum log level to filter unnecessary log messages The log format The log destination Configuring the minimum log level \u00b6 You can configure the minimum log level by adding the following to your configuration: log : level : \"info\" Tip You can configure the log level on a per-user basis using the configuration server . The supported levels are in accordance with the Syslog standard: debug info notice warning error crit alert emerg Configuring the log format \u00b6 ContainerSSH can log in both text and newline-delimited JSON ( ljson ) format. You can change the format with the following setting: log : format : \"ljson\" The JSON log format \u00b6 The LJSON format outputs a JSON-formatted string per log message. For file and stdout these JSON messages are separated by a newline from each other. When writing to the stdout or file destinations the format is the following: { \"timestamp\" : \"Timestamp in RFC3339 format\" , \"level\" : \"the log level\" , \"code\" : \"ERROR_CODE_HERE\" , \"message\" : \"the message (optional)\" , \"details\" : { \"the detail object if any (optional)\" } } When writing to syslog the format is the same, but does not contain the timestamp and level fields as they are redundant. The error codes are documented in the troubleshooting section . The text log format \u00b6 When writing to the stdout or file destinations the text log format has the following fields delimited by a tab ( \\t ) character: TIMESTAMP\\tLEVEL\\tMESSAGE The TIMESTAMP will be formatted according to RFC3339, while the LEVEL will be the text-representation of the log level. The MESSAGE field will contain the text representation of the message. Warning The text message is not intended for machine processing and may change across versions. If you intend to do machine processing please use the details field from the ljson format. Setting the output \u00b6 The output configuration of ContainerSSH is the following: log : destination : \"stdout|file|syslog\" <other destination options> Writing to the stdout \u00b6 The stdout destination is the simplest: it will write error messages to the standard output of ContainerSSH. This is the default logging method. log : destination : \"stdout\" Writing to a file \u00b6 You can write to a specific file using the following options: log : destination : \"file\" file : \"/var/log/containerssh/containerssh.log\" ContainerSSH will write to the specified file in the specified format. Tip ContainerSSH supports log rotation . You can trigger a log rotation by sending a HUP (hangup) signal to ContainerSSH. Writing to syslog \u00b6 ContainerSSH supports writing to syslog via a Unix socket or UDP. TCP syslog is not supported due to the complexity of maintaining a stable connection and buffering messages between disconnects. Syslog can be configured as follows: log : destination : syslog syslog : # Change to IP and port for UDP destination : /dev/log # See below for supported names facility : auth # Program name to log as tag : ContainerSSH # Log PID with program name pid : false The following facilities are supported: kern user mail daemon auth syslog lpr news uucp cron authpriv ftp ntp logaudit logalert clock local0..7 Warning ContainerSSH can log very long messages with lots of details. Please make sure to bump your maximum line length to at least 4096 characters in your Syslog server to avoid message truncation. ( rsyslog and syslog-ng have higher default values, so you don't need to change the configuration if you are using these syslog servers.)","title":"Logging"},{"location":"reference/0.4.0/logging/#configuring-the-minimum-log-level","text":"You can configure the minimum log level by adding the following to your configuration: log : level : \"info\" Tip You can configure the log level on a per-user basis using the configuration server . The supported levels are in accordance with the Syslog standard: debug info notice warning error crit alert emerg","title":"Configuring the minimum log level"},{"location":"reference/0.4.0/logging/#configuring-the-log-format","text":"ContainerSSH can log in both text and newline-delimited JSON ( ljson ) format. You can change the format with the following setting: log : format : \"ljson\"","title":"Configuring the log format"},{"location":"reference/0.4.0/logging/#the-json-log-format","text":"The LJSON format outputs a JSON-formatted string per log message. For file and stdout these JSON messages are separated by a newline from each other. When writing to the stdout or file destinations the format is the following: { \"timestamp\" : \"Timestamp in RFC3339 format\" , \"level\" : \"the log level\" , \"code\" : \"ERROR_CODE_HERE\" , \"message\" : \"the message (optional)\" , \"details\" : { \"the detail object if any (optional)\" } } When writing to syslog the format is the same, but does not contain the timestamp and level fields as they are redundant. The error codes are documented in the troubleshooting section .","title":"The JSON log format"},{"location":"reference/0.4.0/logging/#the-text-log-format","text":"When writing to the stdout or file destinations the text log format has the following fields delimited by a tab ( \\t ) character: TIMESTAMP\\tLEVEL\\tMESSAGE The TIMESTAMP will be formatted according to RFC3339, while the LEVEL will be the text-representation of the log level. The MESSAGE field will contain the text representation of the message. Warning The text message is not intended for machine processing and may change across versions. If you intend to do machine processing please use the details field from the ljson format.","title":"The text log format"},{"location":"reference/0.4.0/logging/#setting-the-output","text":"The output configuration of ContainerSSH is the following: log : destination : \"stdout|file|syslog\" <other destination options>","title":"Setting the output"},{"location":"reference/0.4.0/logging/#writing-to-the-stdout","text":"The stdout destination is the simplest: it will write error messages to the standard output of ContainerSSH. This is the default logging method. log : destination : \"stdout\"","title":"Writing to the stdout"},{"location":"reference/0.4.0/logging/#writing-to-a-file","text":"You can write to a specific file using the following options: log : destination : \"file\" file : \"/var/log/containerssh/containerssh.log\" ContainerSSH will write to the specified file in the specified format. Tip ContainerSSH supports log rotation . You can trigger a log rotation by sending a HUP (hangup) signal to ContainerSSH.","title":"Writing to a file"},{"location":"reference/0.4.0/logging/#writing-to-syslog","text":"ContainerSSH supports writing to syslog via a Unix socket or UDP. TCP syslog is not supported due to the complexity of maintaining a stable connection and buffering messages between disconnects. Syslog can be configured as follows: log : destination : syslog syslog : # Change to IP and port for UDP destination : /dev/log # See below for supported names facility : auth # Program name to log as tag : ContainerSSH # Log PID with program name pid : false The following facilities are supported: kern user mail daemon auth syslog lpr news uucp cron authpriv ftp ntp logaudit logalert clock local0..7 Warning ContainerSSH can log very long messages with lots of details. Please make sure to bump your maximum line length to at least 4096 characters in your Syslog server to avoid message truncation. ( rsyslog and syslog-ng have higher default values, so you don't need to change the configuration if you are using these syslog servers.)","title":"Writing to syslog"},{"location":"reference/0.4.0/metrics/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Metrics ContainerSSH contains a Prometheus -compatible metrics server which can be enabled using the following configuration: metrics : <options here> The metrics server has the following options: Option Type Description enable bool Enable metrics server. Defaults to false. path string HTTP path to serve metrics on. Defaults to /metrics . listen string IP and port to listen on. Defaults to 0.0.0.0:9100 . clientcacert string CA certificate in PEM format or filename that contains the CA certificate used for authenticating connecting clients. cert string Client certificate in PEM format or filename that contains the server certificate. key string Private key in PEM format or filename that contains the server certificate. tlsVersion []string Minimum TLS version to support. See the TLS version section below. curve []string Elliptic curve algorithms to support. See the Elliptic curve algorithms section below. cipher []string,string Which cipher suites to support. See the Cipher suites section below. Available metrics \u00b6 You can configure Prometheus to grab the following metrics: containerssh_auth_server_failures Number of failed requests to the authentication server since start. containerssh_auth_success Number of successful authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_auth_failures Number of failed authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_config_server_failures Number of failed requests to the configuration server since start. containerssh_ssh_connections Number of SSH connections since start. Contains a label for country (see below). containerssh_ssh_handshake_successful Number of successful SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_handshake_failed Number of failed SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_current_connections Number of currently open SSH connections. Contains a label for country (see below). Country identification \u00b6 Country identification works using GeoIP2 or GeoLite2 from MaxMind . This database needs to be provided to ContainerSSH externally due to licensing concerns. The default path for the GeoIP database is /var/lib/GeoIP/GeoIP2-Country.mmdb , but you can change that using the following configuration snippet: geoip : provider : \"maxmind\" maxmind-geoip2-file : '/var/lib/GeoIP/GeoIP2-Country.mmdb' Configuring TLS \u00b6 TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure. TLS version \u00b6 The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported. Elliptic curve algorithms \u00b6 The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1 Cipher suites \u00b6 The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 Client authentication \u00b6 In order to safeguard the metrics ContainerSSH supports authenticating connecting clients using x509 mutual TLS authentication. For this you will need to generate a CA certificate and configure the metrics service with it, as well as client certificates that your connecting clients must use. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added to the metrics configuration: metrics : clientcacert : /path/to/ca.pem Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem can be used in your connecting client. For an example see the Prometheus documentation .","title":"Metrics"},{"location":"reference/0.4.0/metrics/#available-metrics","text":"You can configure Prometheus to grab the following metrics: containerssh_auth_server_failures Number of failed requests to the authentication server since start. containerssh_auth_success Number of successful authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_auth_failures Number of failed authentications since start. Contains labels for authtype ( password or pubkey ) and country (see below). containerssh_config_server_failures Number of failed requests to the configuration server since start. containerssh_ssh_connections Number of SSH connections since start. Contains a label for country (see below). containerssh_ssh_handshake_successful Number of successful SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_handshake_failed Number of failed SSH handshakes since start. Contains a label for country (see below). containerssh_ssh_current_connections Number of currently open SSH connections. Contains a label for country (see below).","title":"Available metrics"},{"location":"reference/0.4.0/metrics/#country-identification","text":"Country identification works using GeoIP2 or GeoLite2 from MaxMind . This database needs to be provided to ContainerSSH externally due to licensing concerns. The default path for the GeoIP database is /var/lib/GeoIP/GeoIP2-Country.mmdb , but you can change that using the following configuration snippet: geoip : provider : \"maxmind\" maxmind-geoip2-file : '/var/lib/GeoIP/GeoIP2-Country.mmdb'","title":"Country identification"},{"location":"reference/0.4.0/metrics/#configuring-tls","text":"TLS ensures that the connection between ContainerSSH and the configuration server cannot be intercepted using a Man-in-the-Mittle attack. We recommend checking the Mozilla Wiki for information about which configuration can be considered secure.","title":"Configuring TLS"},{"location":"reference/0.4.0/metrics/#tls-version","text":"The minimum supported TLS version can be configured using the tlsVersion option. It defaults to 1.3 and also supports 1.2 . Versions lower than 1.2 are not supported.","title":"TLS version"},{"location":"reference/0.4.0/metrics/#elliptic-curve-algorithms","text":"The elliptic curve algorithms can be specified in the curve option. We support and default to the following options: x25519 secp256r1 secp384r1 secp521r1","title":"Elliptic curve algorithms"},{"location":"reference/0.4.0/metrics/#cipher-suites","text":"The following cipher suites are supported in ContainerSSH: Suite Default TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305","title":"Cipher suites"},{"location":"reference/0.4.0/metrics/#client-authentication","text":"In order to safeguard the metrics ContainerSSH supports authenticating connecting clients using x509 mutual TLS authentication. For this you will need to generate a CA certificate and configure the metrics service with it, as well as client certificates that your connecting clients must use. We recommend using cfssl for creating the CA infrastructure. First we need to create the CA certificates: cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"containerssh\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"ContainerSSH CA\", \"key\": { \"algo\": \"rsa\", \"size\": 4096 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca The resulting ca.pem should be added to the metrics configuration: metrics : clientcacert : /path/to/ca.pem Then we can create the client certificate: cat > containerssh-csr.json <<EOF { \"CN\": \"ContainerSSH\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"Your Country Code\", \"L\": \"Your Locality\", \"O\": \"Your Company\", \"OU\": \"\", \"ST\": \"Your State\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = containerssh \\ containerssh-csr.json | cfssljson -bare containerssh The resulting containerssh.pem and containerssh-key.pem can be used in your connecting client. For an example see the Prometheus documentation .","title":"Client authentication"},{"location":"reference/0.4.0/security/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Security configuration The security module provides the ability to limit or force the behavior of SSH. It can be configured using the following structure: security : maxSessions : 10 forceCommand : \"/run/this/command\" defaultMode : enable|filter|disable env : mode : enable|filter|disable allow : - ENV_VARIABLE_NAME deny : - ENV_VARIABLE_NAME command : mode : enable|filter|disable allow : - /allow/this/command shell : mode : enable|disable subsystem : mode : enable|filter|disable allow : - <enable-this-subsystem> deny : - <disable-this-subsystem> tty : mode : enable|disable signal : mode : enable|filter|disable allow : - TERM deny : - KILL Maximum sessions \u00b6 The maxSessions option lets you limit the number of parallel sessions a client can open. When this number of sessions is reached further session requests are rejected until a session is closed. The recommended value for this option is 10 . Forcing commands \u00b6 The forceCommand option lets you force the execution of a command even when the client has specified a different command to be run. This turns all shell and subsystem requests into command execution requests to run the specified command. The original command will be available in the SSH_ORIGINAL_COMMAND environment variable. Filtering requests \u00b6 SSH allows a client to request a multitude of things. The security module allows you to either enable, filter, or deny requests. allow will allow all requests except the ones specified in the deny list. filter will only allow requests specified in the allow list. deny denies all requests. You can configure the settings either individually, or using the defaultMode setting. It is strongly recommended to set a default mode so future ContainerSSH versions adding new features don't accidentally allow something you don't want to enable. Environment variable filtering \u00b6 Using the env option you can filter which environment variables the client can set. In enable mode you can deny specific environment variables by specifying disallowed variables in the deny list. In filter mode you can specify allowed variables in the allow list. If you want to completely disable setting environment variables you can set the mode to disable . Command execution \u00b6 A client can explicitly request running a specific command by specifying it in the command line: ssh yourserver.com run-this-program In enable mode command execution is enabled with no filtering. There is no deny option as it is trivially easy to work around a simple matching. In filter mode only the commands that are specified in the allow list are executed. The match must be exact. In deny mode all command execution is disabled. Shell execution \u00b6 When not specifying a command to the SSH client by default a shell is launched. You can only set the enable and disable modes. The filter mode is valid, but is equal to the disable mode. Subsystem execution \u00b6 SSH clients can also execute well-known subsystems, such as sftp . The server then decides which binary to execute for the requested subsystem. When set to enable all subsystems except the ones in the deny list. In filter mode only subsystems in the allow list are allowed. In deny mode no subsystem execution is allowed. TTY/PTY requests \u00b6 When a client wants to use the SSH server interactively they can send a PTY request to the server before executing the program. The only security options for TTY are enable and disable . filter mode is not explicitly invalid, but behaves like deny . Signals \u00b6 Although not used very often, SSH clients can request signals to be delivered to the running program. In enable mode all signals except for the ones listed in the deny list are allowed. In filter mode only the signals in the allow list are allowed. In disable mode no signal delivery is allowed. Warning Signal names have to be specified without the SIG prefix. ContainerSSH supports the following signals: ABRT ALRM FPE HUP ILL INT KILL PIPE QUIT SEGV TERM USR1 USR2","title":"Restrictions"},{"location":"reference/0.4.0/security/#maximum-sessions","text":"The maxSessions option lets you limit the number of parallel sessions a client can open. When this number of sessions is reached further session requests are rejected until a session is closed. The recommended value for this option is 10 .","title":"Maximum sessions"},{"location":"reference/0.4.0/security/#forcing-commands","text":"The forceCommand option lets you force the execution of a command even when the client has specified a different command to be run. This turns all shell and subsystem requests into command execution requests to run the specified command. The original command will be available in the SSH_ORIGINAL_COMMAND environment variable.","title":"Forcing commands"},{"location":"reference/0.4.0/security/#filtering-requests","text":"SSH allows a client to request a multitude of things. The security module allows you to either enable, filter, or deny requests. allow will allow all requests except the ones specified in the deny list. filter will only allow requests specified in the allow list. deny denies all requests. You can configure the settings either individually, or using the defaultMode setting. It is strongly recommended to set a default mode so future ContainerSSH versions adding new features don't accidentally allow something you don't want to enable.","title":"Filtering requests"},{"location":"reference/0.4.0/security/#environment-variable-filtering","text":"Using the env option you can filter which environment variables the client can set. In enable mode you can deny specific environment variables by specifying disallowed variables in the deny list. In filter mode you can specify allowed variables in the allow list. If you want to completely disable setting environment variables you can set the mode to disable .","title":"Environment variable filtering"},{"location":"reference/0.4.0/security/#command-execution","text":"A client can explicitly request running a specific command by specifying it in the command line: ssh yourserver.com run-this-program In enable mode command execution is enabled with no filtering. There is no deny option as it is trivially easy to work around a simple matching. In filter mode only the commands that are specified in the allow list are executed. The match must be exact. In deny mode all command execution is disabled.","title":"Command execution"},{"location":"reference/0.4.0/security/#shell-execution","text":"When not specifying a command to the SSH client by default a shell is launched. You can only set the enable and disable modes. The filter mode is valid, but is equal to the disable mode.","title":"Shell execution"},{"location":"reference/0.4.0/security/#subsystem-execution","text":"SSH clients can also execute well-known subsystems, such as sftp . The server then decides which binary to execute for the requested subsystem. When set to enable all subsystems except the ones in the deny list. In filter mode only subsystems in the allow list are allowed. In deny mode no subsystem execution is allowed.","title":"Subsystem execution"},{"location":"reference/0.4.0/security/#ttypty-requests","text":"When a client wants to use the SSH server interactively they can send a PTY request to the server before executing the program. The only security options for TTY are enable and disable . filter mode is not explicitly invalid, but behaves like deny .","title":"TTY/PTY requests"},{"location":"reference/0.4.0/security/#signals","text":"Although not used very often, SSH clients can request signals to be delivered to the running program. In enable mode all signals except for the ones listed in the deny list are allowed. In filter mode only the signals in the allow list are allowed. In disable mode no signal delivery is allowed. Warning Signal names have to be specified without the SIG prefix. ContainerSSH supports the following signals: ABRT ALRM FPE HUP ILL INT KILL PIPE QUIT SEGV TERM USR1 USR2","title":"Signals"},{"location":"reference/0.4.0/ssh/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb SSH configuration SSH is the main service of ContainerSSH. It has the following configuration structure: ssh : <options> The options are as follows: Name Type Description listen string IP and port pair to bind the SSH service to. Defaults to 0.0.0.0:2222 serverVersion string Server version string presented to any connecting client. Must start with SSH-2.0- . Defaults to SSH-2.0-ContainerSSH . cipher []string List of ciphers the server should support. See the Ciphers section below. kex []string List of key exchange algorithms the server should support. See the Key exchange section below. macs []string List of MAC algorithms the server should support. See the MAC section below. banner []string The banner text to presented to any connecting client. hostkeys []string List of host keys in PEM format, or file names to read the key from. Generate with openssl genrsa Configuring the server version \u00b6 The SSH server version is presented to any connecting client in plain text upon connection. It has the following format: SSH-2.0-softwareversion <SP> comments The softwareversion can only contain printable US-ASCII characters without whitespace and minus ( - ) signs. The comments field is optional and is separated from the softwareversion with a single space. The maximum length of the version string is 255 characters. Configuring a banner \u00b6 SSH offers the ability to output a message to the clients before they enter passwords. This can be configured in the banner option. The banner can contain multiple lines. Ciphers \u00b6 ContainerSSH supports the following ciphers. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID Key exchange \u00b6 ContainerSSH supports the following key exchange algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1 MAC \u00b6 ContainerSSH supports the following MAC algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96","title":"Configuration"},{"location":"reference/0.4.0/ssh/#configuring-the-server-version","text":"The SSH server version is presented to any connecting client in plain text upon connection. It has the following format: SSH-2.0-softwareversion <SP> comments The softwareversion can only contain printable US-ASCII characters without whitespace and minus ( - ) signs. The comments field is optional and is separated from the softwareversion with a single space. The maximum length of the version string is 255 characters.","title":"Configuring the server version"},{"location":"reference/0.4.0/ssh/#configuring-a-banner","text":"SSH offers the ability to output a message to the clients before they enter passwords. This can be configured in the banner option. The banner can contain multiple lines.","title":"Configuring a banner"},{"location":"reference/0.4.0/ssh/#ciphers","text":"ContainerSSH supports the following ciphers. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID","title":"Ciphers"},{"location":"reference/0.4.0/ssh/#key-exchange","text":"ContainerSSH supports the following key exchange algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1","title":"Key exchange"},{"location":"reference/0.4.0/ssh/#mac","text":"ContainerSSH supports the following MAC algorithms. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96","title":"MAC"},{"location":"reference/0.4.0/sshproxy/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb The SSH proxy backend The SSH proxy backend does not launch containers, instead it connects to a second SSH server and forwards the connections to that backend. This allows for using the audit log to inspect SSH traffic, or to dynamically forwarding connections using the configuration webhook . The base configuration structure \u00b6 The minimum configuration is the following: backend : sshproxy sshproxy : # Add the backend server here server : 127.0.0.1 # Set the following option to true to reuse the connecting user's username. usernamePassThrough : true # Or specify a username manually username : root # Specify the password password : changeme # Or the private key. This can reference a file or be added directly. privateKey : | -----BEGIN OPENSSH PRIVATE KEY----- ... # Provide all fingerprints of the backing SSH server's host keys: allowedHostKeyFingerprints : - SHA256:... Tip You can obtain the fingerprints of OpenSSH host keys by running the following script: for i in /etc/ssh/ssh_host_*.pub; do ssh-keygen -l -f $i; done | cut -d ' ' -f 2 Warning ContainerSSH does not support passing through passwords or public key authentication to the backing server. We recommend setting up private-public key authentication with the backing server. Configuration options \u00b6 Option Type Description server string Host name or IP address of the backing SSH server. Required. port uint16 Port number of the backing SSH service. Defaults to 22. usernamePassThrough bool Take username from the connecting client. username string Explicitly set the username to use for the backing connection. Required if usernamePassThrough is false . password string Password to use to authenticate with the backing SSH server. privateKey string Private key to use to authenticate with the backing SSH server. Can be a reference to a file or the private key in PEM or OpenSSH format. allowedHostKeyFingerprints []string List of SHA256 fingerprints of the backing SSH server. ciphers []string List of SSH ciphers to use. See Ciphers below. kex []string List of key exchange algorithms to use. See Key exchange algorithms below. macs []string List of MAC algorithms to use. See MAC algorithms below. hostKeyAlgorithms []string List of host key algorithms to request from the backing server. See Host key algorithms below. timeout string Timeout for connecting / retrying the SSH connection. clientVersion string Client version string to send to the backing server. Must be in the format of SSH-protoversion-softwareversion SPACE comments . See RFC 4235 section 4.2. Protocol Version Exchange for details. The trailing CR and LF characters should not be added to this string. Ciphers \u00b6 ContainerSSH supports the following ciphers for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID Key exchange algorithms \u00b6 ContainerSSH supports the following key exchange algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1 MAC algorithms \u00b6 ContainerSSH supports the following MAC algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96 Host key algorithms \u00b6 ContainerSSH supports the following host key algorithms for verifying the backing server identity. Algorithm Default ssh-rsa-cert-v01@openssh.com ssh-dss-cert-v01@openssh.com ecdsa-sha2-nistp256-cert-v01@openssh.com ecdsa-sha2-nistp384-cert-v01@openssh.com ecdsa-sha2-nistp521-cert-v01@openssh.com ssh-ed25519-cert-v01@openssh.com ssh-rsa ssh-dss ssh-ed25519","title":"SSH proxy"},{"location":"reference/0.4.0/sshproxy/#the-base-configuration-structure","text":"The minimum configuration is the following: backend : sshproxy sshproxy : # Add the backend server here server : 127.0.0.1 # Set the following option to true to reuse the connecting user's username. usernamePassThrough : true # Or specify a username manually username : root # Specify the password password : changeme # Or the private key. This can reference a file or be added directly. privateKey : | -----BEGIN OPENSSH PRIVATE KEY----- ... # Provide all fingerprints of the backing SSH server's host keys: allowedHostKeyFingerprints : - SHA256:... Tip You can obtain the fingerprints of OpenSSH host keys by running the following script: for i in /etc/ssh/ssh_host_*.pub; do ssh-keygen -l -f $i; done | cut -d ' ' -f 2 Warning ContainerSSH does not support passing through passwords or public key authentication to the backing server. We recommend setting up private-public key authentication with the backing server.","title":"The base configuration structure"},{"location":"reference/0.4.0/sshproxy/#configuration-options","text":"Option Type Description server string Host name or IP address of the backing SSH server. Required. port uint16 Port number of the backing SSH service. Defaults to 22. usernamePassThrough bool Take username from the connecting client. username string Explicitly set the username to use for the backing connection. Required if usernamePassThrough is false . password string Password to use to authenticate with the backing SSH server. privateKey string Private key to use to authenticate with the backing SSH server. Can be a reference to a file or the private key in PEM or OpenSSH format. allowedHostKeyFingerprints []string List of SHA256 fingerprints of the backing SSH server. ciphers []string List of SSH ciphers to use. See Ciphers below. kex []string List of key exchange algorithms to use. See Key exchange algorithms below. macs []string List of MAC algorithms to use. See MAC algorithms below. hostKeyAlgorithms []string List of host key algorithms to request from the backing server. See Host key algorithms below. timeout string Timeout for connecting / retrying the SSH connection. clientVersion string Client version string to send to the backing server. Must be in the format of SSH-protoversion-softwareversion SPACE comments . See RFC 4235 section 4.2. Protocol Version Exchange for details. The trailing CR and LF characters should not be added to this string.","title":"Configuration options"},{"location":"reference/0.4.0/sshproxy/#ciphers","text":"ContainerSSH supports the following ciphers for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default chacha20-poly1305@openssh.com aes256-gcm@openssh.com aes128-gcm@openssh.com aes256-ctr aes192-ctr aes128-ctr aes128-cbc arcfour256 arcfour128 arcfour tripledescbcID","title":"Ciphers"},{"location":"reference/0.4.0/sshproxy/#key-exchange-algorithms","text":"ContainerSSH supports the following key exchange algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default curve25519-sha256@libssh.org ecdh-sha2-nistp521 ecdh-sha2-nistp384 ecdh-sha2-nistp256 diffie-hellman-group14-sha1 diffie-hellman-group1-sha1","title":"Key exchange algorithms"},{"location":"reference/0.4.0/sshproxy/#mac-algorithms","text":"ContainerSSH supports the following MAC algorithms for contacting the backing server. The defaults are configured based on Mozilla Modern suite . Algorithm Default hmac-sha2-256-etm@openssh.com hmac-sha2-256 hmac-sha1 hmac-sha1-96","title":"MAC algorithms"},{"location":"reference/0.4.0/sshproxy/#host-key-algorithms","text":"ContainerSSH supports the following host key algorithms for verifying the backing server identity. Algorithm Default ssh-rsa-cert-v01@openssh.com ssh-dss-cert-v01@openssh.com ecdsa-sha2-nistp256-cert-v01@openssh.com ecdsa-sha2-nistp384-cert-v01@openssh.com ecdsa-sha2-nistp521-cert-v01@openssh.com ssh-ed25519-cert-v01@openssh.com ssh-rsa ssh-dss ssh-ed25519","title":"Host key algorithms"},{"location":"reference/0.4.0/troubleshooting/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb Troubleshooting ContainerSSH When ContainerSSH doesn't work several components can be involved. Is it ContainerSSH? Can it connect to the authentication server ? How about the configuration server ? Is the backend working ? This guide will lead you through the steps of debugging issues with ContainerSSH. Turning on debug logging \u00b6 The first step in determining any potential issues is to enable debug logging . This will output a slew of log messages into the designated log destination that you can use to determine what's going wrong. If you are trying to debug production failures with lots of traffic it can sometimes be hard to read these logs. That's why a useful field in all connection-related messages is the connectionId field. This can be used to tie related log messages together. If your configuration server is flexible enough you can pass the debug log level on a per-user basis to increase the log verbosity for a single user only. Each message has a unique code. The list of codes is documented in the codes section . Connecting in debug mode \u00b6 Most SSH clients have a debug mode. In the command line SSH you can add the -vvv flag to get a lot more verbose output: ssh youruser@youserver.com -vvv Turning on audit logging \u00b6 Another useful tool to turn on is audit logging . Audit logging can be another helpful tool when trying to figure out what a certain user is doing. It can record every single interaction that happens over SSH. More importantly, audit logs are also tied to the connectionId that is also present in the logs above. Debugging webhook server failures \u00b6 Apart from turning on logging you can also use network monitoring to debug authentication and configuration webhook failures. Even if the connection itself is encrypted, using a tool like tcpdump or Wireshark can give you useful clues if the connection is even established correctly or if something is failing on a connection level. Debugging container backend failures \u00b6 When it comes to interacting with container backend you can also use logs as well as network monitoring to determine basic failures. Furthermore, both Docker and Kubernetes provide the ability to monitor events that are happening to get an idea of what's going on. Docker docker events Kubernetes kubectl get events --watch Debugging with strace \u00b6 If none of the above steps help it is time to unpack the big tools. strace is a Linux utility that lets you list all system calls ContainerSSH is doing. This is not the easiest log to read, but the following snippet should help: strace \\ -e trace=open,read,write,readv,writev,recv,recvfrom,send,sendto \\ -s 999 \\ -p CONTAINERSSH-PID-HERE If all else fails: ask for help \u00b6 If all else fails we are here for you. Please collect the following items: The debug logs from above. Your configuration file without any sensitive details (credentials, IPs). Please also prepare the following items if you can, but don't submit them as they may contain sensitive credentials: Audit logs (if you have them). Any pcap files from tcpdump or Wireshark you may have. Any strace outputs you may have. Any Docker or Kubernetes events you may have recorded. You can raise your question in one of the following channels: As a GitHub issue. As a discussion post. On the debugged.it Discord. Please link the debug logs and configuration from a GitHub Gist or a Pastebin . Don't worry about submitting duplicate issues, just make sure to describe your issue in as much detail as possible.","title":"Troubleshooting guide"},{"location":"reference/0.4.0/troubleshooting/#turning-on-debug-logging","text":"The first step in determining any potential issues is to enable debug logging . This will output a slew of log messages into the designated log destination that you can use to determine what's going wrong. If you are trying to debug production failures with lots of traffic it can sometimes be hard to read these logs. That's why a useful field in all connection-related messages is the connectionId field. This can be used to tie related log messages together. If your configuration server is flexible enough you can pass the debug log level on a per-user basis to increase the log verbosity for a single user only. Each message has a unique code. The list of codes is documented in the codes section .","title":"Turning on debug logging"},{"location":"reference/0.4.0/troubleshooting/#connecting-in-debug-mode","text":"Most SSH clients have a debug mode. In the command line SSH you can add the -vvv flag to get a lot more verbose output: ssh youruser@youserver.com -vvv","title":"Connecting in debug mode"},{"location":"reference/0.4.0/troubleshooting/#turning-on-audit-logging","text":"Another useful tool to turn on is audit logging . Audit logging can be another helpful tool when trying to figure out what a certain user is doing. It can record every single interaction that happens over SSH. More importantly, audit logs are also tied to the connectionId that is also present in the logs above.","title":"Turning on audit logging"},{"location":"reference/0.4.0/troubleshooting/#debugging-webhook-server-failures","text":"Apart from turning on logging you can also use network monitoring to debug authentication and configuration webhook failures. Even if the connection itself is encrypted, using a tool like tcpdump or Wireshark can give you useful clues if the connection is even established correctly or if something is failing on a connection level.","title":"Debugging webhook server failures"},{"location":"reference/0.4.0/troubleshooting/#debugging-container-backend-failures","text":"When it comes to interacting with container backend you can also use logs as well as network monitoring to determine basic failures. Furthermore, both Docker and Kubernetes provide the ability to monitor events that are happening to get an idea of what's going on. Docker docker events Kubernetes kubectl get events --watch","title":"Debugging container backend failures"},{"location":"reference/0.4.0/troubleshooting/#debugging-with-strace","text":"If none of the above steps help it is time to unpack the big tools. strace is a Linux utility that lets you list all system calls ContainerSSH is doing. This is not the easiest log to read, but the following snippet should help: strace \\ -e trace=open,read,write,readv,writev,recv,recvfrom,send,sendto \\ -s 999 \\ -p CONTAINERSSH-PID-HERE","title":"Debugging with strace"},{"location":"reference/0.4.0/troubleshooting/#if-all-else-fails-ask-for-help","text":"If all else fails we are here for you. Please collect the following items: The debug logs from above. Your configuration file without any sensitive details (credentials, IPs). Please also prepare the following items if you can, but don't submit them as they may contain sensitive credentials: Audit logs (if you have them). Any pcap files from tcpdump or Wireshark you may have. Any strace outputs you may have. Any Docker or Kubernetes events you may have recorded. You can raise your question in one of the following channels: As a GitHub issue. As a discussion post. On the debugged.it Discord. Please link the debug logs and configuration from a GitHub Gist or a Pastebin . Don't worry about submitting duplicate issues, just make sure to describe your issue in as much detail as possible.","title":"If all else fails: ask for help"},{"location":"reference/0.4.0/api/","text":"Old manual You are reading the reference manual of an older release. Read the current manual \u00bb ContainerSSH itself does not have an API apart from exposing the metrics . However, it requires two external APIs: the authentication server for user authentication and the config server for supplying a user-specific container configuration. We are providing an OpenAPI document for both. Access the OpenAPI docs \u00bb","title":"API"},{"location":"reference/api/","text":"ContainerSSH itself does not have an API apart from exposing the metrics . However, it requires two external APIs: the authentication server for user authentication and the config server for supplying a user-specific container configuration. We are providing an OpenAPI document for both. Access the OpenAPI docs \u00bb","title":"API"},{"location":"usecases/honeypots/","text":"When left undefended, SSH can be a large attack surface towards the internet. If you leave an SSH server open to the Internet, bots will try to brute force their way in within minutes. Why not build a honeypot? Honeypots can provide valuable early warning: log the IP addresses of connection attempts and dynamically firewall them. Collect credentials attackers are trying to use and match them against your user database to root out weak passwords. Collect logs of what attackers are doing in a containerized environment. ContainerSSH version 0.4 and up offer comprehensive audit logging to record everything the attacker is doing. ContainerSSH can do all that. When a user connects, ContainerSSH reaches out to your authentication server where you can log IP addresses and credentials. If you allow attackers to connect, ContainerSSH fetches a dynamic container configuration from your configuration server . You can specify what environment and on which Docker or Kubernetes setup to run your honeypot. Restrict attackers to a set amount of resources or a networkless environment. Get started \u00bb Read the guide \u00bb","title":"Honeypots"},{"location":"usecases/learning/","text":"Providing console access to your students can be a hurdle. No matter if you want to provide access to a Linux environment, databases, or something else, you will have to create users, make sure they have everything they need, and clean up when they are done. Containers can provide an easier solution: launch a specific environment for a student and simply remove the container when they are done. However, manually provisioning containers can still be tedious and continuously running a large number of containers can be resource-intensive. ContainerSSH provides a vital role here: it can dynamically launch containers as needed. When users connect via SSH, ContainerSSH reaches out to your authentication server to verify user credentials and then contacts your configuration server to fetch the customized container configuration for your user. When your user disconnects, ContainerSSH removes their container and leaves no trace behind. Get started \u00bb","title":"Learning environments"},{"location":"usecases/security/","text":"Do you need to provide secure access to a console environment and highly sensitive credentials to users? Key management systems like HashiCorp Vault can change credentials frequently to counteract credential leakage or theft by users. However, educating your users to use the key management system can be time-consuming. ContainerSSH provides a user-friendly solution. When your users connect to the SSH server it reaches out to an authentication server provided by you. This lets you authenticate them against your own user database using passwords or SSH keys. When authenticated successfully, ContainerSSH contacts your configuration server to get the configuration for your user. The configuration server can expose short lived credentials from the key management system in the container environment. Even if your users steal or leak the credentials, they are only valid for a short time. The audit log can record any action taken by a user and upload it to an S3-compatible object storage. Get started \u00bb","title":"Security"},{"location":"usecases/webhosting/","text":"Providing SSH access in a web hosting environment is tricky. Users may run unexpected scripts that consume lots of resources. They may have permission issues if they are not able to SSH with the same user as the webserver, which in turn presents security issues. Containers present a good solution for this problem: you can run a container as the same user as the web server, but keep them in isolation from the actual production environment. You can use NFS mounts to isolate them from the production servers. You can even mount folders based on an advanced permission matrix. However, running an SSH server per user is very cost-intensive in an industry where individual customers don't pay much. That's where ContainerSSH fills an important role: when users connect via SSH, ContainerSSH reaches out to your authentication server to verify user credentials and then contacts your configuration server to fetch the customized container configuration for your user. When your user disconnects, ContainerSSH removes their container and leaves no trace behind. ContainerSSH also supports SFTP, which provides secure file transfers. It can replace the old and arguably broken FTP, so you no longer have to worry about that either. If you are running multiple servers, you can even provide dynamic Docker connect strings and credentials to connect the server where the user is located. Get started \u00bb","title":"Web hosting"}]}